EXECUTION:
  DATAREADER:
    CRAWLER_ARGS: 
      azstorage: ednadatasets
      azcontainer: edna-covid-raw
      azfile: tweets-2020-01-22.json.gz
    DATAREADER: AlbertReader
    DATASET_ARGS:
      classificationclass: ['fnews']
      maxlen: 512
      shardcache: True
      shardsize: 20000
      shard_replace: False
      shardpath: datashard-artifacts
      shardname: fnc-filtermask-shard
      mlm_probability: 0.15
      masking: False
      shuffle: True
      data_shuffle: True
      keyword_mask: True
      keywords: ["covid", "corona", "mask", "wuhan", "n95", "sars", "monkey", "pandemic", "social", "quarantin", "virus", "infect", "lock", "ppe", "variant", "vaccine", "travel", "omicron", "ivermectin", 
        "plandemic", "5g", "gates", "hoax", "bioweapon", "bat", "fauci"]
    GENERATOR_ARGS: 
      tokenizer: HFAutoTokenizer
      from_pretrained: albert-base-v2
  EPOCHS: 3
  FP16: False
  MODEL_SERVING: False
  OPTIMIZER_BUILDER: ClassificationOptimizer
  SKIPEVAL: False
  TEST_FREQUENCY: 1
  TRAINER: BaseTrainer
  TRAINER_ARGS: 
    accumulation_steps: 8


TRANSFORMATION:
  BATCH_SIZE: 16


SAVE:
  MODEL_VERSION: 1
  MODEL_CORE_NAME: fnc-extension
  MODEL_BACKBONE: albert
  MODEL_QUALIFIER: tweets-2020-01-22
  DRIVE_BACKUP: True
  SAVE_FREQUENCY: 1 # Epoch
  CHECKPOINT_DIRECTORY: "./drive/MyDrive/Projects/FNC/Models/"


MODEL:
  BUILDER: ednaml_model_builder
  MODEL_ARCH: FNCAlbertModeler
  MODEL_BASE: Albert
  MODEL_KWARGS: 
    vocab_size_or_config_json_file: 30000
    embedding_size: 128
    hidden_size: 768
    num_hidden_layers: 12
    num_hidden_groups: 1
    num_attention_heads: 12
    intermediate_size: 3072
    inner_group_num: 1
    hidden_act: gelu
    hidden_dropout_prob: 0
    attention_probs_dropout_prob: 0
    max_position_embeddings: 512
    type_vocab_size: 2
    initializer_range: 0.02
    layer_norm_eps: 1.0e-12
    pooling: pooled
  MODEL_NORMALIZATION: bn
  PARAMETER_GROUPS: [opt-1]


LOSS: 
  - LOSSES: ['TorchLoss']
    KWARGS: 
      - loss_class: CrossEntropyLoss
        loss_kwargs:
          ignore_index: -1
    LAMBDAS: [1.0]
    NAME: mask_lm
    LABEL: mask_lm

OPTIMIZER:
  - OPTIMIZER: AdamW
    BASE_LR: 1.0e-5
    LR_BIAS_FACTOR: 1.0
    OPTIMIZER_KWARGS: 
      eps: 1.0e-6
    OPTIMIZER_NAME: opt-1
    WEIGHT_BIAS_FACTOR: 0.0005
    WEIGHT_DECAY: 0.0005

SCHEDULER:
  - LR_KWARGS: 
      step_size: 5
      gamma: 0.5
    LR_SCHEDULER: StepLR
    SCHEDULER_NAME: opt-1

LOGGING:
  STEP_VERBOSE: 100
  INPUT_SIZE: [16, 512]

