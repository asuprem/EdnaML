<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>ednaml.models.MultiClassificationResnet API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ednaml.models.MultiClassificationResnet</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from typing import List
from torch import nn
from ednaml.models.ClassificationResnet import ClassificationResnet


class MultiClassificationResnet(ClassificationResnet):
    &#34;&#34;&#34;Multiclassification Resnet model, that performs multiple classifications from the same backbone.

    A MulticlassificationResnet model is a base ResNet with multiple FC classification layers.

    Args: (TODO)
        base (str): The architecture base for resnet, i.e. resnet50, resnet18
        weights (str, None): Path to weights file for the architecture base ONLY. If not provided, base initialized with random values.
        normalization (str, None): Can be None, where it is torch&#39;s normalization. Else create a normalization layer. Supports: [&#34;bn&#34;, &#34;l2&#34;, &#34;in&#34;, &#34;gn&#34;, &#34;ln&#34;]
        metadata (Dict[str:int], None): FC dimensions for each classification, keyed by class names

    Kwargs (MODEL_KWARGS):
        number_outputs (int): Number of different FC layers connected to the feature layer of the backbone.
        softmax_dimensions (List[int]): Classes of each FC layer, in order. Optional. If not provided, `MultiClassificationResnet` will use `output_classnames` and `metadata` to infer dimension size
        output_classnames (List[str]): The name for each output, in order. These should be the same as the label names for the multiple classes.
        labelnames (List[str]): The order of labels provided by the crawler. This is used during model training, where crawler ground truth labels must be matched to model outputs. 
        
        last_stride (int, 1): The final stride parameter for the architecture core. Should be one of 1 or 2.
        attention (str, None): The attention module to use. Only supports [&#39;cbam&#39;, &#39;dbam&#39;]
        input_attention (bool, false): Whether to include the IA module
        secondary_attention (List[int], None): Whether to modify CBAM to apply it to specific Resnet basic blocks. None means CBAM is applied to all. Otherwise, CBAM is applied only to the basic blocks provided here in List.
        part_attention (bool): Whether to use local attention

    Default Kwargs (DO NOT CHANGE OR ADD TO MODEL_KWARGS):
        zero_init_residual (bool, false): Whether the final layer uses zero initialization
        top_only (bool, true): Whether to keep only the architecture base without imagenet fully-connected layers (1000 classes)
        num_classes (int, 1000): Number of features in final imagenet FC layer
        groups (int, 1): Used during resnet variants construction
        width_per_group (int, 64): Used during resnet variants construction
        replace_stride_with_dilation (bool, None): Well, replace stride with dilation...
        norm_layer (nn.Module, None): The normalization layer within resnet. Internally defaults to nn.BatchNorm2D

    Methods: 
        forward: Process a batch

    &#34;&#34;&#34;

    model_name = &#34;MultiClassificationResNet&#34;
    model_arch = &#34;MultiClassificationResNet&#34;
    number_outputs = 1
    output_names = [&#34;out0&#34;]
    softmax_dimensions = [None]
    output_labels = [&#34;color&#34;]
    secondary_outputs = []

    _internal_name_count = 0

    def __init__(
        self, base=&#34;resnet50&#34;, weights=None, normalization=None, metadata=None, parameter_groups: List[str]=None, **kwargs
    ):
        &#34;&#34;&#34;We will inherit the base construction from ClassificationResNet, and modify the softmax head.

        Args:
            base (str, optional): _description_. Defaults to &#39;resnet50&#39;.
            weights (_type_, optional): _description_. Defaults to None.
            normalization (_type_, optional): _description_. Defaults to None.
            metadata (_type_, optional): _description_. Defaults to None.
        &#34;&#34;&#34;

        super().__init__(
            base=base,
            weights=weights,
            normalization=normalization,
            metadata=metadata,
            parameter_groups=parameter_groups,
            **kwargs
        )

    def model_attributes_setup(self, **kwargs):

        self.embedding_dimensions = kwargs.get(&#34;embedding_dimensions&#34;, None)
        if self.normalization == &#34;&#34;:
            self.normalization = None

        self.number_outputs = kwargs.get(&#34;number_outputs&#34;, 1)

        outputs = kwargs.get(
            &#34;outputs&#34;,
            [
                {
                    &#34;name&#34;: self._internal_name_counter(),
                    &#34;label&#34;: &#34;color&#34;,
                    &#34;dimensions&#34;: None,
                }
            ],
        )

        if len(outputs) != self.number_outputs:
            raise ValueError(
                &#34;Mismatch in length of outputs %i and number of outputs %i&#34;
                % (len(outputs), self.number_outputs)
            )

        self.softmax_dimensions = [None] * self.number_outputs
        self.output_names = [None] * self.number_outputs
        self.output_labels = [None] * self.number_outputs

        for idx, output_details in enumerate(outputs):
            self.softmax_dimensions[idx] = output_details.get(&#34;dimensions&#34;, None)
            self.output_names[idx] = output_details.get(
                &#34;name&#34;, self._internal_name_counter()
            )
            self.output_labels[idx] = output_details[&#34;label&#34;]
            if self.softmax_dimensions[idx] is None:
                self.softmax_dimensions[idx] = self.metadata.getLabelDimensions(
                    self.output_labels[idx]
                )
        self.base = None
        self.gap = None
        self.emb_linear = None
        self.feat_norm = None
        self.softmax = None

    def _internal_name_counter(self):
        out = &#34;out&#34; + str(self._internal_name_count)
        self._internal_name_count += 1
        return out

    def build_softmax(self, **kwargs):
        &#34;&#34;&#34;Build the softmax layers, using info either in self.softmax_dimensions or by combining metadata info of labelname-&gt;numclasses and the outputclassnames
        &#34;&#34;&#34;
        # NOTE, for re-id type models...multiclassification model will anyway yield the features with softmax outputs, so we don&#39;t have to worry about that...
        # For pure-reid model, probably best to use ClassificationResNet and modify to use no softmax...TODO this is a future step...
        tsoftmax = [None] * self.number_outputs
        for idx, fc_dimension in enumerate(self.softmax_dimensions):
            tsoftmax[idx] = nn.Linear(
                self.embedding_dimensions, fc_dimension, bias=False
            )
            tsoftmax[idx].apply(self.weights_init_softmax)
        self.softmax = nn.ModuleList(tsoftmax)

    def base_forward(self, x):
        features = self.gap(self.base(x))
        features = features.view(features.shape[0], -1)
        features = self.emb_linear(features)
        return features

    def forward_impl(self, x, **kwargs):
        features = self.base_forward(x)

        # if self.feat_norm is not None: &lt;-- no need, identity
        features = self.feat_norm(features)

        softmax_logits = [None] * self.number_outputs
        for idx, softmaxlayer in enumerate(self.softmax):
            softmax_logits[idx] = softmaxlayer(features)
        return (
            softmax_logits,
            features,
            [],
        )  # soft logits are the softmax logits we will use to for training. We can use features to store the historical probability????</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="ednaml.models.MultiClassificationResnet.MultiClassificationResnet"><code class="flex name class">
<span>class <span class="ident">MultiClassificationResnet</span></span>
<span>(</span><span>base='resnet50', weights=None, normalization=None, metadata=None, parameter_groups: List[str] = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Multiclassification Resnet model, that performs multiple classifications from the same backbone.</p>
<p>A MulticlassificationResnet model is a base ResNet with multiple FC classification layers.</p>
<p>Args: (TODO)
base (str): The architecture base for resnet, i.e. resnet50, resnet18
weights (str, None): Path to weights file for the architecture base ONLY. If not provided, base initialized with random values.
normalization (str, None): Can be None, where it is torch's normalization. Else create a normalization layer. Supports: ["bn", "l2", "in", "gn", "ln"]
metadata (Dict[str:int], None): FC dimensions for each classification, keyed by class names</p>
<p>Kwargs (MODEL_KWARGS):
number_outputs (int): Number of different FC layers connected to the feature layer of the backbone.
softmax_dimensions (List[int]): Classes of each FC layer, in order. Optional. If not provided, <code><a title="ednaml.models.MultiClassificationResnet.MultiClassificationResnet" href="#ednaml.models.MultiClassificationResnet.MultiClassificationResnet">MultiClassificationResnet</a></code> will use <code>output_classnames</code> and <code>metadata</code> to infer dimension size
output_classnames (List[str]): The name for each output, in order. These should be the same as the label names for the multiple classes.
labelnames (List[str]): The order of labels provided by the crawler. This is used during model training, where crawler ground truth labels must be matched to model outputs. </p>
<pre><code>last_stride (int, 1): The final stride parameter for the architecture core. Should be one of 1 or 2.
attention (str, None): The attention module to use. Only supports ['cbam', 'dbam']
input_attention (bool, false): Whether to include the IA module
secondary_attention (List[int], None): Whether to modify CBAM to apply it to specific Resnet basic blocks. None means CBAM is applied to all. Otherwise, CBAM is applied only to the basic blocks provided here in List.
part_attention (bool): Whether to use local attention
</code></pre>
<p>Default Kwargs (DO NOT CHANGE OR ADD TO MODEL_KWARGS):
zero_init_residual (bool, false): Whether the final layer uses zero initialization
top_only (bool, true): Whether to keep only the architecture base without imagenet fully-connected layers (1000 classes)
num_classes (int, 1000): Number of features in final imagenet FC layer
groups (int, 1): Used during resnet variants construction
width_per_group (int, 64): Used during resnet variants construction
replace_stride_with_dilation (bool, None): Well, replace stride with dilation&hellip;
norm_layer (nn.Module, None): The normalization layer within resnet. Internally defaults to nn.BatchNorm2D</p>
<p>Methods:
forward: Process a batch</p>
<p>We will inherit the base construction from ClassificationResNet, and modify the softmax head.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>base</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd><em>description</em>. Defaults to 'resnet50'.</dd>
<dt><strong><code>weights</code></strong> :&ensp;<code>_type_</code>, optional</dt>
<dd><em>description</em>. Defaults to None.</dd>
<dt><strong><code>normalization</code></strong> :&ensp;<code>_type_</code>, optional</dt>
<dd><em>description</em>. Defaults to None.</dd>
<dt><strong><code>metadata</code></strong> :&ensp;<code>_type_</code>, optional</dt>
<dd><em>description</em>. Defaults to None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MultiClassificationResnet(ClassificationResnet):
    &#34;&#34;&#34;Multiclassification Resnet model, that performs multiple classifications from the same backbone.

    A MulticlassificationResnet model is a base ResNet with multiple FC classification layers.

    Args: (TODO)
        base (str): The architecture base for resnet, i.e. resnet50, resnet18
        weights (str, None): Path to weights file for the architecture base ONLY. If not provided, base initialized with random values.
        normalization (str, None): Can be None, where it is torch&#39;s normalization. Else create a normalization layer. Supports: [&#34;bn&#34;, &#34;l2&#34;, &#34;in&#34;, &#34;gn&#34;, &#34;ln&#34;]
        metadata (Dict[str:int], None): FC dimensions for each classification, keyed by class names

    Kwargs (MODEL_KWARGS):
        number_outputs (int): Number of different FC layers connected to the feature layer of the backbone.
        softmax_dimensions (List[int]): Classes of each FC layer, in order. Optional. If not provided, `MultiClassificationResnet` will use `output_classnames` and `metadata` to infer dimension size
        output_classnames (List[str]): The name for each output, in order. These should be the same as the label names for the multiple classes.
        labelnames (List[str]): The order of labels provided by the crawler. This is used during model training, where crawler ground truth labels must be matched to model outputs. 
        
        last_stride (int, 1): The final stride parameter for the architecture core. Should be one of 1 or 2.
        attention (str, None): The attention module to use. Only supports [&#39;cbam&#39;, &#39;dbam&#39;]
        input_attention (bool, false): Whether to include the IA module
        secondary_attention (List[int], None): Whether to modify CBAM to apply it to specific Resnet basic blocks. None means CBAM is applied to all. Otherwise, CBAM is applied only to the basic blocks provided here in List.
        part_attention (bool): Whether to use local attention

    Default Kwargs (DO NOT CHANGE OR ADD TO MODEL_KWARGS):
        zero_init_residual (bool, false): Whether the final layer uses zero initialization
        top_only (bool, true): Whether to keep only the architecture base without imagenet fully-connected layers (1000 classes)
        num_classes (int, 1000): Number of features in final imagenet FC layer
        groups (int, 1): Used during resnet variants construction
        width_per_group (int, 64): Used during resnet variants construction
        replace_stride_with_dilation (bool, None): Well, replace stride with dilation...
        norm_layer (nn.Module, None): The normalization layer within resnet. Internally defaults to nn.BatchNorm2D

    Methods: 
        forward: Process a batch

    &#34;&#34;&#34;

    model_name = &#34;MultiClassificationResNet&#34;
    model_arch = &#34;MultiClassificationResNet&#34;
    number_outputs = 1
    output_names = [&#34;out0&#34;]
    softmax_dimensions = [None]
    output_labels = [&#34;color&#34;]
    secondary_outputs = []

    _internal_name_count = 0

    def __init__(
        self, base=&#34;resnet50&#34;, weights=None, normalization=None, metadata=None, parameter_groups: List[str]=None, **kwargs
    ):
        &#34;&#34;&#34;We will inherit the base construction from ClassificationResNet, and modify the softmax head.

        Args:
            base (str, optional): _description_. Defaults to &#39;resnet50&#39;.
            weights (_type_, optional): _description_. Defaults to None.
            normalization (_type_, optional): _description_. Defaults to None.
            metadata (_type_, optional): _description_. Defaults to None.
        &#34;&#34;&#34;

        super().__init__(
            base=base,
            weights=weights,
            normalization=normalization,
            metadata=metadata,
            parameter_groups=parameter_groups,
            **kwargs
        )

    def model_attributes_setup(self, **kwargs):

        self.embedding_dimensions = kwargs.get(&#34;embedding_dimensions&#34;, None)
        if self.normalization == &#34;&#34;:
            self.normalization = None

        self.number_outputs = kwargs.get(&#34;number_outputs&#34;, 1)

        outputs = kwargs.get(
            &#34;outputs&#34;,
            [
                {
                    &#34;name&#34;: self._internal_name_counter(),
                    &#34;label&#34;: &#34;color&#34;,
                    &#34;dimensions&#34;: None,
                }
            ],
        )

        if len(outputs) != self.number_outputs:
            raise ValueError(
                &#34;Mismatch in length of outputs %i and number of outputs %i&#34;
                % (len(outputs), self.number_outputs)
            )

        self.softmax_dimensions = [None] * self.number_outputs
        self.output_names = [None] * self.number_outputs
        self.output_labels = [None] * self.number_outputs

        for idx, output_details in enumerate(outputs):
            self.softmax_dimensions[idx] = output_details.get(&#34;dimensions&#34;, None)
            self.output_names[idx] = output_details.get(
                &#34;name&#34;, self._internal_name_counter()
            )
            self.output_labels[idx] = output_details[&#34;label&#34;]
            if self.softmax_dimensions[idx] is None:
                self.softmax_dimensions[idx] = self.metadata.getLabelDimensions(
                    self.output_labels[idx]
                )
        self.base = None
        self.gap = None
        self.emb_linear = None
        self.feat_norm = None
        self.softmax = None

    def _internal_name_counter(self):
        out = &#34;out&#34; + str(self._internal_name_count)
        self._internal_name_count += 1
        return out

    def build_softmax(self, **kwargs):
        &#34;&#34;&#34;Build the softmax layers, using info either in self.softmax_dimensions or by combining metadata info of labelname-&gt;numclasses and the outputclassnames
        &#34;&#34;&#34;
        # NOTE, for re-id type models...multiclassification model will anyway yield the features with softmax outputs, so we don&#39;t have to worry about that...
        # For pure-reid model, probably best to use ClassificationResNet and modify to use no softmax...TODO this is a future step...
        tsoftmax = [None] * self.number_outputs
        for idx, fc_dimension in enumerate(self.softmax_dimensions):
            tsoftmax[idx] = nn.Linear(
                self.embedding_dimensions, fc_dimension, bias=False
            )
            tsoftmax[idx].apply(self.weights_init_softmax)
        self.softmax = nn.ModuleList(tsoftmax)

    def base_forward(self, x):
        features = self.gap(self.base(x))
        features = features.view(features.shape[0], -1)
        features = self.emb_linear(features)
        return features

    def forward_impl(self, x, **kwargs):
        features = self.base_forward(x)

        # if self.feat_norm is not None: &lt;-- no need, identity
        features = self.feat_norm(features)

        softmax_logits = [None] * self.number_outputs
        for idx, softmaxlayer in enumerate(self.softmax):
            softmax_logits[idx] = softmaxlayer(features)
        return (
            softmax_logits,
            features,
            [],
        )  # soft logits are the softmax logits we will use to for training. We can use features to store the historical probability????</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="ednaml.models.ClassificationResnet.ClassificationResnet" href="ClassificationResnet.html#ednaml.models.ClassificationResnet.ClassificationResnet">ClassificationResnet</a></li>
<li><a title="ednaml.models.ModelAbstract.ModelAbstract" href="ModelAbstract.html#ednaml.models.ModelAbstract.ModelAbstract">ModelAbstract</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="ednaml.models.MultiClassificationResnet.MultiClassificationResnet.metadata"><code class="name">var <span class="ident">metadata</span> : <a title="ednaml.utils.LabelMetadata.LabelMetadata" href="../utils/LabelMetadata.html#ednaml.utils.LabelMetadata.LabelMetadata">LabelMetadata</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.models.MultiClassificationResnet.MultiClassificationResnet.model_arch"><code class="name">var <span class="ident">model_arch</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.models.MultiClassificationResnet.MultiClassificationResnet.model_base"><code class="name">var <span class="ident">model_base</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.models.MultiClassificationResnet.MultiClassificationResnet.model_name"><code class="name">var <span class="ident">model_name</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.models.MultiClassificationResnet.MultiClassificationResnet.normalization"><code class="name">var <span class="ident">normalization</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.models.MultiClassificationResnet.MultiClassificationResnet.number_outputs"><code class="name">var <span class="ident">number_outputs</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.models.MultiClassificationResnet.MultiClassificationResnet.output_labels"><code class="name">var <span class="ident">output_labels</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.models.MultiClassificationResnet.MultiClassificationResnet.output_names"><code class="name">var <span class="ident">output_names</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.models.MultiClassificationResnet.MultiClassificationResnet.parameter_groups"><code class="name">var <span class="ident">parameter_groups</span> : Dict[str, torch.nn.modules.module.Module]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.models.MultiClassificationResnet.MultiClassificationResnet.secondary_outputs"><code class="name">var <span class="ident">secondary_outputs</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.models.MultiClassificationResnet.MultiClassificationResnet.softmax_dimensions"><code class="name">var <span class="ident">softmax_dimensions</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.models.MultiClassificationResnet.MultiClassificationResnet.weights"><code class="name">var <span class="ident">weights</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="ednaml.models.MultiClassificationResnet.MultiClassificationResnet.base_forward"><code class="name flex">
<span>def <span class="ident">base_forward</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def base_forward(self, x):
    features = self.gap(self.base(x))
    features = features.view(features.shape[0], -1)
    features = self.emb_linear(features)
    return features</code></pre>
</details>
</dd>
<dt id="ednaml.models.MultiClassificationResnet.MultiClassificationResnet.build_softmax"><code class="name flex">
<span>def <span class="ident">build_softmax</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Build the softmax layers, using info either in self.softmax_dimensions or by combining metadata info of labelname-&gt;numclasses and the outputclassnames</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_softmax(self, **kwargs):
    &#34;&#34;&#34;Build the softmax layers, using info either in self.softmax_dimensions or by combining metadata info of labelname-&gt;numclasses and the outputclassnames
    &#34;&#34;&#34;
    # NOTE, for re-id type models...multiclassification model will anyway yield the features with softmax outputs, so we don&#39;t have to worry about that...
    # For pure-reid model, probably best to use ClassificationResNet and modify to use no softmax...TODO this is a future step...
    tsoftmax = [None] * self.number_outputs
    for idx, fc_dimension in enumerate(self.softmax_dimensions):
        tsoftmax[idx] = nn.Linear(
            self.embedding_dimensions, fc_dimension, bias=False
        )
        tsoftmax[idx].apply(self.weights_init_softmax)
    self.softmax = nn.ModuleList(tsoftmax)</code></pre>
</details>
</dd>
<dt id="ednaml.models.MultiClassificationResnet.MultiClassificationResnet.forward_impl"><code class="name flex">
<span>def <span class="ident">forward_impl</span></span>(<span>self, x, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_impl(self, x, **kwargs):
    features = self.base_forward(x)

    # if self.feat_norm is not None: &lt;-- no need, identity
    features = self.feat_norm(features)

    softmax_logits = [None] * self.number_outputs
    for idx, softmaxlayer in enumerate(self.softmax):
        softmax_logits[idx] = softmaxlayer(features)
    return (
        softmax_logits,
        features,
        [],
    )  # soft logits are the softmax logits we will use to for training. We can use features to store the historical probability????</code></pre>
</details>
</dd>
<dt id="ednaml.models.MultiClassificationResnet.MultiClassificationResnet.model_attributes_setup"><code class="name flex">
<span>def <span class="ident">model_attributes_setup</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def model_attributes_setup(self, **kwargs):

    self.embedding_dimensions = kwargs.get(&#34;embedding_dimensions&#34;, None)
    if self.normalization == &#34;&#34;:
        self.normalization = None

    self.number_outputs = kwargs.get(&#34;number_outputs&#34;, 1)

    outputs = kwargs.get(
        &#34;outputs&#34;,
        [
            {
                &#34;name&#34;: self._internal_name_counter(),
                &#34;label&#34;: &#34;color&#34;,
                &#34;dimensions&#34;: None,
            }
        ],
    )

    if len(outputs) != self.number_outputs:
        raise ValueError(
            &#34;Mismatch in length of outputs %i and number of outputs %i&#34;
            % (len(outputs), self.number_outputs)
        )

    self.softmax_dimensions = [None] * self.number_outputs
    self.output_names = [None] * self.number_outputs
    self.output_labels = [None] * self.number_outputs

    for idx, output_details in enumerate(outputs):
        self.softmax_dimensions[idx] = output_details.get(&#34;dimensions&#34;, None)
        self.output_names[idx] = output_details.get(
            &#34;name&#34;, self._internal_name_counter()
        )
        self.output_labels[idx] = output_details[&#34;label&#34;]
        if self.softmax_dimensions[idx] is None:
            self.softmax_dimensions[idx] = self.metadata.getLabelDimensions(
                self.output_labels[idx]
            )
    self.base = None
    self.gap = None
    self.emb_linear = None
    self.feat_norm = None
    self.softmax = None</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="ednaml.models.ClassificationResnet.ClassificationResnet" href="ClassificationResnet.html#ednaml.models.ClassificationResnet.ClassificationResnet">ClassificationResnet</a></b></code>:
<ul class="hlist">
<li><code><a title="ednaml.models.ClassificationResnet.ClassificationResnet.build_base" href="ClassificationResnet.html#ednaml.models.ClassificationResnet.ClassificationResnet.build_base">build_base</a></code></li>
<li><code><a title="ednaml.models.ClassificationResnet.ClassificationResnet.forward" href="ModelAbstract.html#ednaml.models.ModelAbstract.ModelAbstract.forward">forward</a></code></li>
<li><code><a title="ednaml.models.ClassificationResnet.ClassificationResnet.weights_init_softmax" href="ModelAbstract.html#ednaml.models.ModelAbstract.ModelAbstract.weights_init_softmax">weights_init_softmax</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="ednaml.models" href="index.html">ednaml.models</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="ednaml.models.MultiClassificationResnet.MultiClassificationResnet" href="#ednaml.models.MultiClassificationResnet.MultiClassificationResnet">MultiClassificationResnet</a></code></h4>
<ul class="">
<li><code><a title="ednaml.models.MultiClassificationResnet.MultiClassificationResnet.base_forward" href="#ednaml.models.MultiClassificationResnet.MultiClassificationResnet.base_forward">base_forward</a></code></li>
<li><code><a title="ednaml.models.MultiClassificationResnet.MultiClassificationResnet.build_softmax" href="#ednaml.models.MultiClassificationResnet.MultiClassificationResnet.build_softmax">build_softmax</a></code></li>
<li><code><a title="ednaml.models.MultiClassificationResnet.MultiClassificationResnet.forward_impl" href="#ednaml.models.MultiClassificationResnet.MultiClassificationResnet.forward_impl">forward_impl</a></code></li>
<li><code><a title="ednaml.models.MultiClassificationResnet.MultiClassificationResnet.metadata" href="#ednaml.models.MultiClassificationResnet.MultiClassificationResnet.metadata">metadata</a></code></li>
<li><code><a title="ednaml.models.MultiClassificationResnet.MultiClassificationResnet.model_arch" href="#ednaml.models.MultiClassificationResnet.MultiClassificationResnet.model_arch">model_arch</a></code></li>
<li><code><a title="ednaml.models.MultiClassificationResnet.MultiClassificationResnet.model_attributes_setup" href="#ednaml.models.MultiClassificationResnet.MultiClassificationResnet.model_attributes_setup">model_attributes_setup</a></code></li>
<li><code><a title="ednaml.models.MultiClassificationResnet.MultiClassificationResnet.model_base" href="#ednaml.models.MultiClassificationResnet.MultiClassificationResnet.model_base">model_base</a></code></li>
<li><code><a title="ednaml.models.MultiClassificationResnet.MultiClassificationResnet.model_name" href="#ednaml.models.MultiClassificationResnet.MultiClassificationResnet.model_name">model_name</a></code></li>
<li><code><a title="ednaml.models.MultiClassificationResnet.MultiClassificationResnet.normalization" href="#ednaml.models.MultiClassificationResnet.MultiClassificationResnet.normalization">normalization</a></code></li>
<li><code><a title="ednaml.models.MultiClassificationResnet.MultiClassificationResnet.number_outputs" href="#ednaml.models.MultiClassificationResnet.MultiClassificationResnet.number_outputs">number_outputs</a></code></li>
<li><code><a title="ednaml.models.MultiClassificationResnet.MultiClassificationResnet.output_labels" href="#ednaml.models.MultiClassificationResnet.MultiClassificationResnet.output_labels">output_labels</a></code></li>
<li><code><a title="ednaml.models.MultiClassificationResnet.MultiClassificationResnet.output_names" href="#ednaml.models.MultiClassificationResnet.MultiClassificationResnet.output_names">output_names</a></code></li>
<li><code><a title="ednaml.models.MultiClassificationResnet.MultiClassificationResnet.parameter_groups" href="#ednaml.models.MultiClassificationResnet.MultiClassificationResnet.parameter_groups">parameter_groups</a></code></li>
<li><code><a title="ednaml.models.MultiClassificationResnet.MultiClassificationResnet.secondary_outputs" href="#ednaml.models.MultiClassificationResnet.MultiClassificationResnet.secondary_outputs">secondary_outputs</a></code></li>
<li><code><a title="ednaml.models.MultiClassificationResnet.MultiClassificationResnet.softmax_dimensions" href="#ednaml.models.MultiClassificationResnet.MultiClassificationResnet.softmax_dimensions">softmax_dimensions</a></code></li>
<li><code><a title="ednaml.models.MultiClassificationResnet.MultiClassificationResnet.weights" href="#ednaml.models.MultiClassificationResnet.MultiClassificationResnet.weights">weights</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>