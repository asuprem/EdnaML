<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>ednaml.models API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ednaml.models</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from ednaml.utils import locate_class
from ednaml.utils.LabelMetadata import LabelMetadata


def classification_model_builder(
    arch,
    base,
    weights=None,
    normalization=None,
    metadata: LabelMetadata = None,
    parameter_groups=None,
    **kwargs
):
    &#34;&#34;&#34;Corroborative/Colaborative/Complementary Labeler Model Builder

    This builds a model for colabeler. Refer to paper [] for general construction. The model contains:
        * Architecture core
        * Convolutional attention
        * Spatial average pooling
        * FC-Layer for embedding dimensionality change
        * Normalization layer
        * Softmax FC Layers

    Args:
        arch (str): Ther architecture to use. The string &#34;Base&#34; is added after architecture (e.g. &#34;Resnet&#34;, &#34;Inception&#34;). Only &#34;Resnet&#34; is currently supported.
        base (str): The architecture subtype, e.g. &#34;resnet50&#34;, &#34;resnet18&#34;
        weights (str): Local path to weights file for the architecture core, e.g. pretrained resnet50 weights path.
        normalization (str): Normalization layer for reid-model. Can be None. Supported normalizations: [&#34;bn&#34;, &#34;l2&#34;, &#34;in&#34;, &#34;gn&#34;, &#34;ln&#34;]
    
    KWARGS (dict):
        embedding_dimension (int): Dimensions for the feature embedding. Leave empty if feature dimensions should be same as architecture core output (e.g. resnet50 base model has 2048-dim feature outputs). If providing a value, it should be less than the architecture core&#39;s base feature dimensions
        metadata: dict of class-name: num-classes to infer size of softmax dimensions.


    Returns:
        Torch Model: A Torch classification model

    &#34;&#34;&#34;
    if arch != &#34;ClassificationResnet&#34;:
        raise NotImplementedError()
    archbase = locate_class(subpackage=&#34;models&#34;, classpackage=arch, classfile=arch)
    # Extract the dimensions from the classes metadata provided by EdnaML
    kwargs[&#34;softmax_dimensions&#34;] = metadata.getLabelDimensions()

    model = archbase(
        base=base,
        weights=weights,
        normalization=normalization,
        metadata=metadata,
        parameter_groups=parameter_groups,
        **kwargs
    )
    return model


def multiclassification_model_builder(
    arch, base, weights=None, normalization=None, metadata=None, parameter_groups=None, **kwargs
):
    &#34;&#34;&#34;Multiclassification model builder. This builds a model with a single backbone, and multiple classification FC layers.

    The model contains:
        * Architecture core
        * Convolutional attention
        * Spatial average pooling
        * Normalization layer
        * Softmax FC Layers

    Args:
        arch (str): Ther architecture to use. The string &#34;Base&#34; is added after architecture (e.g. &#34;MultiClassificationResnet&#34;, &#34;MultiClassificationInception&#34;). Only &#34;MultiClassificationResnet&#34; is currently supported.
        base (str): The architecture subtype, e.g. &#34;resnet50&#34;, &#34;resnet18&#34;
        weights (str): Local path to weights file for the architecture core, e.g. pretrained resnet50 weights path.
        normalization (str): Normalization layer for reid-model. Can be None. Supported normalizations: [&#34;bn&#34;, &#34;l2&#34;, &#34;in&#34;, &#34;gn&#34;, &#34;ln&#34;]
        metadata (Dict[str, int]): Label names to num-classes dictionary

    Kwargs:
        - `number_outputs`: This is the number of classification outputs for the model
        - `outputs`: A list, each element is the i-th output&#39;s metadata
            - `dimensions`: This is the number of classes for this output. This can be left blank if you want EdnaML to infer the size from the `LABEL` parameter below. 
            - `name`: This is the name of this output. This is used by model_builder to keep track of output names and labels. If left blank, EdnaML will automatically name all outputs. Outputs are named because multiple outputs can track the same class, sometimes, e.g. in a contrastive non-weight sharing setting
            - `label`: This is the name of the label this output is tracking. &lt;span style=&#34;color:magenta; font-weight:bold&#34;&gt;THIS SHOULD CORRESPOND EXACTLY WITH `DATASET_ARGS.classificationclass`&lt;/span&gt; labels. 
    Returns:
        Torch Model: A Torch multiclassification model

    &#34;&#34;&#34;

    # make the MultiClassificationResnet, with resnet base, with multiple output fc layers
    if arch != &#34;MultiClassificationResnet&#34;:
        raise NotImplementedError()
    archbase = locate_class(subpackage=&#34;models&#34;, classpackage=arch, classfile=arch)

    model = archbase(
        base=base,
        weights=weights,
        normalization=normalization,
        metadata=metadata,
        parameter_groups=parameter_groups,
        **kwargs
    )
    return model


def multibranch_model_builder(
    arch, base, weights=None, normalization=None, metadata=None, parameter_groups=None,**kwargs
):
    &#34;&#34;&#34;Multibranch model builder builds a model with multiple branches, each with their set of outputs.

    The model contains:
        * Architecture core
        * Convolutional attention
        * Spatial average pooling
        * Normalization layer
        * Softmax FC Layers

    Args:
        arch (str): Ther architecture to use. The string &#34;Base&#34; is added after architecture (e.g. &#34;MultiClassificationResnet&#34;, &#34;MultiClassificationInception&#34;). Only &#34;MultiClassificationResnet&#34; is currently supported.
        base (str): The architecture subtype, e.g. &#34;resnet50&#34;, &#34;resnet18&#34;
        weights (str): Local path to weights file for the architecture core, e.g. pretrained resnet50 weights path.
        normalization (str): Normalization layer for reid-model. Can be None. Supported normalizations: [&#34;bn&#34;, &#34;l2&#34;, &#34;in&#34;, &#34;gn&#34;, &#34;ln&#34;]
        metadata (Dict[str, int]): Label names to num-classes dictionary

    Kwargs:
        - `number_branches`: This is the number of branches for the model
        - `branches`: A list, each element is the i-th branch&#39;s metadata
            - `name`: This is the name of this branch. This is used by model_builder to keep track of output names and labels. 
            - `number_outputs`: This is the number of classification outputs for this branch
            - `outputs`: A list, each element is the j-th output&#39;s metadata for this branch
                - `dimensions`: This is the number of classes for this output. This can be left blank if you want EdnaML to infer the size from the `LABEL` parameter below. 
                - `name`: This is the name of this output. 
                - `label`: This is the name of the label this output is tracking. &lt;span style=&#34;color:magenta; font-weight:bold&#34;&gt;THIS SHOULD CORRESPOND EXACTLY WITH `DATASET_ARGS.classificationclass`&lt;/span&gt; labels.
        - `fuse`: **Bool**. Whether branch outputs are going to be fused
        - `fuse_outputs`: List of output names (not branch names) that are fused
        - `fuse_dimensions`: The dimensions of the fused output. If left blank, EdnaML will infer from `fuse_label`
        - `fuse_label`: The label that tracks the fused output 

    Returns:
        Torch Model: A Torch Re-ID model

    &#34;&#34;&#34;

    # make the MultiClassificationResnet, with resnet base, with multiple output fc layers
    if arch != &#34;MultiBranchResnet&#34;:
        raise NotImplementedError()
    archbase = locate_class(subpackage=&#34;models&#34;, classpackage=arch, classfile=arch)


    model = archbase(
        base=base,
        weights=weights,
        normalization=normalization,
        metadata=metadata,
        parameter_groups=parameter_groups,
        **kwargs
    )
    return model</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="ednaml.models.ClassificationResnet" href="ClassificationResnet.html">ednaml.models.ClassificationResnet</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="ednaml.models.ModelAbstract" href="ModelAbstract.html">ednaml.models.ModelAbstract</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="ednaml.models.MultiBranchResnet" href="MultiBranchResnet.html">ednaml.models.MultiBranchResnet</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="ednaml.models.MultiClassificationResnet" href="MultiClassificationResnet.html">ednaml.models.MultiClassificationResnet</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="ednaml.models.classification_model_builder"><code class="name flex">
<span>def <span class="ident">classification_model_builder</span></span>(<span>arch, base, weights=None, normalization=None, metadata: <a title="ednaml.utils.LabelMetadata.LabelMetadata" href="../utils/LabelMetadata.html#ednaml.utils.LabelMetadata.LabelMetadata">LabelMetadata</a> = None, parameter_groups=None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Corroborative/Colaborative/Complementary Labeler Model Builder</p>
<p>This builds a model for colabeler. Refer to paper [] for general construction. The model contains:
* Architecture core
* Convolutional attention
* Spatial average pooling
* FC-Layer for embedding dimensionality change
* Normalization layer
* Softmax FC Layers</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>arch</code></strong> :&ensp;<code>str</code></dt>
<dd>Ther architecture to use. The string "Base" is added after architecture (e.g. "Resnet", "Inception"). Only "Resnet" is currently supported.</dd>
<dt><strong><code>base</code></strong> :&ensp;<code>str</code></dt>
<dd>The architecture subtype, e.g. "resnet50", "resnet18"</dd>
<dt><strong><code>weights</code></strong> :&ensp;<code>str</code></dt>
<dd>Local path to weights file for the architecture core, e.g. pretrained resnet50 weights path.</dd>
<dt><strong><code>normalization</code></strong> :&ensp;<code>str</code></dt>
<dd>Normalization layer for reid-model. Can be None. Supported normalizations: ["bn", "l2", "in", "gn", "ln"]</dd>
</dl>
<p>KWARGS (dict):
embedding_dimension (int): Dimensions for the feature embedding. Leave empty if feature dimensions should be same as architecture core output (e.g. resnet50 base model has 2048-dim feature outputs). If providing a value, it should be less than the architecture core's base feature dimensions
metadata: dict of class-name: num-classes to infer size of softmax dimensions.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Torch Model</code></dt>
<dd>A Torch classification model</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def classification_model_builder(
    arch,
    base,
    weights=None,
    normalization=None,
    metadata: LabelMetadata = None,
    parameter_groups=None,
    **kwargs
):
    &#34;&#34;&#34;Corroborative/Colaborative/Complementary Labeler Model Builder

    This builds a model for colabeler. Refer to paper [] for general construction. The model contains:
        * Architecture core
        * Convolutional attention
        * Spatial average pooling
        * FC-Layer for embedding dimensionality change
        * Normalization layer
        * Softmax FC Layers

    Args:
        arch (str): Ther architecture to use. The string &#34;Base&#34; is added after architecture (e.g. &#34;Resnet&#34;, &#34;Inception&#34;). Only &#34;Resnet&#34; is currently supported.
        base (str): The architecture subtype, e.g. &#34;resnet50&#34;, &#34;resnet18&#34;
        weights (str): Local path to weights file for the architecture core, e.g. pretrained resnet50 weights path.
        normalization (str): Normalization layer for reid-model. Can be None. Supported normalizations: [&#34;bn&#34;, &#34;l2&#34;, &#34;in&#34;, &#34;gn&#34;, &#34;ln&#34;]
    
    KWARGS (dict):
        embedding_dimension (int): Dimensions for the feature embedding. Leave empty if feature dimensions should be same as architecture core output (e.g. resnet50 base model has 2048-dim feature outputs). If providing a value, it should be less than the architecture core&#39;s base feature dimensions
        metadata: dict of class-name: num-classes to infer size of softmax dimensions.


    Returns:
        Torch Model: A Torch classification model

    &#34;&#34;&#34;
    if arch != &#34;ClassificationResnet&#34;:
        raise NotImplementedError()
    archbase = locate_class(subpackage=&#34;models&#34;, classpackage=arch, classfile=arch)
    # Extract the dimensions from the classes metadata provided by EdnaML
    kwargs[&#34;softmax_dimensions&#34;] = metadata.getLabelDimensions()

    model = archbase(
        base=base,
        weights=weights,
        normalization=normalization,
        metadata=metadata,
        parameter_groups=parameter_groups,
        **kwargs
    )
    return model</code></pre>
</details>
</dd>
<dt id="ednaml.models.multibranch_model_builder"><code class="name flex">
<span>def <span class="ident">multibranch_model_builder</span></span>(<span>arch, base, weights=None, normalization=None, metadata=None, parameter_groups=None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Multibranch model builder builds a model with multiple branches, each with their set of outputs.</p>
<p>The model contains:
* Architecture core
* Convolutional attention
* Spatial average pooling
* Normalization layer
* Softmax FC Layers</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>arch</code></strong> :&ensp;<code>str</code></dt>
<dd>Ther architecture to use. The string "Base" is added after architecture (e.g. "MultiClassificationResnet", "MultiClassificationInception"). Only "MultiClassificationResnet" is currently supported.</dd>
<dt><strong><code>base</code></strong> :&ensp;<code>str</code></dt>
<dd>The architecture subtype, e.g. "resnet50", "resnet18"</dd>
<dt><strong><code>weights</code></strong> :&ensp;<code>str</code></dt>
<dd>Local path to weights file for the architecture core, e.g. pretrained resnet50 weights path.</dd>
<dt><strong><code>normalization</code></strong> :&ensp;<code>str</code></dt>
<dd>Normalization layer for reid-model. Can be None. Supported normalizations: ["bn", "l2", "in", "gn", "ln"]</dd>
<dt><strong><code>metadata</code></strong> :&ensp;<code>Dict[str, int]</code></dt>
<dd>Label names to num-classes dictionary</dd>
</dl>
<h2 id="kwargs">Kwargs</h2>
<ul>
<li><code>number_branches</code>: This is the number of branches for the model</li>
<li><code>branches</code>: A list, each element is the i-th branch's metadata<ul>
<li><code>name</code>: This is the name of this branch. This is used by model_builder to keep track of output names and labels. </li>
<li><code>number_outputs</code>: This is the number of classification outputs for this branch</li>
<li><code>outputs</code>: A list, each element is the j-th output's metadata for this branch<ul>
<li><code>dimensions</code>: This is the number of classes for this output. This can be left blank if you want EdnaML to infer the size from the <code>LABEL</code> parameter below. </li>
<li><code>name</code>: This is the name of this output. </li>
<li><code>label</code>: This is the name of the label this output is tracking. <span style="color:magenta; font-weight:bold">THIS SHOULD CORRESPOND EXACTLY WITH <code>DATASET_ARGS.classificationclass</code></span> labels.</li>
</ul>
</li>
</ul>
</li>
<li><code>fuse</code>: <strong>Bool</strong>. Whether branch outputs are going to be fused</li>
<li><code>fuse_outputs</code>: List of output names (not branch names) that are fused</li>
<li><code>fuse_dimensions</code>: The dimensions of the fused output. If left blank, EdnaML will infer from <code>fuse_label</code></li>
<li><code>fuse_label</code>: The label that tracks the fused output </li>
</ul>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Torch Model</code></dt>
<dd>A Torch Re-ID model</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def multibranch_model_builder(
    arch, base, weights=None, normalization=None, metadata=None, parameter_groups=None,**kwargs
):
    &#34;&#34;&#34;Multibranch model builder builds a model with multiple branches, each with their set of outputs.

    The model contains:
        * Architecture core
        * Convolutional attention
        * Spatial average pooling
        * Normalization layer
        * Softmax FC Layers

    Args:
        arch (str): Ther architecture to use. The string &#34;Base&#34; is added after architecture (e.g. &#34;MultiClassificationResnet&#34;, &#34;MultiClassificationInception&#34;). Only &#34;MultiClassificationResnet&#34; is currently supported.
        base (str): The architecture subtype, e.g. &#34;resnet50&#34;, &#34;resnet18&#34;
        weights (str): Local path to weights file for the architecture core, e.g. pretrained resnet50 weights path.
        normalization (str): Normalization layer for reid-model. Can be None. Supported normalizations: [&#34;bn&#34;, &#34;l2&#34;, &#34;in&#34;, &#34;gn&#34;, &#34;ln&#34;]
        metadata (Dict[str, int]): Label names to num-classes dictionary

    Kwargs:
        - `number_branches`: This is the number of branches for the model
        - `branches`: A list, each element is the i-th branch&#39;s metadata
            - `name`: This is the name of this branch. This is used by model_builder to keep track of output names and labels. 
            - `number_outputs`: This is the number of classification outputs for this branch
            - `outputs`: A list, each element is the j-th output&#39;s metadata for this branch
                - `dimensions`: This is the number of classes for this output. This can be left blank if you want EdnaML to infer the size from the `LABEL` parameter below. 
                - `name`: This is the name of this output. 
                - `label`: This is the name of the label this output is tracking. &lt;span style=&#34;color:magenta; font-weight:bold&#34;&gt;THIS SHOULD CORRESPOND EXACTLY WITH `DATASET_ARGS.classificationclass`&lt;/span&gt; labels.
        - `fuse`: **Bool**. Whether branch outputs are going to be fused
        - `fuse_outputs`: List of output names (not branch names) that are fused
        - `fuse_dimensions`: The dimensions of the fused output. If left blank, EdnaML will infer from `fuse_label`
        - `fuse_label`: The label that tracks the fused output 

    Returns:
        Torch Model: A Torch Re-ID model

    &#34;&#34;&#34;

    # make the MultiClassificationResnet, with resnet base, with multiple output fc layers
    if arch != &#34;MultiBranchResnet&#34;:
        raise NotImplementedError()
    archbase = locate_class(subpackage=&#34;models&#34;, classpackage=arch, classfile=arch)


    model = archbase(
        base=base,
        weights=weights,
        normalization=normalization,
        metadata=metadata,
        parameter_groups=parameter_groups,
        **kwargs
    )
    return model</code></pre>
</details>
</dd>
<dt id="ednaml.models.multiclassification_model_builder"><code class="name flex">
<span>def <span class="ident">multiclassification_model_builder</span></span>(<span>arch, base, weights=None, normalization=None, metadata=None, parameter_groups=None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Multiclassification model builder. This builds a model with a single backbone, and multiple classification FC layers.</p>
<p>The model contains:
* Architecture core
* Convolutional attention
* Spatial average pooling
* Normalization layer
* Softmax FC Layers</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>arch</code></strong> :&ensp;<code>str</code></dt>
<dd>Ther architecture to use. The string "Base" is added after architecture (e.g. "MultiClassificationResnet", "MultiClassificationInception"). Only "MultiClassificationResnet" is currently supported.</dd>
<dt><strong><code>base</code></strong> :&ensp;<code>str</code></dt>
<dd>The architecture subtype, e.g. "resnet50", "resnet18"</dd>
<dt><strong><code>weights</code></strong> :&ensp;<code>str</code></dt>
<dd>Local path to weights file for the architecture core, e.g. pretrained resnet50 weights path.</dd>
<dt><strong><code>normalization</code></strong> :&ensp;<code>str</code></dt>
<dd>Normalization layer for reid-model. Can be None. Supported normalizations: ["bn", "l2", "in", "gn", "ln"]</dd>
<dt><strong><code>metadata</code></strong> :&ensp;<code>Dict[str, int]</code></dt>
<dd>Label names to num-classes dictionary</dd>
</dl>
<h2 id="kwargs">Kwargs</h2>
<ul>
<li><code>number_outputs</code>: This is the number of classification outputs for the model</li>
<li><code>outputs</code>: A list, each element is the i-th output's metadata<ul>
<li><code>dimensions</code>: This is the number of classes for this output. This can be left blank if you want EdnaML to infer the size from the <code>LABEL</code> parameter below. </li>
<li><code>name</code>: This is the name of this output. This is used by model_builder to keep track of output names and labels. If left blank, EdnaML will automatically name all outputs. Outputs are named because multiple outputs can track the same class, sometimes, e.g. in a contrastive non-weight sharing setting</li>
<li><code>label</code>: This is the name of the label this output is tracking. <span style="color:magenta; font-weight:bold">THIS SHOULD CORRESPOND EXACTLY WITH <code>DATASET_ARGS.classificationclass</code></span> labels. </li>
</ul>
</li>
</ul>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Torch Model</code></dt>
<dd>A Torch multiclassification model</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def multiclassification_model_builder(
    arch, base, weights=None, normalization=None, metadata=None, parameter_groups=None, **kwargs
):
    &#34;&#34;&#34;Multiclassification model builder. This builds a model with a single backbone, and multiple classification FC layers.

    The model contains:
        * Architecture core
        * Convolutional attention
        * Spatial average pooling
        * Normalization layer
        * Softmax FC Layers

    Args:
        arch (str): Ther architecture to use. The string &#34;Base&#34; is added after architecture (e.g. &#34;MultiClassificationResnet&#34;, &#34;MultiClassificationInception&#34;). Only &#34;MultiClassificationResnet&#34; is currently supported.
        base (str): The architecture subtype, e.g. &#34;resnet50&#34;, &#34;resnet18&#34;
        weights (str): Local path to weights file for the architecture core, e.g. pretrained resnet50 weights path.
        normalization (str): Normalization layer for reid-model. Can be None. Supported normalizations: [&#34;bn&#34;, &#34;l2&#34;, &#34;in&#34;, &#34;gn&#34;, &#34;ln&#34;]
        metadata (Dict[str, int]): Label names to num-classes dictionary

    Kwargs:
        - `number_outputs`: This is the number of classification outputs for the model
        - `outputs`: A list, each element is the i-th output&#39;s metadata
            - `dimensions`: This is the number of classes for this output. This can be left blank if you want EdnaML to infer the size from the `LABEL` parameter below. 
            - `name`: This is the name of this output. This is used by model_builder to keep track of output names and labels. If left blank, EdnaML will automatically name all outputs. Outputs are named because multiple outputs can track the same class, sometimes, e.g. in a contrastive non-weight sharing setting
            - `label`: This is the name of the label this output is tracking. &lt;span style=&#34;color:magenta; font-weight:bold&#34;&gt;THIS SHOULD CORRESPOND EXACTLY WITH `DATASET_ARGS.classificationclass`&lt;/span&gt; labels. 
    Returns:
        Torch Model: A Torch multiclassification model

    &#34;&#34;&#34;

    # make the MultiClassificationResnet, with resnet base, with multiple output fc layers
    if arch != &#34;MultiClassificationResnet&#34;:
        raise NotImplementedError()
    archbase = locate_class(subpackage=&#34;models&#34;, classpackage=arch, classfile=arch)

    model = archbase(
        base=base,
        weights=weights,
        normalization=normalization,
        metadata=metadata,
        parameter_groups=parameter_groups,
        **kwargs
    )
    return model</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="ednaml" href="../index.html">ednaml</a></code></li>
</ul>
</li>
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="ednaml.models.ClassificationResnet" href="ClassificationResnet.html">ednaml.models.ClassificationResnet</a></code></li>
<li><code><a title="ednaml.models.ModelAbstract" href="ModelAbstract.html">ednaml.models.ModelAbstract</a></code></li>
<li><code><a title="ednaml.models.MultiBranchResnet" href="MultiBranchResnet.html">ednaml.models.MultiBranchResnet</a></code></li>
<li><code><a title="ednaml.models.MultiClassificationResnet" href="MultiClassificationResnet.html">ednaml.models.MultiClassificationResnet</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="ednaml.models.classification_model_builder" href="#ednaml.models.classification_model_builder">classification_model_builder</a></code></li>
<li><code><a title="ednaml.models.multibranch_model_builder" href="#ednaml.models.multibranch_model_builder">multibranch_model_builder</a></code></li>
<li><code><a title="ednaml.models.multiclassification_model_builder" href="#ednaml.models.multiclassification_model_builder">multiclassification_model_builder</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>