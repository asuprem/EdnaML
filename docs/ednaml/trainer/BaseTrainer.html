<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>ednaml.trainer.BaseTrainer API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ednaml.trainer.BaseTrainer</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import json, logging, os, shutil
from logging import Logger
from typing import Dict, List

import torch
from torch.utils.data import DataLoader
import ednaml.loss.builders
from ednaml.config.EdnaMLConfig import EdnaMLConfig
from ednaml.crawlers import Crawler
from ednaml.models.ModelAbstract import ModelAbstract
from ednaml.utils.LabelMetadata import LabelMetadata


class BaseTrainer:
    model: ModelAbstract
    loss_fn: Dict[str, ednaml.loss.builders.LossBuilder]  # output-name: lossBuilder
    optimizer: Dict[str, torch.optim.Optimizer]  
    loss_optimizer = Dict[str, List[torch.optim.Optimizer]] 
    scheduler: Dict[str, torch.optim.lr_scheduler._LRScheduler]  
    loss_scheduler: Dict[str, List[torch.optim.lr_scheduler._LRScheduler]] 

    skipeval: bool
    train_loader: DataLoader
    test_loader: DataLoader

    epochs: int
    logger: Logger
    global_batch: int
    global_epoch: int
    num_losses: int
    losses: Dict[str, List[int]] 
    metadata: Dict[str, str]
    labelMetadata: LabelMetadata
    logger: logging.Logger

    def __init__(
        self,
        model: ModelAbstract,
        loss_fn: List[ednaml.loss.builders.LossBuilder],
        optimizer: List[torch.optim.Optimizer],
        loss_optimizer: List[torch.optim.Optimizer],
        scheduler: List[torch.optim.lr_scheduler._LRScheduler],
        loss_scheduler: List[torch.optim.lr_scheduler._LRScheduler],
        train_loader: DataLoader,
        test_loader: DataLoader,
        epochs: int,
        skipeval: bool,
        logger: Logger,
        crawler: Crawler,
        config: EdnaMLConfig,
        labels: LabelMetadata,
        **kwargs
    ):

        self.model = model
        self.parameter_groups = list(self.model.parameter_groups.keys())
        self.loss_fn_order = {
            idx: lossbuilder.loss_labelname for idx, lossbuilder in enumerate(loss_fn)
        }
        self.loss_fn = {
            lossbuilder.loss_labelname: lossbuilder for lossbuilder in loss_fn
        }
        self.num_losses = len(self.loss_fn)
        self.losses = {lossname: [] for lossname in self.loss_fn}
        self.loss_optimizer = {
            self.loss_fn_order[idx]: loss_optimizer_content
            for idx, loss_optimizer_content in enumerate(loss_optimizer)
        }
        if type(loss_scheduler) is list:
            self.loss_scheduler = {
                self.loss_fn_order[idx]: loss_scheduler_content
                for idx, loss_scheduler_content in enumerate(loss_scheduler)
            }
        else:
            self.loss_scheduler = {
                self.loss_fn_order[idx]: loss_scheduler
                for idx in range(self.num_losses)
            }

        self.optimizer = {self.parameter_groups[idx]:optimizer_item for idx, optimizer_item in enumerate(optimizer)}
        self.scheduler = {self.parameter_groups[idx]:scheduler_item for idx, scheduler_item in enumerate(scheduler)}
        self.skipeval = skipeval
        self.train_loader = train_loader
        self.test_loader = test_loader

        self.epochs = epochs
        self.logger = logger

        self.global_batch = 0  # Current batch number in the epoch
        self.global_epoch = 0

        self.metadata = {}
        self.labelMetadata = labels
        self.crawler = crawler
        self.config = config

        self.buildMetadata(
            crawler=crawler.classes, config=json.loads(config.export(&#34;json&#34;))
        )

    def buildMetadata(self, **kwargs):
        for keys in kwargs:
            self.metadata[keys] = kwargs.get(keys)

    def setup(
        self,
        step_verbose: int = 5,
        save_frequency: int = 5,
        test_frequency: int = 5,
        save_directory: str = &#34;./checkpoint/&#34;,
        save_backup: bool = False,
        backup_directory: str = None,
        gpus: int = 1,
        fp16: bool = False,
        model_save_name: str = None,
        logger_file: str = None,
    ):
        self.step_verbose = step_verbose
        self.save_frequency = save_frequency
        self.test_frequency = test_frequency
        self.save_directory = save_directory
        self.backup_directory = None
        self.model_save_name = model_save_name
        self.logger_file = logger_file
        self.save_backup = save_backup
        if self.save_backup:
            self.backup_directory = backup_directory
            os.makedirs(self.backup_directory, exist_ok=True)
        os.makedirs(self.save_directory, exist_ok=True)
        self.saveMetadata()

        self.gpus = gpus

        if self.gpus != 1:
            raise NotImplementedError()

        self.model.cuda()

        self.fp16 = fp16
        # if self.fp16 and self.apex is not None:
        #    self.model, self.optimizer = self.apex.amp.initialize(self.model, self.optimizer, opt_level=&#39;O1&#39;)

    def saveMetadata(self):
        print(&#34;NOT saving metadata. saveMetadata() function not set up.&#34;)

    def save(self):
        self.logger.info(&#34;Saving model, optimizer, and scheduler.&#34;)
        MODEL_SAVE = self.model_save_name + &#34;_epoch%i&#34; % self.global_epoch + &#34;.pth&#34;
        TRAINING_SAVE = (
            self.model_save_name + &#34;_epoch%i&#34; % self.global_epoch + &#34;_training.pth&#34;
        )

        save_dict = {}
        save_dict[&#34;optimizer&#34;] = {
            pgn:self.optimizer[pgn].state_dict() for pgn in self.parameter_groups
        }
        save_dict[&#34;scheduler&#34;] = {
            pgn:self.scheduler[pgn].state_dict() for pgn in self.parameter_groups
        }
        save_dict[&#34;loss_fn&#34;] = {
            lossname: self.loss_fn[lossname].state_dict() for lossname in self.loss_fn
        }
        save_dict[&#34;loss_optimizer&#34;] = {
            lossname: (
                self.loss_optimizer[lossname].state_dict()
                if self.loss_optimizer[lossname] is not None
                else None
            )
            for lossname in self.loss_optimizer
        }
        save_dict[&#34;loss_scheduler&#34;] = {
            lossname: (
                self.loss_scheduler[lossname].state_dict()
                if self.loss_scheduler[lossname] is not None
                else None
            )
            for lossname in self.loss_scheduler
        }

        # save_dict[&#34;loss_optimizer&#34;] = [self.loss_optimizer[idx].state_dict() if self.loss_optimizer[idx] is not None else None for idx in range(self.num_losses)]
        # save_dict[&#34;loss_scheduler&#34;] = [self.loss_scheduler[idx].state_dict() if self.loss_scheduler[idx] is not None else None for idx in range(self.num_losses)]

        torch.save(
            self.model.state_dict(), os.path.join(self.save_directory, MODEL_SAVE)
        )
        torch.save(save_dict, os.path.join(self.save_directory, TRAINING_SAVE))

        if self.save_backup:
            shutil.copy2(
                os.path.join(self.save_directory, MODEL_SAVE), self.backup_directory
            )
            shutil.copy2(
                os.path.join(self.save_directory, TRAINING_SAVE), self.backup_directory
            )

            self.logger.info(
                &#34;Performing drive backup of model, optimizer, and scheduler.&#34;
            )

            LOGGER_SAVE = os.path.join(self.backup_directory, self.logger_file)
            if os.path.exists(LOGGER_SAVE):
                os.remove(LOGGER_SAVE)
            shutil.copy2(
                os.path.join(self.save_directory, self.logger_file), LOGGER_SAVE
            )

    def load(self, load_epoch):
        self.logger.info(
            &#34;Resuming training from epoch %i. Loading saved state from %i&#34;
            % (load_epoch + 1, load_epoch)
        )
        model_load = self.model_save_name + &#34;_epoch%i&#34; % load_epoch + &#34;.pth&#34;
        training_load = self.model_save_name + &#34;_epoch%i&#34; % load_epoch + &#34;_training.pth&#34;

        if self.save_backup:
            self.logger.info(
                &#34;Loading model, optimizer, and scheduler from drive backup.&#34;
            )
            model_load_path = os.path.join(self.backup_directory, model_load)
            training_load_path = os.path.join(self.backup_directory, training_load)

        else:
            self.logger.info(
                &#34;Loading model, optimizer, and scheduler from local backup.&#34;
            )
            model_load_path = os.path.join(self.save_directory, model_load)
            training_load_path = os.path.join(self.save_directory, training_load)

        self.model.load_state_dict(torch.load(model_load_path))
        self.logger.info(&#34;Finished loading model state_dict from %s&#34; % model_load_path)

        checkpoint = torch.load(training_load_path)
        for pgn in self.parameter_groups:
            self.optimizer[pgn].load_state_dict(
                checkpoint[&#34;optimizer&#34;][pgn]
            )

            self.scheduler[pgn].load_state_dict(
                checkpoint[&#34;scheduler&#34;][pgn]
            )
        self.logger.info(
            &#34;Finished loading optimizer state_dict from %s&#34; % training_load_path
        )
        self.logger.info(
            &#34;Finished loading scheduler state_dict from %s&#34; % training_load_path
        )

        for lossname in self.loss_fn:
            self.loss_fn[lossname].load_state_dict(checkpoint[&#34;loss_fn&#34;][lossname])
            if self.loss_optimizer[lossname] is not None:
                self.loss_optimizer[lossname].load_state_dict(
                    checkpoint[&#34;loss_optimizer&#34;][lossname]
                )
            if self.loss_scheduler[lossname] is not None:
                self.loss_scheduler[lossname].load_state_dict(
                    checkpoint[&#34;loss_scheduler&#34;][lossname]
                )

            self.logger.info(
                &#34;Finished loading loss state_dict from %s&#34; % training_load_path
            )

        # for idx in range(self.num_losses):
        #    if self.loss_optimizer[idx] is not None:
        #        self.loss_optimizer[idx].load_state_dict(checkpoint[&#34;loss_optimizer&#34;][idx])
        #    if self.loss_scheduler[idx] is not None:
        #        self.loss_scheduler[idx].load_state_dict(checkpoint[&#34;loss_scheduler&#34;][idx])

    def train(self, continue_epoch=0):
        self.logger.info(&#34;Starting training&#34;)
        self.logger.info(&#34;Logging to:\t%s&#34; % self.logger_file)
        self.logger.info(
            &#34;Models will be saved to local directory:\t%s&#34; % self.save_directory
        )
        if self.save_backup:
            self.logger.info(
                &#34;Models will be backed up to drive directory:\t%s&#34;
                % self.backup_directory
            )
        self.logger.info(
            &#34;Models will be saved with base name:\t%s_epoch[].pth&#34;
            % self.model_save_name
        )
        self.logger.info(
            &#34;Optimizers will be saved with base name:\t%s_epoch[]_optimizer.pth&#34;
            % self.model_save_name
        )
        self.logger.info(
            &#34;Schedulers will be saved with base name:\t%s_epoch[]_scheduler.pth&#34;
            % self.model_save_name
        )

        if continue_epoch &gt; 0:
            load_epoch = continue_epoch - 1
            self.load(load_epoch)

        if not self.skipeval:
            self.logger.info(&#34;Performing initial evaluation...&#34;)
            self.initial_evaluate()
        else:
            self.logger.info(&#34;Skipping initial evaluation.&#34;)

        self.logger.info(&#34;Starting training from %i&#34; % continue_epoch)
        for epoch in range(self.epochs):
            if epoch &gt;= continue_epoch:
                self.epoch_step(epoch)
            else:
                self.global_epoch = epoch + 1

    def initial_evaluate(self):
        &#34;&#34;&#34;Evaluation of model before we start training
        &#34;&#34;&#34;
        self.evaluate()

    def epoch_step(self, epoch):
        &#34;&#34;&#34;Trains model for an epoch.
        &#34;&#34;&#34;
        for batch in self.train_loader:
            if self.global_batch == 0:
                self.printOptimizerLearningRates()
            
            self.model.train()
            self.zeroGradOptimizers()
            self.zeroGradLossOptimizers()

            self.step(batch)
            
            self.global_batch+=1

            if (self.global_batch + 1) % self.step_verbose == 0:
                self.printStepInformation()

        self.global_batch = 0
        self.stepSchedulers()
        self.stepLossSchedulers()

        self.logger.info(
            &#34;{0} Completed epoch {1} {2}&#34;.format(&#34;*&#34; * 10, self.global_epoch, &#34;*&#34; * 10)
        )

        if self.global_epoch % self.test_frequency == 0:
            self.logger.info(&#34;Evaluating model at test-frequency&#34;)
            self.evaluate()
        if self.global_epoch % self.save_frequency == 0:
            self.logger.info(&#34;Saving model at save-frequency&#34;)
            self.save()
        self.global_epoch += 1

    def printStepInformation(self):
        loss_avg = 0.0
        for lossname in self.losses:
            loss_avg += (
                sum(self.losses[lossname][-self.step_verbose :])
                / self.step_verbose
            )
        loss_avg /= self.num_losses
        soft_avg = sum(self.softaccuracy[-100:]) / float(
            len(self.softaccuracy[-100:])
        )
        self.logger.info(
            &#34;Epoch{0}.{1}\tTotal Avg Loss: {2:.3f} Softmax: {3:.3f}&#34;.format(
                self.global_epoch, self.global_batch, loss_avg, soft_avg
            )
        )

    def evaluate(self):
        logit_labels, true_labels, features = self.evaluate_impl()
        return logit_labels, true_labels, self.crawler.classes, features

    def evaluate_impl(self):
        raise NotImplementedError


    def zeroGradOptimizers(self):
        for optim in self.optimizer:
            self.optimizer[optim].zero_grad()

    def zeroGradLossOptimizers(self):
        for lossname in self.loss_fn:
            if self.loss_optimizer[lossname] is not None:
                self.loss_optimizer[lossname].zero_grad()

    def stepOptimizers(self):
        for optim in self.optimizer:
            self.optimizer[optim].step()

    def stepLossOptimizers(self):
        for lossname in self.loss_fn:
            if (
                self.loss_optimizer[lossname] is not None
            ):  # In case loss object doesn;t have any parameters, this will be None. See optimizers.StandardLossOptimizer
                self.loss_optimizer[lossname].step()

    def stepSchedulers(self):
        for scheduler in self.scheduler:
            self.scheduler[scheduler].step()

    def stepLossSchedulers(self):
        for lossname in self.loss_fn:
            if self.loss_scheduler[lossname] is not None:
                self.loss_scheduler[lossname].step()

    def printOptimizerLearningRates(self):
        for param_group_name in self.optimizer:
            lrs = self.scheduler[param_group_name].get_last_lr()
            lrs = sum(lrs) / float(len(lrs))
            self.logger.info(
                    &#34;Parameter Group `{0}`: Starting epoch {1} with {2} steps and learning rate {3:2.5E}&#34;.format(
                        param_group_name,
                        self.global_epoch,
                        len(self.train_loader) - (len(self.train_loader) % 10),
                        lrs,
                    )
                )</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="ednaml.trainer.BaseTrainer.BaseTrainer"><code class="flex name class">
<span>class <span class="ident">BaseTrainer</span></span>
<span>(</span><span>model: <a title="ednaml.models.ModelAbstract.ModelAbstract" href="../models/ModelAbstract.html#ednaml.models.ModelAbstract.ModelAbstract">ModelAbstract</a>, loss_fn: List[<a title="ednaml.loss.builders.LossBuilder" href="../loss/builders/index.html#ednaml.loss.builders.LossBuilder">LossBuilder</a>], optimizer: List[torch.optim.optimizer.Optimizer], loss_optimizer: List[torch.optim.optimizer.Optimizer], scheduler: List[torch.optim.lr_scheduler._LRScheduler], loss_scheduler: List[torch.optim.lr_scheduler._LRScheduler], train_loader: torch.utils.data.dataloader.DataLoader, test_loader: torch.utils.data.dataloader.DataLoader, epochs: int, skipeval: bool, logger: logging.Logger, crawler: <a title="ednaml.crawlers.Crawler" href="../crawlers/index.html#ednaml.crawlers.Crawler">Crawler</a>, config: <a title="ednaml.config.EdnaMLConfig.EdnaMLConfig" href="../config/EdnaMLConfig.html#ednaml.config.EdnaMLConfig.EdnaMLConfig">EdnaMLConfig</a>, labels: <a title="ednaml.utils.LabelMetadata.LabelMetadata" href="../utils/LabelMetadata.html#ednaml.utils.LabelMetadata.LabelMetadata">LabelMetadata</a>, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BaseTrainer:
    model: ModelAbstract
    loss_fn: Dict[str, ednaml.loss.builders.LossBuilder]  # output-name: lossBuilder
    optimizer: Dict[str, torch.optim.Optimizer]  
    loss_optimizer = Dict[str, List[torch.optim.Optimizer]] 
    scheduler: Dict[str, torch.optim.lr_scheduler._LRScheduler]  
    loss_scheduler: Dict[str, List[torch.optim.lr_scheduler._LRScheduler]] 

    skipeval: bool
    train_loader: DataLoader
    test_loader: DataLoader

    epochs: int
    logger: Logger
    global_batch: int
    global_epoch: int
    num_losses: int
    losses: Dict[str, List[int]] 
    metadata: Dict[str, str]
    labelMetadata: LabelMetadata
    logger: logging.Logger

    def __init__(
        self,
        model: ModelAbstract,
        loss_fn: List[ednaml.loss.builders.LossBuilder],
        optimizer: List[torch.optim.Optimizer],
        loss_optimizer: List[torch.optim.Optimizer],
        scheduler: List[torch.optim.lr_scheduler._LRScheduler],
        loss_scheduler: List[torch.optim.lr_scheduler._LRScheduler],
        train_loader: DataLoader,
        test_loader: DataLoader,
        epochs: int,
        skipeval: bool,
        logger: Logger,
        crawler: Crawler,
        config: EdnaMLConfig,
        labels: LabelMetadata,
        **kwargs
    ):

        self.model = model
        self.parameter_groups = list(self.model.parameter_groups.keys())
        self.loss_fn_order = {
            idx: lossbuilder.loss_labelname for idx, lossbuilder in enumerate(loss_fn)
        }
        self.loss_fn = {
            lossbuilder.loss_labelname: lossbuilder for lossbuilder in loss_fn
        }
        self.num_losses = len(self.loss_fn)
        self.losses = {lossname: [] for lossname in self.loss_fn}
        self.loss_optimizer = {
            self.loss_fn_order[idx]: loss_optimizer_content
            for idx, loss_optimizer_content in enumerate(loss_optimizer)
        }
        if type(loss_scheduler) is list:
            self.loss_scheduler = {
                self.loss_fn_order[idx]: loss_scheduler_content
                for idx, loss_scheduler_content in enumerate(loss_scheduler)
            }
        else:
            self.loss_scheduler = {
                self.loss_fn_order[idx]: loss_scheduler
                for idx in range(self.num_losses)
            }

        self.optimizer = {self.parameter_groups[idx]:optimizer_item for idx, optimizer_item in enumerate(optimizer)}
        self.scheduler = {self.parameter_groups[idx]:scheduler_item for idx, scheduler_item in enumerate(scheduler)}
        self.skipeval = skipeval
        self.train_loader = train_loader
        self.test_loader = test_loader

        self.epochs = epochs
        self.logger = logger

        self.global_batch = 0  # Current batch number in the epoch
        self.global_epoch = 0

        self.metadata = {}
        self.labelMetadata = labels
        self.crawler = crawler
        self.config = config

        self.buildMetadata(
            crawler=crawler.classes, config=json.loads(config.export(&#34;json&#34;))
        )

    def buildMetadata(self, **kwargs):
        for keys in kwargs:
            self.metadata[keys] = kwargs.get(keys)

    def setup(
        self,
        step_verbose: int = 5,
        save_frequency: int = 5,
        test_frequency: int = 5,
        save_directory: str = &#34;./checkpoint/&#34;,
        save_backup: bool = False,
        backup_directory: str = None,
        gpus: int = 1,
        fp16: bool = False,
        model_save_name: str = None,
        logger_file: str = None,
    ):
        self.step_verbose = step_verbose
        self.save_frequency = save_frequency
        self.test_frequency = test_frequency
        self.save_directory = save_directory
        self.backup_directory = None
        self.model_save_name = model_save_name
        self.logger_file = logger_file
        self.save_backup = save_backup
        if self.save_backup:
            self.backup_directory = backup_directory
            os.makedirs(self.backup_directory, exist_ok=True)
        os.makedirs(self.save_directory, exist_ok=True)
        self.saveMetadata()

        self.gpus = gpus

        if self.gpus != 1:
            raise NotImplementedError()

        self.model.cuda()

        self.fp16 = fp16
        # if self.fp16 and self.apex is not None:
        #    self.model, self.optimizer = self.apex.amp.initialize(self.model, self.optimizer, opt_level=&#39;O1&#39;)

    def saveMetadata(self):
        print(&#34;NOT saving metadata. saveMetadata() function not set up.&#34;)

    def save(self):
        self.logger.info(&#34;Saving model, optimizer, and scheduler.&#34;)
        MODEL_SAVE = self.model_save_name + &#34;_epoch%i&#34; % self.global_epoch + &#34;.pth&#34;
        TRAINING_SAVE = (
            self.model_save_name + &#34;_epoch%i&#34; % self.global_epoch + &#34;_training.pth&#34;
        )

        save_dict = {}
        save_dict[&#34;optimizer&#34;] = {
            pgn:self.optimizer[pgn].state_dict() for pgn in self.parameter_groups
        }
        save_dict[&#34;scheduler&#34;] = {
            pgn:self.scheduler[pgn].state_dict() for pgn in self.parameter_groups
        }
        save_dict[&#34;loss_fn&#34;] = {
            lossname: self.loss_fn[lossname].state_dict() for lossname in self.loss_fn
        }
        save_dict[&#34;loss_optimizer&#34;] = {
            lossname: (
                self.loss_optimizer[lossname].state_dict()
                if self.loss_optimizer[lossname] is not None
                else None
            )
            for lossname in self.loss_optimizer
        }
        save_dict[&#34;loss_scheduler&#34;] = {
            lossname: (
                self.loss_scheduler[lossname].state_dict()
                if self.loss_scheduler[lossname] is not None
                else None
            )
            for lossname in self.loss_scheduler
        }

        # save_dict[&#34;loss_optimizer&#34;] = [self.loss_optimizer[idx].state_dict() if self.loss_optimizer[idx] is not None else None for idx in range(self.num_losses)]
        # save_dict[&#34;loss_scheduler&#34;] = [self.loss_scheduler[idx].state_dict() if self.loss_scheduler[idx] is not None else None for idx in range(self.num_losses)]

        torch.save(
            self.model.state_dict(), os.path.join(self.save_directory, MODEL_SAVE)
        )
        torch.save(save_dict, os.path.join(self.save_directory, TRAINING_SAVE))

        if self.save_backup:
            shutil.copy2(
                os.path.join(self.save_directory, MODEL_SAVE), self.backup_directory
            )
            shutil.copy2(
                os.path.join(self.save_directory, TRAINING_SAVE), self.backup_directory
            )

            self.logger.info(
                &#34;Performing drive backup of model, optimizer, and scheduler.&#34;
            )

            LOGGER_SAVE = os.path.join(self.backup_directory, self.logger_file)
            if os.path.exists(LOGGER_SAVE):
                os.remove(LOGGER_SAVE)
            shutil.copy2(
                os.path.join(self.save_directory, self.logger_file), LOGGER_SAVE
            )

    def load(self, load_epoch):
        self.logger.info(
            &#34;Resuming training from epoch %i. Loading saved state from %i&#34;
            % (load_epoch + 1, load_epoch)
        )
        model_load = self.model_save_name + &#34;_epoch%i&#34; % load_epoch + &#34;.pth&#34;
        training_load = self.model_save_name + &#34;_epoch%i&#34; % load_epoch + &#34;_training.pth&#34;

        if self.save_backup:
            self.logger.info(
                &#34;Loading model, optimizer, and scheduler from drive backup.&#34;
            )
            model_load_path = os.path.join(self.backup_directory, model_load)
            training_load_path = os.path.join(self.backup_directory, training_load)

        else:
            self.logger.info(
                &#34;Loading model, optimizer, and scheduler from local backup.&#34;
            )
            model_load_path = os.path.join(self.save_directory, model_load)
            training_load_path = os.path.join(self.save_directory, training_load)

        self.model.load_state_dict(torch.load(model_load_path))
        self.logger.info(&#34;Finished loading model state_dict from %s&#34; % model_load_path)

        checkpoint = torch.load(training_load_path)
        for pgn in self.parameter_groups:
            self.optimizer[pgn].load_state_dict(
                checkpoint[&#34;optimizer&#34;][pgn]
            )

            self.scheduler[pgn].load_state_dict(
                checkpoint[&#34;scheduler&#34;][pgn]
            )
        self.logger.info(
            &#34;Finished loading optimizer state_dict from %s&#34; % training_load_path
        )
        self.logger.info(
            &#34;Finished loading scheduler state_dict from %s&#34; % training_load_path
        )

        for lossname in self.loss_fn:
            self.loss_fn[lossname].load_state_dict(checkpoint[&#34;loss_fn&#34;][lossname])
            if self.loss_optimizer[lossname] is not None:
                self.loss_optimizer[lossname].load_state_dict(
                    checkpoint[&#34;loss_optimizer&#34;][lossname]
                )
            if self.loss_scheduler[lossname] is not None:
                self.loss_scheduler[lossname].load_state_dict(
                    checkpoint[&#34;loss_scheduler&#34;][lossname]
                )

            self.logger.info(
                &#34;Finished loading loss state_dict from %s&#34; % training_load_path
            )

        # for idx in range(self.num_losses):
        #    if self.loss_optimizer[idx] is not None:
        #        self.loss_optimizer[idx].load_state_dict(checkpoint[&#34;loss_optimizer&#34;][idx])
        #    if self.loss_scheduler[idx] is not None:
        #        self.loss_scheduler[idx].load_state_dict(checkpoint[&#34;loss_scheduler&#34;][idx])

    def train(self, continue_epoch=0):
        self.logger.info(&#34;Starting training&#34;)
        self.logger.info(&#34;Logging to:\t%s&#34; % self.logger_file)
        self.logger.info(
            &#34;Models will be saved to local directory:\t%s&#34; % self.save_directory
        )
        if self.save_backup:
            self.logger.info(
                &#34;Models will be backed up to drive directory:\t%s&#34;
                % self.backup_directory
            )
        self.logger.info(
            &#34;Models will be saved with base name:\t%s_epoch[].pth&#34;
            % self.model_save_name
        )
        self.logger.info(
            &#34;Optimizers will be saved with base name:\t%s_epoch[]_optimizer.pth&#34;
            % self.model_save_name
        )
        self.logger.info(
            &#34;Schedulers will be saved with base name:\t%s_epoch[]_scheduler.pth&#34;
            % self.model_save_name
        )

        if continue_epoch &gt; 0:
            load_epoch = continue_epoch - 1
            self.load(load_epoch)

        if not self.skipeval:
            self.logger.info(&#34;Performing initial evaluation...&#34;)
            self.initial_evaluate()
        else:
            self.logger.info(&#34;Skipping initial evaluation.&#34;)

        self.logger.info(&#34;Starting training from %i&#34; % continue_epoch)
        for epoch in range(self.epochs):
            if epoch &gt;= continue_epoch:
                self.epoch_step(epoch)
            else:
                self.global_epoch = epoch + 1

    def initial_evaluate(self):
        &#34;&#34;&#34;Evaluation of model before we start training
        &#34;&#34;&#34;
        self.evaluate()

    def epoch_step(self, epoch):
        &#34;&#34;&#34;Trains model for an epoch.
        &#34;&#34;&#34;
        for batch in self.train_loader:
            if self.global_batch == 0:
                self.printOptimizerLearningRates()
            
            self.model.train()
            self.zeroGradOptimizers()
            self.zeroGradLossOptimizers()

            self.step(batch)
            
            self.global_batch+=1

            if (self.global_batch + 1) % self.step_verbose == 0:
                self.printStepInformation()

        self.global_batch = 0
        self.stepSchedulers()
        self.stepLossSchedulers()

        self.logger.info(
            &#34;{0} Completed epoch {1} {2}&#34;.format(&#34;*&#34; * 10, self.global_epoch, &#34;*&#34; * 10)
        )

        if self.global_epoch % self.test_frequency == 0:
            self.logger.info(&#34;Evaluating model at test-frequency&#34;)
            self.evaluate()
        if self.global_epoch % self.save_frequency == 0:
            self.logger.info(&#34;Saving model at save-frequency&#34;)
            self.save()
        self.global_epoch += 1

    def printStepInformation(self):
        loss_avg = 0.0
        for lossname in self.losses:
            loss_avg += (
                sum(self.losses[lossname][-self.step_verbose :])
                / self.step_verbose
            )
        loss_avg /= self.num_losses
        soft_avg = sum(self.softaccuracy[-100:]) / float(
            len(self.softaccuracy[-100:])
        )
        self.logger.info(
            &#34;Epoch{0}.{1}\tTotal Avg Loss: {2:.3f} Softmax: {3:.3f}&#34;.format(
                self.global_epoch, self.global_batch, loss_avg, soft_avg
            )
        )

    def evaluate(self):
        logit_labels, true_labels, features = self.evaluate_impl()
        return logit_labels, true_labels, self.crawler.classes, features

    def evaluate_impl(self):
        raise NotImplementedError


    def zeroGradOptimizers(self):
        for optim in self.optimizer:
            self.optimizer[optim].zero_grad()

    def zeroGradLossOptimizers(self):
        for lossname in self.loss_fn:
            if self.loss_optimizer[lossname] is not None:
                self.loss_optimizer[lossname].zero_grad()

    def stepOptimizers(self):
        for optim in self.optimizer:
            self.optimizer[optim].step()

    def stepLossOptimizers(self):
        for lossname in self.loss_fn:
            if (
                self.loss_optimizer[lossname] is not None
            ):  # In case loss object doesn;t have any parameters, this will be None. See optimizers.StandardLossOptimizer
                self.loss_optimizer[lossname].step()

    def stepSchedulers(self):
        for scheduler in self.scheduler:
            self.scheduler[scheduler].step()

    def stepLossSchedulers(self):
        for lossname in self.loss_fn:
            if self.loss_scheduler[lossname] is not None:
                self.loss_scheduler[lossname].step()

    def printOptimizerLearningRates(self):
        for param_group_name in self.optimizer:
            lrs = self.scheduler[param_group_name].get_last_lr()
            lrs = sum(lrs) / float(len(lrs))
            self.logger.info(
                    &#34;Parameter Group `{0}`: Starting epoch {1} with {2} steps and learning rate {3:2.5E}&#34;.format(
                        param_group_name,
                        self.global_epoch,
                        len(self.train_loader) - (len(self.train_loader) % 10),
                        lrs,
                    )
                )</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="ednaml.trainer.ClassificationTrainer.ClassificationTrainer" href="ClassificationTrainer.html#ednaml.trainer.ClassificationTrainer.ClassificationTrainer">ClassificationTrainer</a></li>
<li><a title="ednaml.trainer.MultiBranchTrainer.MultiBranchTrainer" href="MultiBranchTrainer.html#ednaml.trainer.MultiBranchTrainer.MultiBranchTrainer">MultiBranchTrainer</a></li>
<li><a title="ednaml.trainer.MultiClassificationTrainer.MultiClassificationTrainer" href="MultiClassificationTrainer.html#ednaml.trainer.MultiClassificationTrainer.MultiClassificationTrainer">MultiClassificationTrainer</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="ednaml.trainer.BaseTrainer.BaseTrainer.epochs"><code class="name">var <span class="ident">epochs</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.trainer.BaseTrainer.BaseTrainer.global_batch"><code class="name">var <span class="ident">global_batch</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.trainer.BaseTrainer.BaseTrainer.global_epoch"><code class="name">var <span class="ident">global_epoch</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.trainer.BaseTrainer.BaseTrainer.labelMetadata"><code class="name">var <span class="ident">labelMetadata</span> : <a title="ednaml.utils.LabelMetadata.LabelMetadata" href="../utils/LabelMetadata.html#ednaml.utils.LabelMetadata.LabelMetadata">LabelMetadata</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.trainer.BaseTrainer.BaseTrainer.logger"><code class="name">var <span class="ident">logger</span> : logging.Logger</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.trainer.BaseTrainer.BaseTrainer.loss_fn"><code class="name">var <span class="ident">loss_fn</span> : Dict[str, <a title="ednaml.loss.builders.LossBuilder" href="../loss/builders/index.html#ednaml.loss.builders.LossBuilder">LossBuilder</a>]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.trainer.BaseTrainer.BaseTrainer.loss_optimizer"><code class="name">var <span class="ident">loss_optimizer</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.trainer.BaseTrainer.BaseTrainer.loss_scheduler"><code class="name">var <span class="ident">loss_scheduler</span> : Dict[str, List[torch.optim.lr_scheduler._LRScheduler]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.trainer.BaseTrainer.BaseTrainer.losses"><code class="name">var <span class="ident">losses</span> : Dict[str, List[int]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.trainer.BaseTrainer.BaseTrainer.metadata"><code class="name">var <span class="ident">metadata</span> : Dict[str, str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.trainer.BaseTrainer.BaseTrainer.model"><code class="name">var <span class="ident">model</span> : <a title="ednaml.models.ModelAbstract.ModelAbstract" href="../models/ModelAbstract.html#ednaml.models.ModelAbstract.ModelAbstract">ModelAbstract</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.trainer.BaseTrainer.BaseTrainer.num_losses"><code class="name">var <span class="ident">num_losses</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.trainer.BaseTrainer.BaseTrainer.optimizer"><code class="name">var <span class="ident">optimizer</span> : Dict[str, torch.optim.optimizer.Optimizer]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.trainer.BaseTrainer.BaseTrainer.scheduler"><code class="name">var <span class="ident">scheduler</span> : Dict[str, torch.optim.lr_scheduler._LRScheduler]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.trainer.BaseTrainer.BaseTrainer.skipeval"><code class="name">var <span class="ident">skipeval</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.trainer.BaseTrainer.BaseTrainer.test_loader"><code class="name">var <span class="ident">test_loader</span> : torch.utils.data.dataloader.DataLoader</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.trainer.BaseTrainer.BaseTrainer.train_loader"><code class="name">var <span class="ident">train_loader</span> : torch.utils.data.dataloader.DataLoader</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="ednaml.trainer.BaseTrainer.BaseTrainer.buildMetadata"><code class="name flex">
<span>def <span class="ident">buildMetadata</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def buildMetadata(self, **kwargs):
    for keys in kwargs:
        self.metadata[keys] = kwargs.get(keys)</code></pre>
</details>
</dd>
<dt id="ednaml.trainer.BaseTrainer.BaseTrainer.epoch_step"><code class="name flex">
<span>def <span class="ident">epoch_step</span></span>(<span>self, epoch)</span>
</code></dt>
<dd>
<div class="desc"><p>Trains model for an epoch.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def epoch_step(self, epoch):
    &#34;&#34;&#34;Trains model for an epoch.
    &#34;&#34;&#34;
    for batch in self.train_loader:
        if self.global_batch == 0:
            self.printOptimizerLearningRates()
        
        self.model.train()
        self.zeroGradOptimizers()
        self.zeroGradLossOptimizers()

        self.step(batch)
        
        self.global_batch+=1

        if (self.global_batch + 1) % self.step_verbose == 0:
            self.printStepInformation()

    self.global_batch = 0
    self.stepSchedulers()
    self.stepLossSchedulers()

    self.logger.info(
        &#34;{0} Completed epoch {1} {2}&#34;.format(&#34;*&#34; * 10, self.global_epoch, &#34;*&#34; * 10)
    )

    if self.global_epoch % self.test_frequency == 0:
        self.logger.info(&#34;Evaluating model at test-frequency&#34;)
        self.evaluate()
    if self.global_epoch % self.save_frequency == 0:
        self.logger.info(&#34;Saving model at save-frequency&#34;)
        self.save()
    self.global_epoch += 1</code></pre>
</details>
</dd>
<dt id="ednaml.trainer.BaseTrainer.BaseTrainer.evaluate"><code class="name flex">
<span>def <span class="ident">evaluate</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluate(self):
    logit_labels, true_labels, features = self.evaluate_impl()
    return logit_labels, true_labels, self.crawler.classes, features</code></pre>
</details>
</dd>
<dt id="ednaml.trainer.BaseTrainer.BaseTrainer.evaluate_impl"><code class="name flex">
<span>def <span class="ident">evaluate_impl</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluate_impl(self):
    raise NotImplementedError</code></pre>
</details>
</dd>
<dt id="ednaml.trainer.BaseTrainer.BaseTrainer.initial_evaluate"><code class="name flex">
<span>def <span class="ident">initial_evaluate</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluation of model before we start training</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initial_evaluate(self):
    &#34;&#34;&#34;Evaluation of model before we start training
    &#34;&#34;&#34;
    self.evaluate()</code></pre>
</details>
</dd>
<dt id="ednaml.trainer.BaseTrainer.BaseTrainer.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>self, load_epoch)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load(self, load_epoch):
    self.logger.info(
        &#34;Resuming training from epoch %i. Loading saved state from %i&#34;
        % (load_epoch + 1, load_epoch)
    )
    model_load = self.model_save_name + &#34;_epoch%i&#34; % load_epoch + &#34;.pth&#34;
    training_load = self.model_save_name + &#34;_epoch%i&#34; % load_epoch + &#34;_training.pth&#34;

    if self.save_backup:
        self.logger.info(
            &#34;Loading model, optimizer, and scheduler from drive backup.&#34;
        )
        model_load_path = os.path.join(self.backup_directory, model_load)
        training_load_path = os.path.join(self.backup_directory, training_load)

    else:
        self.logger.info(
            &#34;Loading model, optimizer, and scheduler from local backup.&#34;
        )
        model_load_path = os.path.join(self.save_directory, model_load)
        training_load_path = os.path.join(self.save_directory, training_load)

    self.model.load_state_dict(torch.load(model_load_path))
    self.logger.info(&#34;Finished loading model state_dict from %s&#34; % model_load_path)

    checkpoint = torch.load(training_load_path)
    for pgn in self.parameter_groups:
        self.optimizer[pgn].load_state_dict(
            checkpoint[&#34;optimizer&#34;][pgn]
        )

        self.scheduler[pgn].load_state_dict(
            checkpoint[&#34;scheduler&#34;][pgn]
        )
    self.logger.info(
        &#34;Finished loading optimizer state_dict from %s&#34; % training_load_path
    )
    self.logger.info(
        &#34;Finished loading scheduler state_dict from %s&#34; % training_load_path
    )

    for lossname in self.loss_fn:
        self.loss_fn[lossname].load_state_dict(checkpoint[&#34;loss_fn&#34;][lossname])
        if self.loss_optimizer[lossname] is not None:
            self.loss_optimizer[lossname].load_state_dict(
                checkpoint[&#34;loss_optimizer&#34;][lossname]
            )
        if self.loss_scheduler[lossname] is not None:
            self.loss_scheduler[lossname].load_state_dict(
                checkpoint[&#34;loss_scheduler&#34;][lossname]
            )

        self.logger.info(
            &#34;Finished loading loss state_dict from %s&#34; % training_load_path
        )

    # for idx in range(self.num_losses):
    #    if self.loss_optimizer[idx] is not None:
    #        self.loss_optimizer[idx].load_state_dict(checkpoint[&#34;loss_optimizer&#34;][idx])
    #    if self.loss_scheduler[idx] is not None:
    #        self.loss_scheduler[idx].load_state_dict(checkpoint[&#34;loss_scheduler&#34;][idx])</code></pre>
</details>
</dd>
<dt id="ednaml.trainer.BaseTrainer.BaseTrainer.printOptimizerLearningRates"><code class="name flex">
<span>def <span class="ident">printOptimizerLearningRates</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def printOptimizerLearningRates(self):
    for param_group_name in self.optimizer:
        lrs = self.scheduler[param_group_name].get_last_lr()
        lrs = sum(lrs) / float(len(lrs))
        self.logger.info(
                &#34;Parameter Group `{0}`: Starting epoch {1} with {2} steps and learning rate {3:2.5E}&#34;.format(
                    param_group_name,
                    self.global_epoch,
                    len(self.train_loader) - (len(self.train_loader) % 10),
                    lrs,
                )
            )</code></pre>
</details>
</dd>
<dt id="ednaml.trainer.BaseTrainer.BaseTrainer.printStepInformation"><code class="name flex">
<span>def <span class="ident">printStepInformation</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def printStepInformation(self):
    loss_avg = 0.0
    for lossname in self.losses:
        loss_avg += (
            sum(self.losses[lossname][-self.step_verbose :])
            / self.step_verbose
        )
    loss_avg /= self.num_losses
    soft_avg = sum(self.softaccuracy[-100:]) / float(
        len(self.softaccuracy[-100:])
    )
    self.logger.info(
        &#34;Epoch{0}.{1}\tTotal Avg Loss: {2:.3f} Softmax: {3:.3f}&#34;.format(
            self.global_epoch, self.global_batch, loss_avg, soft_avg
        )
    )</code></pre>
</details>
</dd>
<dt id="ednaml.trainer.BaseTrainer.BaseTrainer.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save(self):
    self.logger.info(&#34;Saving model, optimizer, and scheduler.&#34;)
    MODEL_SAVE = self.model_save_name + &#34;_epoch%i&#34; % self.global_epoch + &#34;.pth&#34;
    TRAINING_SAVE = (
        self.model_save_name + &#34;_epoch%i&#34; % self.global_epoch + &#34;_training.pth&#34;
    )

    save_dict = {}
    save_dict[&#34;optimizer&#34;] = {
        pgn:self.optimizer[pgn].state_dict() for pgn in self.parameter_groups
    }
    save_dict[&#34;scheduler&#34;] = {
        pgn:self.scheduler[pgn].state_dict() for pgn in self.parameter_groups
    }
    save_dict[&#34;loss_fn&#34;] = {
        lossname: self.loss_fn[lossname].state_dict() for lossname in self.loss_fn
    }
    save_dict[&#34;loss_optimizer&#34;] = {
        lossname: (
            self.loss_optimizer[lossname].state_dict()
            if self.loss_optimizer[lossname] is not None
            else None
        )
        for lossname in self.loss_optimizer
    }
    save_dict[&#34;loss_scheduler&#34;] = {
        lossname: (
            self.loss_scheduler[lossname].state_dict()
            if self.loss_scheduler[lossname] is not None
            else None
        )
        for lossname in self.loss_scheduler
    }

    # save_dict[&#34;loss_optimizer&#34;] = [self.loss_optimizer[idx].state_dict() if self.loss_optimizer[idx] is not None else None for idx in range(self.num_losses)]
    # save_dict[&#34;loss_scheduler&#34;] = [self.loss_scheduler[idx].state_dict() if self.loss_scheduler[idx] is not None else None for idx in range(self.num_losses)]

    torch.save(
        self.model.state_dict(), os.path.join(self.save_directory, MODEL_SAVE)
    )
    torch.save(save_dict, os.path.join(self.save_directory, TRAINING_SAVE))

    if self.save_backup:
        shutil.copy2(
            os.path.join(self.save_directory, MODEL_SAVE), self.backup_directory
        )
        shutil.copy2(
            os.path.join(self.save_directory, TRAINING_SAVE), self.backup_directory
        )

        self.logger.info(
            &#34;Performing drive backup of model, optimizer, and scheduler.&#34;
        )

        LOGGER_SAVE = os.path.join(self.backup_directory, self.logger_file)
        if os.path.exists(LOGGER_SAVE):
            os.remove(LOGGER_SAVE)
        shutil.copy2(
            os.path.join(self.save_directory, self.logger_file), LOGGER_SAVE
        )</code></pre>
</details>
</dd>
<dt id="ednaml.trainer.BaseTrainer.BaseTrainer.saveMetadata"><code class="name flex">
<span>def <span class="ident">saveMetadata</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def saveMetadata(self):
    print(&#34;NOT saving metadata. saveMetadata() function not set up.&#34;)</code></pre>
</details>
</dd>
<dt id="ednaml.trainer.BaseTrainer.BaseTrainer.setup"><code class="name flex">
<span>def <span class="ident">setup</span></span>(<span>self, step_verbose: int = 5, save_frequency: int = 5, test_frequency: int = 5, save_directory: str = './checkpoint/', save_backup: bool = False, backup_directory: str = None, gpus: int = 1, fp16: bool = False, model_save_name: str = None, logger_file: str = None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def setup(
    self,
    step_verbose: int = 5,
    save_frequency: int = 5,
    test_frequency: int = 5,
    save_directory: str = &#34;./checkpoint/&#34;,
    save_backup: bool = False,
    backup_directory: str = None,
    gpus: int = 1,
    fp16: bool = False,
    model_save_name: str = None,
    logger_file: str = None,
):
    self.step_verbose = step_verbose
    self.save_frequency = save_frequency
    self.test_frequency = test_frequency
    self.save_directory = save_directory
    self.backup_directory = None
    self.model_save_name = model_save_name
    self.logger_file = logger_file
    self.save_backup = save_backup
    if self.save_backup:
        self.backup_directory = backup_directory
        os.makedirs(self.backup_directory, exist_ok=True)
    os.makedirs(self.save_directory, exist_ok=True)
    self.saveMetadata()

    self.gpus = gpus

    if self.gpus != 1:
        raise NotImplementedError()

    self.model.cuda()

    self.fp16 = fp16
    # if self.fp16 and self.apex is not None:
    #    self.model, self.optimizer = self.apex.amp.initialize(self.model, self.optimizer, opt_level=&#39;O1&#39;)</code></pre>
</details>
</dd>
<dt id="ednaml.trainer.BaseTrainer.BaseTrainer.stepLossOptimizers"><code class="name flex">
<span>def <span class="ident">stepLossOptimizers</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stepLossOptimizers(self):
    for lossname in self.loss_fn:
        if (
            self.loss_optimizer[lossname] is not None
        ):  # In case loss object doesn;t have any parameters, this will be None. See optimizers.StandardLossOptimizer
            self.loss_optimizer[lossname].step()</code></pre>
</details>
</dd>
<dt id="ednaml.trainer.BaseTrainer.BaseTrainer.stepLossSchedulers"><code class="name flex">
<span>def <span class="ident">stepLossSchedulers</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stepLossSchedulers(self):
    for lossname in self.loss_fn:
        if self.loss_scheduler[lossname] is not None:
            self.loss_scheduler[lossname].step()</code></pre>
</details>
</dd>
<dt id="ednaml.trainer.BaseTrainer.BaseTrainer.stepOptimizers"><code class="name flex">
<span>def <span class="ident">stepOptimizers</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stepOptimizers(self):
    for optim in self.optimizer:
        self.optimizer[optim].step()</code></pre>
</details>
</dd>
<dt id="ednaml.trainer.BaseTrainer.BaseTrainer.stepSchedulers"><code class="name flex">
<span>def <span class="ident">stepSchedulers</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stepSchedulers(self):
    for scheduler in self.scheduler:
        self.scheduler[scheduler].step()</code></pre>
</details>
</dd>
<dt id="ednaml.trainer.BaseTrainer.BaseTrainer.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self, continue_epoch=0)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train(self, continue_epoch=0):
    self.logger.info(&#34;Starting training&#34;)
    self.logger.info(&#34;Logging to:\t%s&#34; % self.logger_file)
    self.logger.info(
        &#34;Models will be saved to local directory:\t%s&#34; % self.save_directory
    )
    if self.save_backup:
        self.logger.info(
            &#34;Models will be backed up to drive directory:\t%s&#34;
            % self.backup_directory
        )
    self.logger.info(
        &#34;Models will be saved with base name:\t%s_epoch[].pth&#34;
        % self.model_save_name
    )
    self.logger.info(
        &#34;Optimizers will be saved with base name:\t%s_epoch[]_optimizer.pth&#34;
        % self.model_save_name
    )
    self.logger.info(
        &#34;Schedulers will be saved with base name:\t%s_epoch[]_scheduler.pth&#34;
        % self.model_save_name
    )

    if continue_epoch &gt; 0:
        load_epoch = continue_epoch - 1
        self.load(load_epoch)

    if not self.skipeval:
        self.logger.info(&#34;Performing initial evaluation...&#34;)
        self.initial_evaluate()
    else:
        self.logger.info(&#34;Skipping initial evaluation.&#34;)

    self.logger.info(&#34;Starting training from %i&#34; % continue_epoch)
    for epoch in range(self.epochs):
        if epoch &gt;= continue_epoch:
            self.epoch_step(epoch)
        else:
            self.global_epoch = epoch + 1</code></pre>
</details>
</dd>
<dt id="ednaml.trainer.BaseTrainer.BaseTrainer.zeroGradLossOptimizers"><code class="name flex">
<span>def <span class="ident">zeroGradLossOptimizers</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def zeroGradLossOptimizers(self):
    for lossname in self.loss_fn:
        if self.loss_optimizer[lossname] is not None:
            self.loss_optimizer[lossname].zero_grad()</code></pre>
</details>
</dd>
<dt id="ednaml.trainer.BaseTrainer.BaseTrainer.zeroGradOptimizers"><code class="name flex">
<span>def <span class="ident">zeroGradOptimizers</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def zeroGradOptimizers(self):
    for optim in self.optimizer:
        self.optimizer[optim].zero_grad()</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="ednaml.trainer" href="index.html">ednaml.trainer</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="ednaml.trainer.BaseTrainer.BaseTrainer" href="#ednaml.trainer.BaseTrainer.BaseTrainer">BaseTrainer</a></code></h4>
<ul class="">
<li><code><a title="ednaml.trainer.BaseTrainer.BaseTrainer.buildMetadata" href="#ednaml.trainer.BaseTrainer.BaseTrainer.buildMetadata">buildMetadata</a></code></li>
<li><code><a title="ednaml.trainer.BaseTrainer.BaseTrainer.epoch_step" href="#ednaml.trainer.BaseTrainer.BaseTrainer.epoch_step">epoch_step</a></code></li>
<li><code><a title="ednaml.trainer.BaseTrainer.BaseTrainer.epochs" href="#ednaml.trainer.BaseTrainer.BaseTrainer.epochs">epochs</a></code></li>
<li><code><a title="ednaml.trainer.BaseTrainer.BaseTrainer.evaluate" href="#ednaml.trainer.BaseTrainer.BaseTrainer.evaluate">evaluate</a></code></li>
<li><code><a title="ednaml.trainer.BaseTrainer.BaseTrainer.evaluate_impl" href="#ednaml.trainer.BaseTrainer.BaseTrainer.evaluate_impl">evaluate_impl</a></code></li>
<li><code><a title="ednaml.trainer.BaseTrainer.BaseTrainer.global_batch" href="#ednaml.trainer.BaseTrainer.BaseTrainer.global_batch">global_batch</a></code></li>
<li><code><a title="ednaml.trainer.BaseTrainer.BaseTrainer.global_epoch" href="#ednaml.trainer.BaseTrainer.BaseTrainer.global_epoch">global_epoch</a></code></li>
<li><code><a title="ednaml.trainer.BaseTrainer.BaseTrainer.initial_evaluate" href="#ednaml.trainer.BaseTrainer.BaseTrainer.initial_evaluate">initial_evaluate</a></code></li>
<li><code><a title="ednaml.trainer.BaseTrainer.BaseTrainer.labelMetadata" href="#ednaml.trainer.BaseTrainer.BaseTrainer.labelMetadata">labelMetadata</a></code></li>
<li><code><a title="ednaml.trainer.BaseTrainer.BaseTrainer.load" href="#ednaml.trainer.BaseTrainer.BaseTrainer.load">load</a></code></li>
<li><code><a title="ednaml.trainer.BaseTrainer.BaseTrainer.logger" href="#ednaml.trainer.BaseTrainer.BaseTrainer.logger">logger</a></code></li>
<li><code><a title="ednaml.trainer.BaseTrainer.BaseTrainer.loss_fn" href="#ednaml.trainer.BaseTrainer.BaseTrainer.loss_fn">loss_fn</a></code></li>
<li><code><a title="ednaml.trainer.BaseTrainer.BaseTrainer.loss_optimizer" href="#ednaml.trainer.BaseTrainer.BaseTrainer.loss_optimizer">loss_optimizer</a></code></li>
<li><code><a title="ednaml.trainer.BaseTrainer.BaseTrainer.loss_scheduler" href="#ednaml.trainer.BaseTrainer.BaseTrainer.loss_scheduler">loss_scheduler</a></code></li>
<li><code><a title="ednaml.trainer.BaseTrainer.BaseTrainer.losses" href="#ednaml.trainer.BaseTrainer.BaseTrainer.losses">losses</a></code></li>
<li><code><a title="ednaml.trainer.BaseTrainer.BaseTrainer.metadata" href="#ednaml.trainer.BaseTrainer.BaseTrainer.metadata">metadata</a></code></li>
<li><code><a title="ednaml.trainer.BaseTrainer.BaseTrainer.model" href="#ednaml.trainer.BaseTrainer.BaseTrainer.model">model</a></code></li>
<li><code><a title="ednaml.trainer.BaseTrainer.BaseTrainer.num_losses" href="#ednaml.trainer.BaseTrainer.BaseTrainer.num_losses">num_losses</a></code></li>
<li><code><a title="ednaml.trainer.BaseTrainer.BaseTrainer.optimizer" href="#ednaml.trainer.BaseTrainer.BaseTrainer.optimizer">optimizer</a></code></li>
<li><code><a title="ednaml.trainer.BaseTrainer.BaseTrainer.printOptimizerLearningRates" href="#ednaml.trainer.BaseTrainer.BaseTrainer.printOptimizerLearningRates">printOptimizerLearningRates</a></code></li>
<li><code><a title="ednaml.trainer.BaseTrainer.BaseTrainer.printStepInformation" href="#ednaml.trainer.BaseTrainer.BaseTrainer.printStepInformation">printStepInformation</a></code></li>
<li><code><a title="ednaml.trainer.BaseTrainer.BaseTrainer.save" href="#ednaml.trainer.BaseTrainer.BaseTrainer.save">save</a></code></li>
<li><code><a title="ednaml.trainer.BaseTrainer.BaseTrainer.saveMetadata" href="#ednaml.trainer.BaseTrainer.BaseTrainer.saveMetadata">saveMetadata</a></code></li>
<li><code><a title="ednaml.trainer.BaseTrainer.BaseTrainer.scheduler" href="#ednaml.trainer.BaseTrainer.BaseTrainer.scheduler">scheduler</a></code></li>
<li><code><a title="ednaml.trainer.BaseTrainer.BaseTrainer.setup" href="#ednaml.trainer.BaseTrainer.BaseTrainer.setup">setup</a></code></li>
<li><code><a title="ednaml.trainer.BaseTrainer.BaseTrainer.skipeval" href="#ednaml.trainer.BaseTrainer.BaseTrainer.skipeval">skipeval</a></code></li>
<li><code><a title="ednaml.trainer.BaseTrainer.BaseTrainer.stepLossOptimizers" href="#ednaml.trainer.BaseTrainer.BaseTrainer.stepLossOptimizers">stepLossOptimizers</a></code></li>
<li><code><a title="ednaml.trainer.BaseTrainer.BaseTrainer.stepLossSchedulers" href="#ednaml.trainer.BaseTrainer.BaseTrainer.stepLossSchedulers">stepLossSchedulers</a></code></li>
<li><code><a title="ednaml.trainer.BaseTrainer.BaseTrainer.stepOptimizers" href="#ednaml.trainer.BaseTrainer.BaseTrainer.stepOptimizers">stepOptimizers</a></code></li>
<li><code><a title="ednaml.trainer.BaseTrainer.BaseTrainer.stepSchedulers" href="#ednaml.trainer.BaseTrainer.BaseTrainer.stepSchedulers">stepSchedulers</a></code></li>
<li><code><a title="ednaml.trainer.BaseTrainer.BaseTrainer.test_loader" href="#ednaml.trainer.BaseTrainer.BaseTrainer.test_loader">test_loader</a></code></li>
<li><code><a title="ednaml.trainer.BaseTrainer.BaseTrainer.train" href="#ednaml.trainer.BaseTrainer.BaseTrainer.train">train</a></code></li>
<li><code><a title="ednaml.trainer.BaseTrainer.BaseTrainer.train_loader" href="#ednaml.trainer.BaseTrainer.BaseTrainer.train_loader">train_loader</a></code></li>
<li><code><a title="ednaml.trainer.BaseTrainer.BaseTrainer.zeroGradLossOptimizers" href="#ednaml.trainer.BaseTrainer.BaseTrainer.zeroGradLossOptimizers">zeroGradLossOptimizers</a></code></li>
<li><code><a title="ednaml.trainer.BaseTrainer.BaseTrainer.zeroGradOptimizers" href="#ednaml.trainer.BaseTrainer.BaseTrainer.zeroGradOptimizers">zeroGradOptimizers</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>