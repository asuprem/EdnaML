<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>ednaml.trainer.ClassificationTrainer API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ednaml.trainer.ClassificationTrainer</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import tqdm, json
from sklearn.metrics import f1_score
import shutil
import os
import torch
import numpy as np
import ednaml.loss.builders
from typing import List
from ednaml.crawlers import Crawler
from ednaml.trainer import BaseTrainer
from ednaml.utils.LabelMetadata import LabelMetadata


class ClassificationTrainer(BaseTrainer):
    def __init__(
        self,
        model: torch.nn.Module,
        loss_fn: List[ednaml.loss.builders.LossBuilder],
        optimizer: torch.optim.Optimizer,
        loss_optimizer: List[torch.optim.Optimizer],
        scheduler: torch.optim.lr_scheduler._LRScheduler,
        loss_scheduler: List[torch.optim.lr_scheduler._LRScheduler],
        train_loader,
        test_loader,
        epochs: int,
        skipeval,
        logger,
        crawler: Crawler,
        config,
        labels: LabelMetadata,
        **kwargs
    ):

        super().__init__(
            model,
            loss_fn,
            optimizer,
            loss_optimizer,
            scheduler,
            loss_scheduler,
            train_loader,
            test_loader,
            epochs,
            skipeval,
            logger,
            crawler,
            config,
            labels,
            **kwargs
        )

        self.softaccuracy = []

    # Steps through a batch of data
    def step(self, batch):
        batch_kwargs = {}
        (
            img,
            batch_kwargs[&#34;labels&#34;],
        ) = batch  # This is the tensor response from collate_fn
        img, batch_kwargs[&#34;labels&#34;] = img.cuda(), batch_kwargs[&#34;labels&#34;].cuda()
        # logits, features, labels
        batch_kwargs[&#34;logits&#34;], batch_kwargs[&#34;features&#34;], _ = self.model(img)
        batch_kwargs[&#34;epoch&#34;] = self.global_epoch  # For CompactContrastiveLoss

        # TODO fix this with info about how many output are in the model...from the config file!!!!!
        loss = {loss_name: None for loss_name in self.loss_fn}
        for lossname in self.loss_fn:
            loss[lossname] = self.loss_fn[lossname](**batch_kwargs)
        # if self.fp16 and self.apex is not None:
        #    with self.apex.amp.scale_loss(loss, self.optimizer) as scaled_loss:
        #        scaled_loss.backward()
        # else:
        #    loss.backward()
        lossbackward = sum(loss.values())
        lossbackward.backward()

        self.stepOptimizers()
        self.stepLossOptimizers()

        for idx, lossname in enumerate(self.loss_fn):
            self.losses[lossname].append(loss[lossname].cpu().item())

        if batch_kwargs[&#34;logits&#34;] is not None:
            softmax_accuracy = (
                (batch_kwargs[&#34;logits&#34;].max(1)[1] == batch_kwargs[&#34;labels&#34;])
                .float()
                .mean()
            )
            self.softaccuracy.append(softmax_accuracy.cpu().item())
        else:
            self.softaccuracy.append(0)

    def evaluate_impl(self):
        self.model.eval()
        features, logits, labels = [], [], []
        with torch.no_grad():
            for batch in tqdm.tqdm(
                self.test_loader, total=len(self.test_loader), leave=False
            ):
                data, label = batch
                data = data.cuda()
                logit, feature, _ = self.model(data)
                feature = feature.detach().cpu()
                logit = logit.detach().cpu()
                features.append(feature)
                logits.append(logit)
                labels.append(label)

        features, logits, labels = (
            torch.cat(features, dim=0),
            torch.cat(logits, dim=0),
            torch.cat(labels, dim=0),
        )
        # Now we compute the loss...
        self.logger.info(&#34;Obtained features, validation in progress&#34;)
        # for evaluation...

        logit_labels = torch.argmax(logits, dim=1)
        accuracy = (logit_labels == labels).sum().float() / float(labels.size(0))
        micro_fscore = np.mean(f1_score(labels, logit_labels, average=&#34;micro&#34;))
        weighted_fscore = np.mean(f1_score(labels, logit_labels, average=&#34;weighted&#34;))
        self.logger.info(&#34;Accuracy: {:.3%}&#34;.format(accuracy))
        self.logger.info(&#34;Micro F-score: {:.3f}&#34;.format(micro_fscore))
        self.logger.info(&#34;Weighted F-score: {:.3f}&#34;.format(weighted_fscore))
        return logit_labels, labels, features

    def saveMetadata(self,):
        self.logger.info(&#34;Saving model metadata&#34;)
        jMetadata = json.dumps(self.metadata)
        metafile = &#34;metadata.json&#34;
        localmetafile = os.path.join(self.save_directory, metafile)
        if self.save_backup:
            backupmetafile = os.path.join(self.backup_directory, metafile)
        if not os.path.exists(localmetafile):
            with open(localmetafile, &#34;w&#34;) as localmetaobj:
                localmetaobj.write(jMetadata)
        self.logger.info(&#34;Backing up metadata&#34;)
        if self.save_backup:
            shutil.copy2(localmetafile, backupmetafile)
        self.logger.info(&#34;Finished metadata backup&#34;)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="ednaml.trainer.ClassificationTrainer.ClassificationTrainer"><code class="flex name class">
<span>class <span class="ident">ClassificationTrainer</span></span>
<span>(</span><span>model: torch.nn.modules.module.Module, loss_fn: List[<a title="ednaml.loss.builders.LossBuilder" href="../loss/builders/index.html#ednaml.loss.builders.LossBuilder">LossBuilder</a>], optimizer: torch.optim.optimizer.Optimizer, loss_optimizer: List[torch.optim.optimizer.Optimizer], scheduler: torch.optim.lr_scheduler._LRScheduler, loss_scheduler: List[torch.optim.lr_scheduler._LRScheduler], train_loader, test_loader, epochs: int, skipeval, logger, crawler: <a title="ednaml.crawlers.Crawler" href="../crawlers/index.html#ednaml.crawlers.Crawler">Crawler</a>, config, labels: <a title="ednaml.utils.LabelMetadata.LabelMetadata" href="../utils/LabelMetadata.html#ednaml.utils.LabelMetadata.LabelMetadata">LabelMetadata</a>, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ClassificationTrainer(BaseTrainer):
    def __init__(
        self,
        model: torch.nn.Module,
        loss_fn: List[ednaml.loss.builders.LossBuilder],
        optimizer: torch.optim.Optimizer,
        loss_optimizer: List[torch.optim.Optimizer],
        scheduler: torch.optim.lr_scheduler._LRScheduler,
        loss_scheduler: List[torch.optim.lr_scheduler._LRScheduler],
        train_loader,
        test_loader,
        epochs: int,
        skipeval,
        logger,
        crawler: Crawler,
        config,
        labels: LabelMetadata,
        **kwargs
    ):

        super().__init__(
            model,
            loss_fn,
            optimizer,
            loss_optimizer,
            scheduler,
            loss_scheduler,
            train_loader,
            test_loader,
            epochs,
            skipeval,
            logger,
            crawler,
            config,
            labels,
            **kwargs
        )

        self.softaccuracy = []

    # Steps through a batch of data
    def step(self, batch):
        batch_kwargs = {}
        (
            img,
            batch_kwargs[&#34;labels&#34;],
        ) = batch  # This is the tensor response from collate_fn
        img, batch_kwargs[&#34;labels&#34;] = img.cuda(), batch_kwargs[&#34;labels&#34;].cuda()
        # logits, features, labels
        batch_kwargs[&#34;logits&#34;], batch_kwargs[&#34;features&#34;], _ = self.model(img)
        batch_kwargs[&#34;epoch&#34;] = self.global_epoch  # For CompactContrastiveLoss

        # TODO fix this with info about how many output are in the model...from the config file!!!!!
        loss = {loss_name: None for loss_name in self.loss_fn}
        for lossname in self.loss_fn:
            loss[lossname] = self.loss_fn[lossname](**batch_kwargs)
        # if self.fp16 and self.apex is not None:
        #    with self.apex.amp.scale_loss(loss, self.optimizer) as scaled_loss:
        #        scaled_loss.backward()
        # else:
        #    loss.backward()
        lossbackward = sum(loss.values())
        lossbackward.backward()

        self.stepOptimizers()
        self.stepLossOptimizers()

        for idx, lossname in enumerate(self.loss_fn):
            self.losses[lossname].append(loss[lossname].cpu().item())

        if batch_kwargs[&#34;logits&#34;] is not None:
            softmax_accuracy = (
                (batch_kwargs[&#34;logits&#34;].max(1)[1] == batch_kwargs[&#34;labels&#34;])
                .float()
                .mean()
            )
            self.softaccuracy.append(softmax_accuracy.cpu().item())
        else:
            self.softaccuracy.append(0)

    def evaluate_impl(self):
        self.model.eval()
        features, logits, labels = [], [], []
        with torch.no_grad():
            for batch in tqdm.tqdm(
                self.test_loader, total=len(self.test_loader), leave=False
            ):
                data, label = batch
                data = data.cuda()
                logit, feature, _ = self.model(data)
                feature = feature.detach().cpu()
                logit = logit.detach().cpu()
                features.append(feature)
                logits.append(logit)
                labels.append(label)

        features, logits, labels = (
            torch.cat(features, dim=0),
            torch.cat(logits, dim=0),
            torch.cat(labels, dim=0),
        )
        # Now we compute the loss...
        self.logger.info(&#34;Obtained features, validation in progress&#34;)
        # for evaluation...

        logit_labels = torch.argmax(logits, dim=1)
        accuracy = (logit_labels == labels).sum().float() / float(labels.size(0))
        micro_fscore = np.mean(f1_score(labels, logit_labels, average=&#34;micro&#34;))
        weighted_fscore = np.mean(f1_score(labels, logit_labels, average=&#34;weighted&#34;))
        self.logger.info(&#34;Accuracy: {:.3%}&#34;.format(accuracy))
        self.logger.info(&#34;Micro F-score: {:.3f}&#34;.format(micro_fscore))
        self.logger.info(&#34;Weighted F-score: {:.3f}&#34;.format(weighted_fscore))
        return logit_labels, labels, features

    def saveMetadata(self,):
        self.logger.info(&#34;Saving model metadata&#34;)
        jMetadata = json.dumps(self.metadata)
        metafile = &#34;metadata.json&#34;
        localmetafile = os.path.join(self.save_directory, metafile)
        if self.save_backup:
            backupmetafile = os.path.join(self.backup_directory, metafile)
        if not os.path.exists(localmetafile):
            with open(localmetafile, &#34;w&#34;) as localmetaobj:
                localmetaobj.write(jMetadata)
        self.logger.info(&#34;Backing up metadata&#34;)
        if self.save_backup:
            shutil.copy2(localmetafile, backupmetafile)
        self.logger.info(&#34;Finished metadata backup&#34;)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="ednaml.trainer.BaseTrainer.BaseTrainer" href="BaseTrainer.html#ednaml.trainer.BaseTrainer.BaseTrainer">BaseTrainer</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="ednaml.trainer.ClassificationTrainer.ClassificationTrainer.epochs"><code class="name">var <span class="ident">epochs</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.trainer.ClassificationTrainer.ClassificationTrainer.global_batch"><code class="name">var <span class="ident">global_batch</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.trainer.ClassificationTrainer.ClassificationTrainer.global_epoch"><code class="name">var <span class="ident">global_epoch</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.trainer.ClassificationTrainer.ClassificationTrainer.labelMetadata"><code class="name">var <span class="ident">labelMetadata</span> : <a title="ednaml.utils.LabelMetadata.LabelMetadata" href="../utils/LabelMetadata.html#ednaml.utils.LabelMetadata.LabelMetadata">LabelMetadata</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.trainer.ClassificationTrainer.ClassificationTrainer.logger"><code class="name">var <span class="ident">logger</span> : logging.Logger</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.trainer.ClassificationTrainer.ClassificationTrainer.loss_fn"><code class="name">var <span class="ident">loss_fn</span> : Dict[str, <a title="ednaml.loss.builders.LossBuilder" href="../loss/builders/index.html#ednaml.loss.builders.LossBuilder">LossBuilder</a>]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.trainer.ClassificationTrainer.ClassificationTrainer.loss_scheduler"><code class="name">var <span class="ident">loss_scheduler</span> : Dict[str, List[torch.optim.lr_scheduler._LRScheduler]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.trainer.ClassificationTrainer.ClassificationTrainer.losses"><code class="name">var <span class="ident">losses</span> : Dict[str, List[int]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.trainer.ClassificationTrainer.ClassificationTrainer.metadata"><code class="name">var <span class="ident">metadata</span> : Dict[str, str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.trainer.ClassificationTrainer.ClassificationTrainer.model"><code class="name">var <span class="ident">model</span> : <a title="ednaml.models.ModelAbstract.ModelAbstract" href="../models/ModelAbstract.html#ednaml.models.ModelAbstract.ModelAbstract">ModelAbstract</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.trainer.ClassificationTrainer.ClassificationTrainer.num_losses"><code class="name">var <span class="ident">num_losses</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.trainer.ClassificationTrainer.ClassificationTrainer.optimizer"><code class="name">var <span class="ident">optimizer</span> : Dict[str, torch.optim.optimizer.Optimizer]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.trainer.ClassificationTrainer.ClassificationTrainer.scheduler"><code class="name">var <span class="ident">scheduler</span> : Dict[str, torch.optim.lr_scheduler._LRScheduler]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.trainer.ClassificationTrainer.ClassificationTrainer.skipeval"><code class="name">var <span class="ident">skipeval</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.trainer.ClassificationTrainer.ClassificationTrainer.test_loader"><code class="name">var <span class="ident">test_loader</span> : torch.utils.data.dataloader.DataLoader</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.trainer.ClassificationTrainer.ClassificationTrainer.train_loader"><code class="name">var <span class="ident">train_loader</span> : torch.utils.data.dataloader.DataLoader</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="ednaml.trainer.ClassificationTrainer.ClassificationTrainer.evaluate_impl"><code class="name flex">
<span>def <span class="ident">evaluate_impl</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluate_impl(self):
    self.model.eval()
    features, logits, labels = [], [], []
    with torch.no_grad():
        for batch in tqdm.tqdm(
            self.test_loader, total=len(self.test_loader), leave=False
        ):
            data, label = batch
            data = data.cuda()
            logit, feature, _ = self.model(data)
            feature = feature.detach().cpu()
            logit = logit.detach().cpu()
            features.append(feature)
            logits.append(logit)
            labels.append(label)

    features, logits, labels = (
        torch.cat(features, dim=0),
        torch.cat(logits, dim=0),
        torch.cat(labels, dim=0),
    )
    # Now we compute the loss...
    self.logger.info(&#34;Obtained features, validation in progress&#34;)
    # for evaluation...

    logit_labels = torch.argmax(logits, dim=1)
    accuracy = (logit_labels == labels).sum().float() / float(labels.size(0))
    micro_fscore = np.mean(f1_score(labels, logit_labels, average=&#34;micro&#34;))
    weighted_fscore = np.mean(f1_score(labels, logit_labels, average=&#34;weighted&#34;))
    self.logger.info(&#34;Accuracy: {:.3%}&#34;.format(accuracy))
    self.logger.info(&#34;Micro F-score: {:.3f}&#34;.format(micro_fscore))
    self.logger.info(&#34;Weighted F-score: {:.3f}&#34;.format(weighted_fscore))
    return logit_labels, labels, features</code></pre>
</details>
</dd>
<dt id="ednaml.trainer.ClassificationTrainer.ClassificationTrainer.saveMetadata"><code class="name flex">
<span>def <span class="ident">saveMetadata</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def saveMetadata(self,):
    self.logger.info(&#34;Saving model metadata&#34;)
    jMetadata = json.dumps(self.metadata)
    metafile = &#34;metadata.json&#34;
    localmetafile = os.path.join(self.save_directory, metafile)
    if self.save_backup:
        backupmetafile = os.path.join(self.backup_directory, metafile)
    if not os.path.exists(localmetafile):
        with open(localmetafile, &#34;w&#34;) as localmetaobj:
            localmetaobj.write(jMetadata)
    self.logger.info(&#34;Backing up metadata&#34;)
    if self.save_backup:
        shutil.copy2(localmetafile, backupmetafile)
    self.logger.info(&#34;Finished metadata backup&#34;)</code></pre>
</details>
</dd>
<dt id="ednaml.trainer.ClassificationTrainer.ClassificationTrainer.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, batch)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step(self, batch):
    batch_kwargs = {}
    (
        img,
        batch_kwargs[&#34;labels&#34;],
    ) = batch  # This is the tensor response from collate_fn
    img, batch_kwargs[&#34;labels&#34;] = img.cuda(), batch_kwargs[&#34;labels&#34;].cuda()
    # logits, features, labels
    batch_kwargs[&#34;logits&#34;], batch_kwargs[&#34;features&#34;], _ = self.model(img)
    batch_kwargs[&#34;epoch&#34;] = self.global_epoch  # For CompactContrastiveLoss

    # TODO fix this with info about how many output are in the model...from the config file!!!!!
    loss = {loss_name: None for loss_name in self.loss_fn}
    for lossname in self.loss_fn:
        loss[lossname] = self.loss_fn[lossname](**batch_kwargs)
    # if self.fp16 and self.apex is not None:
    #    with self.apex.amp.scale_loss(loss, self.optimizer) as scaled_loss:
    #        scaled_loss.backward()
    # else:
    #    loss.backward()
    lossbackward = sum(loss.values())
    lossbackward.backward()

    self.stepOptimizers()
    self.stepLossOptimizers()

    for idx, lossname in enumerate(self.loss_fn):
        self.losses[lossname].append(loss[lossname].cpu().item())

    if batch_kwargs[&#34;logits&#34;] is not None:
        softmax_accuracy = (
            (batch_kwargs[&#34;logits&#34;].max(1)[1] == batch_kwargs[&#34;labels&#34;])
            .float()
            .mean()
        )
        self.softaccuracy.append(softmax_accuracy.cpu().item())
    else:
        self.softaccuracy.append(0)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="ednaml.trainer.BaseTrainer.BaseTrainer" href="BaseTrainer.html#ednaml.trainer.BaseTrainer.BaseTrainer">BaseTrainer</a></b></code>:
<ul class="hlist">
<li><code><a title="ednaml.trainer.BaseTrainer.BaseTrainer.epoch_step" href="BaseTrainer.html#ednaml.trainer.BaseTrainer.BaseTrainer.epoch_step">epoch_step</a></code></li>
<li><code><a title="ednaml.trainer.BaseTrainer.BaseTrainer.initial_evaluate" href="BaseTrainer.html#ednaml.trainer.BaseTrainer.BaseTrainer.initial_evaluate">initial_evaluate</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="ednaml.trainer" href="index.html">ednaml.trainer</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="ednaml.trainer.ClassificationTrainer.ClassificationTrainer" href="#ednaml.trainer.ClassificationTrainer.ClassificationTrainer">ClassificationTrainer</a></code></h4>
<ul class="two-column">
<li><code><a title="ednaml.trainer.ClassificationTrainer.ClassificationTrainer.epochs" href="#ednaml.trainer.ClassificationTrainer.ClassificationTrainer.epochs">epochs</a></code></li>
<li><code><a title="ednaml.trainer.ClassificationTrainer.ClassificationTrainer.evaluate_impl" href="#ednaml.trainer.ClassificationTrainer.ClassificationTrainer.evaluate_impl">evaluate_impl</a></code></li>
<li><code><a title="ednaml.trainer.ClassificationTrainer.ClassificationTrainer.global_batch" href="#ednaml.trainer.ClassificationTrainer.ClassificationTrainer.global_batch">global_batch</a></code></li>
<li><code><a title="ednaml.trainer.ClassificationTrainer.ClassificationTrainer.global_epoch" href="#ednaml.trainer.ClassificationTrainer.ClassificationTrainer.global_epoch">global_epoch</a></code></li>
<li><code><a title="ednaml.trainer.ClassificationTrainer.ClassificationTrainer.labelMetadata" href="#ednaml.trainer.ClassificationTrainer.ClassificationTrainer.labelMetadata">labelMetadata</a></code></li>
<li><code><a title="ednaml.trainer.ClassificationTrainer.ClassificationTrainer.logger" href="#ednaml.trainer.ClassificationTrainer.ClassificationTrainer.logger">logger</a></code></li>
<li><code><a title="ednaml.trainer.ClassificationTrainer.ClassificationTrainer.loss_fn" href="#ednaml.trainer.ClassificationTrainer.ClassificationTrainer.loss_fn">loss_fn</a></code></li>
<li><code><a title="ednaml.trainer.ClassificationTrainer.ClassificationTrainer.loss_scheduler" href="#ednaml.trainer.ClassificationTrainer.ClassificationTrainer.loss_scheduler">loss_scheduler</a></code></li>
<li><code><a title="ednaml.trainer.ClassificationTrainer.ClassificationTrainer.losses" href="#ednaml.trainer.ClassificationTrainer.ClassificationTrainer.losses">losses</a></code></li>
<li><code><a title="ednaml.trainer.ClassificationTrainer.ClassificationTrainer.metadata" href="#ednaml.trainer.ClassificationTrainer.ClassificationTrainer.metadata">metadata</a></code></li>
<li><code><a title="ednaml.trainer.ClassificationTrainer.ClassificationTrainer.model" href="#ednaml.trainer.ClassificationTrainer.ClassificationTrainer.model">model</a></code></li>
<li><code><a title="ednaml.trainer.ClassificationTrainer.ClassificationTrainer.num_losses" href="#ednaml.trainer.ClassificationTrainer.ClassificationTrainer.num_losses">num_losses</a></code></li>
<li><code><a title="ednaml.trainer.ClassificationTrainer.ClassificationTrainer.optimizer" href="#ednaml.trainer.ClassificationTrainer.ClassificationTrainer.optimizer">optimizer</a></code></li>
<li><code><a title="ednaml.trainer.ClassificationTrainer.ClassificationTrainer.saveMetadata" href="#ednaml.trainer.ClassificationTrainer.ClassificationTrainer.saveMetadata">saveMetadata</a></code></li>
<li><code><a title="ednaml.trainer.ClassificationTrainer.ClassificationTrainer.scheduler" href="#ednaml.trainer.ClassificationTrainer.ClassificationTrainer.scheduler">scheduler</a></code></li>
<li><code><a title="ednaml.trainer.ClassificationTrainer.ClassificationTrainer.skipeval" href="#ednaml.trainer.ClassificationTrainer.ClassificationTrainer.skipeval">skipeval</a></code></li>
<li><code><a title="ednaml.trainer.ClassificationTrainer.ClassificationTrainer.step" href="#ednaml.trainer.ClassificationTrainer.ClassificationTrainer.step">step</a></code></li>
<li><code><a title="ednaml.trainer.ClassificationTrainer.ClassificationTrainer.test_loader" href="#ednaml.trainer.ClassificationTrainer.ClassificationTrainer.test_loader">test_loader</a></code></li>
<li><code><a title="ednaml.trainer.ClassificationTrainer.ClassificationTrainer.train_loader" href="#ednaml.trainer.ClassificationTrainer.ClassificationTrainer.train_loader">train_loader</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>