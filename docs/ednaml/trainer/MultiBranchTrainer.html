<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>ednaml.trainer.MultiBranchTrainer API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ednaml.trainer.MultiBranchTrainer</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import tqdm, json
from sklearn.metrics import f1_score
import shutil
import os
import torch
import numpy as np
import ednaml.loss.builders
from typing import List
from ednaml.models.MultiBranchResnet import MultiBranchResnet
from ednaml.crawlers import Crawler
from ednaml.trainer import BaseTrainer
from ednaml.utils.LabelMetadata import LabelMetadata


class MultiBranchTrainer(BaseTrainer):
    model: MultiBranchResnet

    def __init__(
        self,
        model: MultiBranchResnet,
        loss_fn: List[ednaml.loss.builders.LossBuilder],
        optimizer: torch.optim.Optimizer,
        loss_optimizer: List[torch.optim.Optimizer],
        scheduler: torch.optim.lr_scheduler._LRScheduler,
        loss_scheduler: torch.optim.lr_scheduler._LRScheduler,
        train_loader,
        test_loader,
        epochs: int,
        skipeval,
        logger,
        crawler: Crawler,
        config,
        labels: LabelMetadata,
        **kwargs
    ):

        super().__init__(
            model,
            loss_fn,
            optimizer,
            loss_optimizer,
            scheduler,
            loss_scheduler,
            train_loader,
            test_loader,
            epochs,
            skipeval,
            logger,
            crawler,
            config,
            labels,
            **kwargs
        )

        self.softaccuracy = []

        # mapping label names and class names to their index for faster retrieval.
        # TODO some way to integrate classificationclass in DATAREADER to
        # labelnames in MODEL, so that there is less redundancy...
        self.model_labelorder = {
            item: idx for idx, item in enumerate(self.model.model_labelorder)
        }
        self.model_nameorder = {
            item: idx for idx, item in enumerate(self.model.model_nameorder)
        }
        self.data_labelorder = {
            item: idx for idx, item in enumerate(self.labelMetadata.labels)
        }
        self.model_name_label_map = {
            item[0]: item[1]
            for item in zip(self.model.model_nameorder, self.model.model_labelorder)
        }


    # Steps through a batch of data
    def step(self, batch):
        batch_kwargs = {}
        (
            img,
            batch_kwargs[&#34;labels&#34;],
        ) = batch  # This is the tensor response from collate_fn
        img, batch_kwargs[&#34;labels&#34;] = (
            img.cuda(),
            batch_kwargs[&#34;labels&#34;].cuda(),
        )  # labels are in order of labelnames
        # logits, features, labels
        batch_kwargs[&#34;logits&#34;], batch_kwargs[&#34;features&#34;], _ = self.model(
            img
        )  # logits are in order of output_classnames --&gt; model.output_classnames
        batch_kwargs[&#34;epoch&#34;] = self.global_epoch  # For CompactContrastiveLoss

        loss = {loss_name: None for loss_name in self.loss_fn}
        for lossname in loss:  # this loss targets a specific output

            akwargs = {}

            akwargs[&#34;logits&#34;] = batch_kwargs[&#34;logits&#34;][
                self.model_nameorder[lossname]
            ]  # this looks up the lossname in the outputclass names

            # Check if this is a loss for a soft target
            if lossname in self.model.soft_names:
                # we need to adjust the labels...otherwise, we do not need to adjust labels...
                akwargs[&#34;labels&#34;] = batch_kwargs[&#34;logits&#34;][
                    self.model_nameorder[self.model.soft_target_output_source]
                ]
            else:
                akwargs[&#34;labels&#34;] = batch_kwargs[&#34;labels&#34;][
                :, self.data_labelorder[self.model_name_label_map[lossname]]
                ]  # ^ditto
            akwargs[&#34;epoch&#34;] = batch_kwargs[&#34;epoch&#34;]
            loss[lossname] = self.loss_fn[lossname](**akwargs)

        lossbackward = sum(loss.values())
        lossbackward.backward()

        # for idx in range(self.num_losses):
        #    loss[idx].backward()

        self.stepOptimizers()
        self.stepLossOptimizers()

        for idx, lossname in enumerate(self.loss_fn):
            self.losses[lossname].append(loss[lossname].cpu().item())

        # if batch_kwargs[&#34;logits&#34;] is not None:
        # softmax_accuracy = (batch_kwargs[&#34;logits&#34;].max(1)[1] == batch_kwargs[&#34;labels&#34;]).float().mean()
        # self.softaccuracy.append(softmax_accuracy.cpu().item())
        # else:
        self.softaccuracy.append(0)  # TODO fix this

    def evaluate_impl(self):
        self.model.eval()
        features, logits, labels = (
            [[] for _ in range(self.model.feature_count)],
            [[] for _ in range(self.model.output_count)],
            [],
        )
        with torch.no_grad():
            for batch in tqdm.tqdm(
                self.test_loader, total=len(self.test_loader), leave=False
            ):
                data, label = batch
                data = data.cuda()
                logit, feature, _ = self.model(data)
                for idx in range(self.model.feature_count):
                    features[idx].append(feature[idx].detach().cpu())
                for idx in range(self.model.output_count):
                    logits[idx].append(logit[idx].detach().cpu())
                labels.append(label)

        # features, logits, labels = torch.cat(features, dim=0), [torch.cat(logit, dim=0) for logit in logits], torch.cat(labels, dim=0)
        features = [torch.cat(feature, dim=0) for feature in features]
        logits = [torch.cat(logit, dim=0) for logit in logits]
        labels = torch.cat(labels, dim=0)
        # Now we compute the loss...
        self.logger.info(&#34;Obtained features, validation in progress&#34;)
        # for evaluation...

        logit_labels = [torch.argmax(logit, dim=1) for logit in logits]
        accuracy = [[] for _ in range(self.model.output_count)]
        micro_fscore = [[] for _ in range(self.model.output_count)]
        weighted_fscore = [[] for _ in range(self.model.output_count)]

        &#34;&#34;&#34;
        akwargs[&#34;logits&#34;] = batch_kwargs[&#34;logits&#34;][self.model_labelorder[self.model_name_label_map[lossname]]] # this looks up the lossname in the outputclass names
        akwargs[&#34;labels&#34;] = batch_kwargs[&#34;labels&#34;][:, self.data_labelorder[self.model_name_label_map[lossname]]] # ^ditto
        &#34;&#34;&#34;
        for idx, lossname in enumerate(self.loss_fn):
            accuracy[idx] = (
                logit_labels[self.model_nameorder[lossname]]
                == labels[:, self.data_labelorder[self.model_name_label_map[lossname]]]
            ).sum().float() / float(labels.size(0))
            micro_fscore[idx] = np.mean(
                f1_score(
                    labels[
                        :, self.data_labelorder[self.model_name_label_map[lossname]]
                    ],
                    logit_labels[
                        self.model_nameorder[lossname]
                    ],
                    average=&#34;micro&#34;,
                )
            )
            weighted_fscore[idx] = np.mean(
                f1_score(
                    labels[
                        :, self.data_labelorder[self.model_name_label_map[lossname]]
                    ],
                    logit_labels[
                        self.model_nameorder[lossname]
                    ],
                    average=&#34;weighted&#34;,
                )
            )

        self.logger.info(
            &#34;Metrics\t&#34; + &#34;\t&#34;.join([&#34;%s&#34; % lossname for lossname in self.loss_fn])
        )
        self.logger.info(
            &#34;Accuracy\t&#34;
            + &#34;\t&#34;.join(
                [
                    &#34;%s: %0.3f&#34;
                    % (self.model.model_nameorder[idx], accuracy[idx].item())
                    for idx in range(self.model.output_count)
                ]
            )
        )
        self.logger.info(
            &#34;M F-Score\t&#34;
            + &#34;\t&#34;.join(
                [
                    &#34;%s: %0.3f&#34;
                    % (self.model.model_nameorder[idx], micro_fscore[idx].item())
                    for idx in range(self.model.output_count)
                ]
            )
        )
        self.logger.info(
            &#34;W F-Score\t&#34;
            + &#34;\t&#34;.join(
                [
                    &#34;%s: %0.3f&#34;
                    % (self.model.model_nameorder[idx], weighted_fscore[idx].item())
                    for idx in range(self.model.output_count)
                ]
            )
        )

        return logit_labels, labels, features

    def saveMetadata(self,):
        self.logger.info(&#34;Saving model metadata&#34;)
        jMetadata = json.dumps(self.metadata)
        metafile = &#34;metadata.json&#34;
        localmetafile = os.path.join(self.save_directory, metafile)
        if self.save_backup:
            backupmetafile = os.path.join(self.backup_directory, metafile)
        if not os.path.exists(localmetafile):
            with open(localmetafile, &#34;w&#34;) as localmetaobj:
                localmetaobj.write(jMetadata)
        self.logger.info(&#34;Backing up metadata&#34;)
        if self.save_backup:
            shutil.copy2(localmetafile, backupmetafile)
        self.logger.info(&#34;Finished metadata backup&#34;)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="ednaml.trainer.MultiBranchTrainer.MultiBranchTrainer"><code class="flex name class">
<span>class <span class="ident">MultiBranchTrainer</span></span>
<span>(</span><span>model: <a title="ednaml.models.MultiBranchResnet.MultiBranchResnet" href="../models/MultiBranchResnet.html#ednaml.models.MultiBranchResnet.MultiBranchResnet">MultiBranchResnet</a>, loss_fn: List[<a title="ednaml.loss.builders.LossBuilder" href="../loss/builders/index.html#ednaml.loss.builders.LossBuilder">LossBuilder</a>], optimizer: torch.optim.optimizer.Optimizer, loss_optimizer: List[torch.optim.optimizer.Optimizer], scheduler: torch.optim.lr_scheduler._LRScheduler, loss_scheduler: torch.optim.lr_scheduler._LRScheduler, train_loader, test_loader, epochs: int, skipeval, logger, crawler: <a title="ednaml.crawlers.Crawler" href="../crawlers/index.html#ednaml.crawlers.Crawler">Crawler</a>, config, labels: <a title="ednaml.utils.LabelMetadata.LabelMetadata" href="../utils/LabelMetadata.html#ednaml.utils.LabelMetadata.LabelMetadata">LabelMetadata</a>, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MultiBranchTrainer(BaseTrainer):
    model: MultiBranchResnet

    def __init__(
        self,
        model: MultiBranchResnet,
        loss_fn: List[ednaml.loss.builders.LossBuilder],
        optimizer: torch.optim.Optimizer,
        loss_optimizer: List[torch.optim.Optimizer],
        scheduler: torch.optim.lr_scheduler._LRScheduler,
        loss_scheduler: torch.optim.lr_scheduler._LRScheduler,
        train_loader,
        test_loader,
        epochs: int,
        skipeval,
        logger,
        crawler: Crawler,
        config,
        labels: LabelMetadata,
        **kwargs
    ):

        super().__init__(
            model,
            loss_fn,
            optimizer,
            loss_optimizer,
            scheduler,
            loss_scheduler,
            train_loader,
            test_loader,
            epochs,
            skipeval,
            logger,
            crawler,
            config,
            labels,
            **kwargs
        )

        self.softaccuracy = []

        # mapping label names and class names to their index for faster retrieval.
        # TODO some way to integrate classificationclass in DATAREADER to
        # labelnames in MODEL, so that there is less redundancy...
        self.model_labelorder = {
            item: idx for idx, item in enumerate(self.model.model_labelorder)
        }
        self.model_nameorder = {
            item: idx for idx, item in enumerate(self.model.model_nameorder)
        }
        self.data_labelorder = {
            item: idx for idx, item in enumerate(self.labelMetadata.labels)
        }
        self.model_name_label_map = {
            item[0]: item[1]
            for item in zip(self.model.model_nameorder, self.model.model_labelorder)
        }


    # Steps through a batch of data
    def step(self, batch):
        batch_kwargs = {}
        (
            img,
            batch_kwargs[&#34;labels&#34;],
        ) = batch  # This is the tensor response from collate_fn
        img, batch_kwargs[&#34;labels&#34;] = (
            img.cuda(),
            batch_kwargs[&#34;labels&#34;].cuda(),
        )  # labels are in order of labelnames
        # logits, features, labels
        batch_kwargs[&#34;logits&#34;], batch_kwargs[&#34;features&#34;], _ = self.model(
            img
        )  # logits are in order of output_classnames --&gt; model.output_classnames
        batch_kwargs[&#34;epoch&#34;] = self.global_epoch  # For CompactContrastiveLoss

        loss = {loss_name: None for loss_name in self.loss_fn}
        for lossname in loss:  # this loss targets a specific output

            akwargs = {}

            akwargs[&#34;logits&#34;] = batch_kwargs[&#34;logits&#34;][
                self.model_nameorder[lossname]
            ]  # this looks up the lossname in the outputclass names

            # Check if this is a loss for a soft target
            if lossname in self.model.soft_names:
                # we need to adjust the labels...otherwise, we do not need to adjust labels...
                akwargs[&#34;labels&#34;] = batch_kwargs[&#34;logits&#34;][
                    self.model_nameorder[self.model.soft_target_output_source]
                ]
            else:
                akwargs[&#34;labels&#34;] = batch_kwargs[&#34;labels&#34;][
                :, self.data_labelorder[self.model_name_label_map[lossname]]
                ]  # ^ditto
            akwargs[&#34;epoch&#34;] = batch_kwargs[&#34;epoch&#34;]
            loss[lossname] = self.loss_fn[lossname](**akwargs)

        lossbackward = sum(loss.values())
        lossbackward.backward()

        # for idx in range(self.num_losses):
        #    loss[idx].backward()

        self.stepOptimizers()
        self.stepLossOptimizers()

        for idx, lossname in enumerate(self.loss_fn):
            self.losses[lossname].append(loss[lossname].cpu().item())

        # if batch_kwargs[&#34;logits&#34;] is not None:
        # softmax_accuracy = (batch_kwargs[&#34;logits&#34;].max(1)[1] == batch_kwargs[&#34;labels&#34;]).float().mean()
        # self.softaccuracy.append(softmax_accuracy.cpu().item())
        # else:
        self.softaccuracy.append(0)  # TODO fix this

    def evaluate_impl(self):
        self.model.eval()
        features, logits, labels = (
            [[] for _ in range(self.model.feature_count)],
            [[] for _ in range(self.model.output_count)],
            [],
        )
        with torch.no_grad():
            for batch in tqdm.tqdm(
                self.test_loader, total=len(self.test_loader), leave=False
            ):
                data, label = batch
                data = data.cuda()
                logit, feature, _ = self.model(data)
                for idx in range(self.model.feature_count):
                    features[idx].append(feature[idx].detach().cpu())
                for idx in range(self.model.output_count):
                    logits[idx].append(logit[idx].detach().cpu())
                labels.append(label)

        # features, logits, labels = torch.cat(features, dim=0), [torch.cat(logit, dim=0) for logit in logits], torch.cat(labels, dim=0)
        features = [torch.cat(feature, dim=0) for feature in features]
        logits = [torch.cat(logit, dim=0) for logit in logits]
        labels = torch.cat(labels, dim=0)
        # Now we compute the loss...
        self.logger.info(&#34;Obtained features, validation in progress&#34;)
        # for evaluation...

        logit_labels = [torch.argmax(logit, dim=1) for logit in logits]
        accuracy = [[] for _ in range(self.model.output_count)]
        micro_fscore = [[] for _ in range(self.model.output_count)]
        weighted_fscore = [[] for _ in range(self.model.output_count)]

        &#34;&#34;&#34;
        akwargs[&#34;logits&#34;] = batch_kwargs[&#34;logits&#34;][self.model_labelorder[self.model_name_label_map[lossname]]] # this looks up the lossname in the outputclass names
        akwargs[&#34;labels&#34;] = batch_kwargs[&#34;labels&#34;][:, self.data_labelorder[self.model_name_label_map[lossname]]] # ^ditto
        &#34;&#34;&#34;
        for idx, lossname in enumerate(self.loss_fn):
            accuracy[idx] = (
                logit_labels[self.model_nameorder[lossname]]
                == labels[:, self.data_labelorder[self.model_name_label_map[lossname]]]
            ).sum().float() / float(labels.size(0))
            micro_fscore[idx] = np.mean(
                f1_score(
                    labels[
                        :, self.data_labelorder[self.model_name_label_map[lossname]]
                    ],
                    logit_labels[
                        self.model_nameorder[lossname]
                    ],
                    average=&#34;micro&#34;,
                )
            )
            weighted_fscore[idx] = np.mean(
                f1_score(
                    labels[
                        :, self.data_labelorder[self.model_name_label_map[lossname]]
                    ],
                    logit_labels[
                        self.model_nameorder[lossname]
                    ],
                    average=&#34;weighted&#34;,
                )
            )

        self.logger.info(
            &#34;Metrics\t&#34; + &#34;\t&#34;.join([&#34;%s&#34; % lossname for lossname in self.loss_fn])
        )
        self.logger.info(
            &#34;Accuracy\t&#34;
            + &#34;\t&#34;.join(
                [
                    &#34;%s: %0.3f&#34;
                    % (self.model.model_nameorder[idx], accuracy[idx].item())
                    for idx in range(self.model.output_count)
                ]
            )
        )
        self.logger.info(
            &#34;M F-Score\t&#34;
            + &#34;\t&#34;.join(
                [
                    &#34;%s: %0.3f&#34;
                    % (self.model.model_nameorder[idx], micro_fscore[idx].item())
                    for idx in range(self.model.output_count)
                ]
            )
        )
        self.logger.info(
            &#34;W F-Score\t&#34;
            + &#34;\t&#34;.join(
                [
                    &#34;%s: %0.3f&#34;
                    % (self.model.model_nameorder[idx], weighted_fscore[idx].item())
                    for idx in range(self.model.output_count)
                ]
            )
        )

        return logit_labels, labels, features

    def saveMetadata(self,):
        self.logger.info(&#34;Saving model metadata&#34;)
        jMetadata = json.dumps(self.metadata)
        metafile = &#34;metadata.json&#34;
        localmetafile = os.path.join(self.save_directory, metafile)
        if self.save_backup:
            backupmetafile = os.path.join(self.backup_directory, metafile)
        if not os.path.exists(localmetafile):
            with open(localmetafile, &#34;w&#34;) as localmetaobj:
                localmetaobj.write(jMetadata)
        self.logger.info(&#34;Backing up metadata&#34;)
        if self.save_backup:
            shutil.copy2(localmetafile, backupmetafile)
        self.logger.info(&#34;Finished metadata backup&#34;)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="ednaml.trainer.BaseTrainer.BaseTrainer" href="BaseTrainer.html#ednaml.trainer.BaseTrainer.BaseTrainer">BaseTrainer</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="ednaml.trainer.MultiBranchTrainer.MultiBranchTrainer.model"><code class="name">var <span class="ident">model</span> : <a title="ednaml.models.MultiBranchResnet.MultiBranchResnet" href="../models/MultiBranchResnet.html#ednaml.models.MultiBranchResnet.MultiBranchResnet">MultiBranchResnet</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="ednaml.trainer.MultiBranchTrainer.MultiBranchTrainer.evaluate_impl"><code class="name flex">
<span>def <span class="ident">evaluate_impl</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluate_impl(self):
    self.model.eval()
    features, logits, labels = (
        [[] for _ in range(self.model.feature_count)],
        [[] for _ in range(self.model.output_count)],
        [],
    )
    with torch.no_grad():
        for batch in tqdm.tqdm(
            self.test_loader, total=len(self.test_loader), leave=False
        ):
            data, label = batch
            data = data.cuda()
            logit, feature, _ = self.model(data)
            for idx in range(self.model.feature_count):
                features[idx].append(feature[idx].detach().cpu())
            for idx in range(self.model.output_count):
                logits[idx].append(logit[idx].detach().cpu())
            labels.append(label)

    # features, logits, labels = torch.cat(features, dim=0), [torch.cat(logit, dim=0) for logit in logits], torch.cat(labels, dim=0)
    features = [torch.cat(feature, dim=0) for feature in features]
    logits = [torch.cat(logit, dim=0) for logit in logits]
    labels = torch.cat(labels, dim=0)
    # Now we compute the loss...
    self.logger.info(&#34;Obtained features, validation in progress&#34;)
    # for evaluation...

    logit_labels = [torch.argmax(logit, dim=1) for logit in logits]
    accuracy = [[] for _ in range(self.model.output_count)]
    micro_fscore = [[] for _ in range(self.model.output_count)]
    weighted_fscore = [[] for _ in range(self.model.output_count)]

    &#34;&#34;&#34;
    akwargs[&#34;logits&#34;] = batch_kwargs[&#34;logits&#34;][self.model_labelorder[self.model_name_label_map[lossname]]] # this looks up the lossname in the outputclass names
    akwargs[&#34;labels&#34;] = batch_kwargs[&#34;labels&#34;][:, self.data_labelorder[self.model_name_label_map[lossname]]] # ^ditto
    &#34;&#34;&#34;
    for idx, lossname in enumerate(self.loss_fn):
        accuracy[idx] = (
            logit_labels[self.model_nameorder[lossname]]
            == labels[:, self.data_labelorder[self.model_name_label_map[lossname]]]
        ).sum().float() / float(labels.size(0))
        micro_fscore[idx] = np.mean(
            f1_score(
                labels[
                    :, self.data_labelorder[self.model_name_label_map[lossname]]
                ],
                logit_labels[
                    self.model_nameorder[lossname]
                ],
                average=&#34;micro&#34;,
            )
        )
        weighted_fscore[idx] = np.mean(
            f1_score(
                labels[
                    :, self.data_labelorder[self.model_name_label_map[lossname]]
                ],
                logit_labels[
                    self.model_nameorder[lossname]
                ],
                average=&#34;weighted&#34;,
            )
        )

    self.logger.info(
        &#34;Metrics\t&#34; + &#34;\t&#34;.join([&#34;%s&#34; % lossname for lossname in self.loss_fn])
    )
    self.logger.info(
        &#34;Accuracy\t&#34;
        + &#34;\t&#34;.join(
            [
                &#34;%s: %0.3f&#34;
                % (self.model.model_nameorder[idx], accuracy[idx].item())
                for idx in range(self.model.output_count)
            ]
        )
    )
    self.logger.info(
        &#34;M F-Score\t&#34;
        + &#34;\t&#34;.join(
            [
                &#34;%s: %0.3f&#34;
                % (self.model.model_nameorder[idx], micro_fscore[idx].item())
                for idx in range(self.model.output_count)
            ]
        )
    )
    self.logger.info(
        &#34;W F-Score\t&#34;
        + &#34;\t&#34;.join(
            [
                &#34;%s: %0.3f&#34;
                % (self.model.model_nameorder[idx], weighted_fscore[idx].item())
                for idx in range(self.model.output_count)
            ]
        )
    )

    return logit_labels, labels, features</code></pre>
</details>
</dd>
<dt id="ednaml.trainer.MultiBranchTrainer.MultiBranchTrainer.saveMetadata"><code class="name flex">
<span>def <span class="ident">saveMetadata</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def saveMetadata(self,):
    self.logger.info(&#34;Saving model metadata&#34;)
    jMetadata = json.dumps(self.metadata)
    metafile = &#34;metadata.json&#34;
    localmetafile = os.path.join(self.save_directory, metafile)
    if self.save_backup:
        backupmetafile = os.path.join(self.backup_directory, metafile)
    if not os.path.exists(localmetafile):
        with open(localmetafile, &#34;w&#34;) as localmetaobj:
            localmetaobj.write(jMetadata)
    self.logger.info(&#34;Backing up metadata&#34;)
    if self.save_backup:
        shutil.copy2(localmetafile, backupmetafile)
    self.logger.info(&#34;Finished metadata backup&#34;)</code></pre>
</details>
</dd>
<dt id="ednaml.trainer.MultiBranchTrainer.MultiBranchTrainer.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, batch)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step(self, batch):
    batch_kwargs = {}
    (
        img,
        batch_kwargs[&#34;labels&#34;],
    ) = batch  # This is the tensor response from collate_fn
    img, batch_kwargs[&#34;labels&#34;] = (
        img.cuda(),
        batch_kwargs[&#34;labels&#34;].cuda(),
    )  # labels are in order of labelnames
    # logits, features, labels
    batch_kwargs[&#34;logits&#34;], batch_kwargs[&#34;features&#34;], _ = self.model(
        img
    )  # logits are in order of output_classnames --&gt; model.output_classnames
    batch_kwargs[&#34;epoch&#34;] = self.global_epoch  # For CompactContrastiveLoss

    loss = {loss_name: None for loss_name in self.loss_fn}
    for lossname in loss:  # this loss targets a specific output

        akwargs = {}

        akwargs[&#34;logits&#34;] = batch_kwargs[&#34;logits&#34;][
            self.model_nameorder[lossname]
        ]  # this looks up the lossname in the outputclass names

        # Check if this is a loss for a soft target
        if lossname in self.model.soft_names:
            # we need to adjust the labels...otherwise, we do not need to adjust labels...
            akwargs[&#34;labels&#34;] = batch_kwargs[&#34;logits&#34;][
                self.model_nameorder[self.model.soft_target_output_source]
            ]
        else:
            akwargs[&#34;labels&#34;] = batch_kwargs[&#34;labels&#34;][
            :, self.data_labelorder[self.model_name_label_map[lossname]]
            ]  # ^ditto
        akwargs[&#34;epoch&#34;] = batch_kwargs[&#34;epoch&#34;]
        loss[lossname] = self.loss_fn[lossname](**akwargs)

    lossbackward = sum(loss.values())
    lossbackward.backward()

    # for idx in range(self.num_losses):
    #    loss[idx].backward()

    self.stepOptimizers()
    self.stepLossOptimizers()

    for idx, lossname in enumerate(self.loss_fn):
        self.losses[lossname].append(loss[lossname].cpu().item())

    # if batch_kwargs[&#34;logits&#34;] is not None:
    # softmax_accuracy = (batch_kwargs[&#34;logits&#34;].max(1)[1] == batch_kwargs[&#34;labels&#34;]).float().mean()
    # self.softaccuracy.append(softmax_accuracy.cpu().item())
    # else:
    self.softaccuracy.append(0)  # TODO fix this</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="ednaml.trainer.BaseTrainer.BaseTrainer" href="BaseTrainer.html#ednaml.trainer.BaseTrainer.BaseTrainer">BaseTrainer</a></b></code>:
<ul class="hlist">
<li><code><a title="ednaml.trainer.BaseTrainer.BaseTrainer.epoch_step" href="BaseTrainer.html#ednaml.trainer.BaseTrainer.BaseTrainer.epoch_step">epoch_step</a></code></li>
<li><code><a title="ednaml.trainer.BaseTrainer.BaseTrainer.initial_evaluate" href="BaseTrainer.html#ednaml.trainer.BaseTrainer.BaseTrainer.initial_evaluate">initial_evaluate</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="ednaml.trainer" href="index.html">ednaml.trainer</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="ednaml.trainer.MultiBranchTrainer.MultiBranchTrainer" href="#ednaml.trainer.MultiBranchTrainer.MultiBranchTrainer">MultiBranchTrainer</a></code></h4>
<ul class="">
<li><code><a title="ednaml.trainer.MultiBranchTrainer.MultiBranchTrainer.evaluate_impl" href="#ednaml.trainer.MultiBranchTrainer.MultiBranchTrainer.evaluate_impl">evaluate_impl</a></code></li>
<li><code><a title="ednaml.trainer.MultiBranchTrainer.MultiBranchTrainer.model" href="#ednaml.trainer.MultiBranchTrainer.MultiBranchTrainer.model">model</a></code></li>
<li><code><a title="ednaml.trainer.MultiBranchTrainer.MultiBranchTrainer.saveMetadata" href="#ednaml.trainer.MultiBranchTrainer.MultiBranchTrainer.saveMetadata">saveMetadata</a></code></li>
<li><code><a title="ednaml.trainer.MultiBranchTrainer.MultiBranchTrainer.step" href="#ednaml.trainer.MultiBranchTrainer.MultiBranchTrainer.step">step</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>