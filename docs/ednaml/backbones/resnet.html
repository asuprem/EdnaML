<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>ednaml.backbones.resnet API documentation</title>
<meta name="description" content="Contains code to build a resnet â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ednaml.backbones.resnet</code></h1>
</header>
<section id="section-intro">
<p>Contains code to build a resnet</p>
<p>Typical usage example:</p>
<p>model = resnet()
x = torch.randn((3,100,100))
features = model(x)</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Contains code to build a resnet

  Typical usage example:

  model = resnet()
  x = torch.randn((3,100,100))
  features = model(x)
&#34;&#34;&#34;

from typing import List
from torch import nn
import torch

from ednaml.utils.blocks import InputAttention
from ednaml.utils.blocks import ResnetBasicBlock as BasicBlock
from ednaml.utils.blocks import ResnetBottleneck as Bottleneck


class resnet(nn.Module):
    def __init__(
        self,
        block: nn.Module = Bottleneck,
        layers: List[int] = [3, 4, 6, 3],
        last_stride: int = 2,
        zero_init_residual: bool = False,
        top_only: bool = True,
        num_classes: bool = 1000,
        groups: int = 1,
        width_per_group: int = 64,
        replace_stride_with_dilation: List[int] = None,
        norm_layer: nn.Module = None,
        attention: str = None,
        input_attention: bool = None,
        secondary_attention: int = None,
        ia_attention: bool = None,
        part_attention: bool = None,
        **kwargs
    ):
        &#34;&#34;&#34;Initializes the resnet model.


        Args:
            block (nn.Module, optional): The block (BasicBlock or Bottleneck) used for the internal Resnet architecture. Defaults to ResnetBottleneck.
            layers (List[int], optional): Number of layers in each block. Defaults to [3, 4, 6, 3].
            last_stride (int, optional): The stride for the last block. Defaults to 2.
            zero_init_residual (bool, optional): Whether to initialize network with only zeros. Unused. Defaults to False.
            top_only (bool, optional): Whether to keep only the feature extractor block. Defaults to True.
            num_classes (bool, optional): Number of classes for the imagenet layer. Unused. Defaults to 1000.
            groups (int, optional): see nn.conv2D. Defaults to 1.
            width_per_group (int, optional): see nn.conv2D. Defaults to 64.
            replace_stride_with_dilation (List[int], optional): Whether to replace stride with dilation for each block. Defaults to None.
            norm_layer (nn.Module, optional): The default normalization layer. If None, uses batchnorm. Defaults to None.
            attention (str, optional): What attention to use, among `cbam`, `dbam`. Defaults to None.
            input_attention (bool, optional): Whether to use input attention at the first resnet block. Defaults to None.
            secondary_attention (int, optional): Whether to use secondary attention to apply `attention` to the specific resnet block only. Defaults to None.
            ia_attention (bool, optional): Whether to use input attention at the first conv layer. Exclusive with `input_attention`. Defaults to None.
            part_attention (bool, optional): Whether to use the local attention module. Defaults to None.

        Raises:
            ValueError: If attention blocks are not self-consistent. Specifically, the following rules:
                - cannot have both `ia_attention` and `input_attention`.
                - cannot have `part_attention` with `attention`, unless `secondary_attention`!=1
            ValueError: If `replace_stride_with_dilation` is not a 3-tuple or None.
        &#34;&#34;&#34;
        super().__init__()

        self.attention = attention
        self.input_attention = input_attention
        self.secondary_attention = secondary_attention
        self.block = block
        self.inplanes = 64
        if norm_layer is None:
            self._norm_layer = nn.BatchNorm2d
        # elif norm_layer == &#34;ln&#34;:
        #    self._norm_layer = nn.LayerNorm
        self.dilation = 1
        if replace_stride_with_dilation is None:
            replace_stride_with_dilation = [False, False, False]
        if len(replace_stride_with_dilation) != 3:
            raise ValueError(
                &#34;replace_stride_with_dilation should be `None` or a 3-element tuple. Got {}&#34;.format(
                    replace_stride_with_dilation
                )
            )
        self.groups = groups
        self.base_width = width_per_group

        self.conv1 = nn.Conv2d(
            3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False
        )
        # if norm_layer == &#34;gn&#34;:
        #    self.bn1 = nn.GroupNorm2d
        self.bn1 = nn.BatchNorm2d(self.inplanes)
        self.relu1 = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

        self.ia_attention = ia_attention
        self.part_attention = part_attention

        # Make sure ia and input_attention do not conflict
        if self.ia_attention is not None and self.input_attention is not None:
            raise ValueError(&#34;Cannot have both ia_attention and input_attention.&#34;)
        if self.part_attention is not None and (
            self.attention is not None and self.secondary_attention is None
        ):
            raise ValueError(&#34;Cannot have part-attention with CBAM everywhere&#34;)
        if self.part_attention is not None and (
            self.attention is not None and self.secondary_attention == 1
        ):
            raise ValueError(&#34;Cannot have part-attention with CBAM-Early&#34;)

        # Create true IA
        if self.ia_attention:
            self.ia_attention = InputAttention(self.inplanes)  # 64, set above
        else:
            self.ia_attention = None

        att = self.attention
        if (
            secondary_attention is not None and secondary_attention != 1
        ):  # leave alone if sec attention not set
            att = None
        self.layer1 = self._make_layer(
            self.block,
            64,
            layers[0],
            attention=att,
            input_attention=self.input_attention,
            part_attention=self.part_attention,
        )
        att = self.attention
        if (
            secondary_attention is not None and secondary_attention != 2
        ):  # leave alone if sec attention not set
            att = None
        self.layer2 = self._make_layer(
            self.block,
            128,
            layers[1],
            stride=2,
            attention=att,
            dilate=replace_stride_with_dilation[0],
        )
        att = self.attention
        if (
            secondary_attention is not None and secondary_attention != 3
        ):  # leave alone if sec attention not set
            att = None
        self.layer3 = self._make_layer(
            self.block,
            256,
            layers[2],
            stride=2,
            attention=att,
            dilate=replace_stride_with_dilation[1],
        )
        att = self.attention
        if (
            secondary_attention is not None and secondary_attention != 4
        ):  # leave alone if sec attention not set
            att = None
        self.layer4 = self._make_layer(
            self.block,
            512,
            layers[3],
            stride=last_stride,
            attention=att,
            dilate=replace_stride_with_dilation[2],
        )

        self.top_only = top_only
        self.avgpool, self.fc = None, None

        if not self.top_only:
            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
            self.fc = nn.Linear(512 * block.expansion, num_classes)

    def _make_layer(
        self,
        block: nn.Module,
        planes: int,
        blocks: int,
        stride: int = 1,
        dilate: bool = False,
        attention: str = None,
        input_attention: bool = False,
        ia_attention: bool = False,
        part_attention: bool = False,
    ) -&gt; nn.Sequential:
        &#34;&#34;&#34;Creates a resnet block

        Args:
            block (nn.Module): The block (BasicBlock or Bottleneck) used for the internal Resnet architecture. Defaults to ResnetBottleneck.
            planes (int): Number of input depth
            blocks (int): Number of blocks in this ResnetBlock
            stride (int, optional): Stride for the conv layers. Defaults to 1.
            dilate (bool, optional): Dilation for the conv layers. Defaults to False.
            attention (str, optional): Which of `cbam`, `dbam` attention to use. Defaults to None.
            input_attention (bool, optional): Whether to use `input_attention`. Defaults to False.
            ia_attention (bool, optional): Whether to use `ia_attention`. Unused. Defaults to False.
            part_attention (bool, optional): Whether to use local attention. Defaults to False.

        Returns:
            nn.Sequential: The layers comprising this Resnet Block as an nn.Sequential
        &#34;&#34;&#34;
        downsample = None
        previous_dilation = self.dilation
        if dilate:
            self.dilation *= stride
            stride = 1
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(
                    self.inplanes,
                    planes * block.expansion,
                    kernel_size=1,
                    stride=stride,
                    bias=False,
                ),
                self._norm_layer(planes * block.expansion),
            )

        layers = []
        layers.append(
            block(
                self.inplanes,
                planes,
                stride,
                downsample,
                groups=self.groups,
                base_width=self.base_width,
                dilation=previous_dilation,
                norm_layer=self._norm_layer,
                attention=attention,
                input_attention=input_attention,
                part_attention=part_attention,
            )
        )
        self.inplanes = planes * block.expansion
        for _ in range(1, blocks):
            layers.append(
                block(
                    self.inplanes,
                    planes,
                    groups=self.groups,
                    base_width=self.base_width,
                    dilation=self.dilation,
                    norm_layer=self._norm_layer,
                    attention=attention,
                )
            )
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv1(x)

        if self.ia_attention is not None:
            x = self.ia_attention(x) * x
        x = self.bn1(x)
        x = self.relu1(x)
        x = self.maxpool(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        if not self.top_only:
            x = self.avgpool(x)
            x = torch.flatten(x, 1)
            x = self.fc(x)
        return x

    def load_param(self, weights_path: str):
        &#34;&#34;&#34;Loads parameters frm saved weights file

        Args:
            weights_path (str): Path to the weights file
        &#34;&#34;&#34;
        param_dict = torch.load(weights_path)
        for i in param_dict:
            if &#34;fc&#34; in i and self.top_only:
                continue
            self.state_dict()[i].copy_(param_dict[i])


def _resnet(arch, block, layers, pretrained, progress, **kwargs) -&gt; resnet:
    &#34;&#34;&#34;Builds a resnet

    Args:
        arch (str): Architecture base. Unused.
        block (nn.Module): The class of the ResnetBlock (BasicBlock or Bottleneck)
        layers (List[int]): The layers for each ResnetBlock
        pretrained (bool): Unused
        progress (bool): Unused

    Returns:
        resnet: The `resnet` model
    &#34;&#34;&#34;
    model = resnet(block, layers, **kwargs)
    return model


def resnet18(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNet-18 model from
    `&#34;Deep Residual Learning for Image Recognition&#34; &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    return _resnet(&#34;resnet18&#34;, BasicBlock, [2, 2, 2, 2], pretrained, progress, **kwargs)


def resnet34(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNet-34 model from
    `&#34;Deep Residual Learning for Image Recognition&#34; &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    return _resnet(&#34;resnet34&#34;, BasicBlock, [3, 4, 6, 3], pretrained, progress, **kwargs)


def resnet50(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNet-50 model from
    `&#34;Deep Residual Learning for Image Recognition&#34; &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    return _resnet(&#34;resnet50&#34;, Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs)


def resnet101(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNet-101 model from
    `&#34;Deep Residual Learning for Image Recognition&#34; &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    return _resnet(
        &#34;resnet101&#34;, Bottleneck, [3, 4, 23, 3], pretrained, progress, **kwargs
    )


def resnet152(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNet-152 model from
    `&#34;Deep Residual Learning for Image Recognition&#34; &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    return _resnet(
        &#34;resnet152&#34;, Bottleneck, [3, 8, 36, 3], pretrained, progress, **kwargs
    )


def resnext50_32x4d(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNeXt-50 32x4d model from
    `&#34;Aggregated Residual Transformation for Deep Neural Networks&#34; &lt;https://arxiv.org/pdf/1611.05431.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    kwargs[&#34;groups&#34;] = 32
    kwargs[&#34;width_per_group&#34;] = 4
    return _resnet(
        &#34;resnext50_32x4d&#34;, Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs
    )


def resnext101_32x8d(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNeXt-101 32x8d model from
    `&#34;Aggregated Residual Transformation for Deep Neural Networks&#34; &lt;https://arxiv.org/pdf/1611.05431.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    kwargs[&#34;groups&#34;] = 32
    kwargs[&#34;width_per_group&#34;] = 8
    return _resnet(
        &#34;resnext101_32x8d&#34;, Bottleneck, [3, 4, 23, 3], pretrained, progress, **kwargs
    )


def wide_resnet50_2(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;Wide ResNet-50-2 model from
    `&#34;Wide Residual Networks&#34; &lt;https://arxiv.org/pdf/1605.07146.pdf&gt;`_
    The model is the same as ResNet except for the bottleneck number of channels
    which is twice larger in every block. The number of channels in outer 1x1
    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048
    channels, and in Wide ResNet-50-2 has 2048-1024-2048.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    kwargs[&#34;width_per_group&#34;] = 64 * 2
    return _resnet(
        &#34;wide_resnet50_2&#34;, Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs
    )


def wide_resnet101_2(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;Wide ResNet-101-2 model from
    `&#34;Wide Residual Networks&#34; &lt;https://arxiv.org/pdf/1605.07146.pdf&gt;`_
    The model is the same as ResNet except for the bottleneck number of channels
    which is twice larger in every block. The number of channels in outer 1x1
    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048
    channels, and in Wide ResNet-50-2 has 2048-1024-2048.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    kwargs[&#34;width_per_group&#34;] = 64 * 2
    return _resnet(
        &#34;wide_resnet101_2&#34;, Bottleneck, [3, 4, 23, 3], pretrained, progress, **kwargs
    )</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="ednaml.backbones.resnet.resnet101"><code class="name flex">
<span>def <span class="ident">resnet101</span></span>(<span>pretrained=False, progress=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>ResNet-101 model from
<code>"Deep Residual Learning for Image Recognition" &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;</code>_</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pretrained</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, returns a model pre-trained on ImageNet</dd>
<dt><strong><code>progress</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, displays a progress bar of the download to stderr</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resnet101(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNet-101 model from
    `&#34;Deep Residual Learning for Image Recognition&#34; &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    return _resnet(
        &#34;resnet101&#34;, Bottleneck, [3, 4, 23, 3], pretrained, progress, **kwargs
    )</code></pre>
</details>
</dd>
<dt id="ednaml.backbones.resnet.resnet152"><code class="name flex">
<span>def <span class="ident">resnet152</span></span>(<span>pretrained=False, progress=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>ResNet-152 model from
<code>"Deep Residual Learning for Image Recognition" &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;</code>_</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pretrained</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, returns a model pre-trained on ImageNet</dd>
<dt><strong><code>progress</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, displays a progress bar of the download to stderr</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resnet152(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNet-152 model from
    `&#34;Deep Residual Learning for Image Recognition&#34; &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    return _resnet(
        &#34;resnet152&#34;, Bottleneck, [3, 8, 36, 3], pretrained, progress, **kwargs
    )</code></pre>
</details>
</dd>
<dt id="ednaml.backbones.resnet.resnet18"><code class="name flex">
<span>def <span class="ident">resnet18</span></span>(<span>pretrained=False, progress=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>ResNet-18 model from
<code>"Deep Residual Learning for Image Recognition" &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;</code>_</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pretrained</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, returns a model pre-trained on ImageNet</dd>
<dt><strong><code>progress</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, displays a progress bar of the download to stderr</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resnet18(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNet-18 model from
    `&#34;Deep Residual Learning for Image Recognition&#34; &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    return _resnet(&#34;resnet18&#34;, BasicBlock, [2, 2, 2, 2], pretrained, progress, **kwargs)</code></pre>
</details>
</dd>
<dt id="ednaml.backbones.resnet.resnet34"><code class="name flex">
<span>def <span class="ident">resnet34</span></span>(<span>pretrained=False, progress=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>ResNet-34 model from
<code>"Deep Residual Learning for Image Recognition" &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;</code>_</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pretrained</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, returns a model pre-trained on ImageNet</dd>
<dt><strong><code>progress</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, displays a progress bar of the download to stderr</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resnet34(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNet-34 model from
    `&#34;Deep Residual Learning for Image Recognition&#34; &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    return _resnet(&#34;resnet34&#34;, BasicBlock, [3, 4, 6, 3], pretrained, progress, **kwargs)</code></pre>
</details>
</dd>
<dt id="ednaml.backbones.resnet.resnet50"><code class="name flex">
<span>def <span class="ident">resnet50</span></span>(<span>pretrained=False, progress=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>ResNet-50 model from
<code>"Deep Residual Learning for Image Recognition" &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;</code>_</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pretrained</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, returns a model pre-trained on ImageNet</dd>
<dt><strong><code>progress</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, displays a progress bar of the download to stderr</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resnet50(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNet-50 model from
    `&#34;Deep Residual Learning for Image Recognition&#34; &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    return _resnet(&#34;resnet50&#34;, Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs)</code></pre>
</details>
</dd>
<dt id="ednaml.backbones.resnet.resnext101_32x8d"><code class="name flex">
<span>def <span class="ident">resnext101_32x8d</span></span>(<span>pretrained=False, progress=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>ResNeXt-101 32x8d model from
<code>"Aggregated Residual Transformation for Deep Neural Networks" &lt;https://arxiv.org/pdf/1611.05431.pdf&gt;</code>_</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pretrained</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, returns a model pre-trained on ImageNet</dd>
<dt><strong><code>progress</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, displays a progress bar of the download to stderr</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resnext101_32x8d(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNeXt-101 32x8d model from
    `&#34;Aggregated Residual Transformation for Deep Neural Networks&#34; &lt;https://arxiv.org/pdf/1611.05431.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    kwargs[&#34;groups&#34;] = 32
    kwargs[&#34;width_per_group&#34;] = 8
    return _resnet(
        &#34;resnext101_32x8d&#34;, Bottleneck, [3, 4, 23, 3], pretrained, progress, **kwargs
    )</code></pre>
</details>
</dd>
<dt id="ednaml.backbones.resnet.resnext50_32x4d"><code class="name flex">
<span>def <span class="ident">resnext50_32x4d</span></span>(<span>pretrained=False, progress=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>ResNeXt-50 32x4d model from
<code>"Aggregated Residual Transformation for Deep Neural Networks" &lt;https://arxiv.org/pdf/1611.05431.pdf&gt;</code>_</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pretrained</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, returns a model pre-trained on ImageNet</dd>
<dt><strong><code>progress</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, displays a progress bar of the download to stderr</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resnext50_32x4d(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNeXt-50 32x4d model from
    `&#34;Aggregated Residual Transformation for Deep Neural Networks&#34; &lt;https://arxiv.org/pdf/1611.05431.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    kwargs[&#34;groups&#34;] = 32
    kwargs[&#34;width_per_group&#34;] = 4
    return _resnet(
        &#34;resnext50_32x4d&#34;, Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs
    )</code></pre>
</details>
</dd>
<dt id="ednaml.backbones.resnet.wide_resnet101_2"><code class="name flex">
<span>def <span class="ident">wide_resnet101_2</span></span>(<span>pretrained=False, progress=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Wide ResNet-101-2 model from
<code>"Wide Residual Networks" &lt;https://arxiv.org/pdf/1605.07146.pdf&gt;</code>_
The model is the same as ResNet except for the bottleneck number of channels
which is twice larger in every block. The number of channels in outer 1x1
convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048
channels, and in Wide ResNet-50-2 has 2048-1024-2048.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pretrained</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, returns a model pre-trained on ImageNet</dd>
<dt><strong><code>progress</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, displays a progress bar of the download to stderr</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wide_resnet101_2(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;Wide ResNet-101-2 model from
    `&#34;Wide Residual Networks&#34; &lt;https://arxiv.org/pdf/1605.07146.pdf&gt;`_
    The model is the same as ResNet except for the bottleneck number of channels
    which is twice larger in every block. The number of channels in outer 1x1
    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048
    channels, and in Wide ResNet-50-2 has 2048-1024-2048.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    kwargs[&#34;width_per_group&#34;] = 64 * 2
    return _resnet(
        &#34;wide_resnet101_2&#34;, Bottleneck, [3, 4, 23, 3], pretrained, progress, **kwargs
    )</code></pre>
</details>
</dd>
<dt id="ednaml.backbones.resnet.wide_resnet50_2"><code class="name flex">
<span>def <span class="ident">wide_resnet50_2</span></span>(<span>pretrained=False, progress=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Wide ResNet-50-2 model from
<code>"Wide Residual Networks" &lt;https://arxiv.org/pdf/1605.07146.pdf&gt;</code>_
The model is the same as ResNet except for the bottleneck number of channels
which is twice larger in every block. The number of channels in outer 1x1
convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048
channels, and in Wide ResNet-50-2 has 2048-1024-2048.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pretrained</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, returns a model pre-trained on ImageNet</dd>
<dt><strong><code>progress</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, displays a progress bar of the download to stderr</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wide_resnet50_2(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;Wide ResNet-50-2 model from
    `&#34;Wide Residual Networks&#34; &lt;https://arxiv.org/pdf/1605.07146.pdf&gt;`_
    The model is the same as ResNet except for the bottleneck number of channels
    which is twice larger in every block. The number of channels in outer 1x1
    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048
    channels, and in Wide ResNet-50-2 has 2048-1024-2048.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    kwargs[&#34;width_per_group&#34;] = 64 * 2
    return _resnet(
        &#34;wide_resnet50_2&#34;, Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs
    )</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="ednaml.backbones.resnet.resnet"><code class="flex name class">
<span>class <span class="ident">resnet</span></span>
<span>(</span><span>block:Â torch.nn.modules.module.ModuleÂ =Â ednaml.utils.blocks.ResnetBottleneck.ResnetBottleneck, layers:Â List[int]Â =Â [3, 4, 6, 3], last_stride:Â intÂ =Â 2, zero_init_residual:Â boolÂ =Â False, top_only:Â boolÂ =Â True, num_classes:Â boolÂ =Â 1000, groups:Â intÂ =Â 1, width_per_group:Â intÂ =Â 64, replace_stride_with_dilation:Â List[int]Â =Â None, norm_layer:Â torch.nn.modules.module.ModuleÂ =Â None, attention:Â strÂ =Â None, input_attention:Â boolÂ =Â None, secondary_attention:Â intÂ =Â None, ia_attention:Â boolÂ =Â None, part_attention:Â boolÂ =Â None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes the resnet model.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>block</code></strong> :&ensp;<code>nn.Module</code>, optional</dt>
<dd>The block (BasicBlock or Bottleneck) used for the internal Resnet architecture. Defaults to ResnetBottleneck.</dd>
<dt><strong><code>layers</code></strong> :&ensp;<code>List[int]</code>, optional</dt>
<dd>Number of layers in each block. Defaults to [3, 4, 6, 3].</dd>
<dt><strong><code>last_stride</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The stride for the last block. Defaults to 2.</dd>
<dt><strong><code>zero_init_residual</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to initialize network with only zeros. Unused. Defaults to False.</dd>
<dt><strong><code>top_only</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to keep only the feature extractor block. Defaults to True.</dd>
<dt><strong><code>num_classes</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Number of classes for the imagenet layer. Unused. Defaults to 1000.</dd>
<dt><strong><code>groups</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>see nn.conv2D. Defaults to 1.</dd>
<dt><strong><code>width_per_group</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>see nn.conv2D. Defaults to 64.</dd>
<dt><strong><code>replace_stride_with_dilation</code></strong> :&ensp;<code>List[int]</code>, optional</dt>
<dd>Whether to replace stride with dilation for each block. Defaults to None.</dd>
<dt><strong><code>norm_layer</code></strong> :&ensp;<code>nn.Module</code>, optional</dt>
<dd>The default normalization layer. If None, uses batchnorm. Defaults to None.</dd>
<dt><strong><code>attention</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>What attention to use, among <code>cbam</code>, <code>dbam</code>. Defaults to None.</dd>
<dt><strong><code>input_attention</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to use input attention at the first resnet block. Defaults to None.</dd>
<dt><strong><code>secondary_attention</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Whether to use secondary attention to apply <code>attention</code> to the specific resnet block only. Defaults to None.</dd>
<dt><strong><code>ia_attention</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to use input attention at the first conv layer. Exclusive with <code>input_attention</code>. Defaults to None.</dd>
<dt><strong><code>part_attention</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to use the local attention module. Defaults to None.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If attention blocks are not self-consistent. Specifically, the following rules:
- cannot have both <code>ia_attention</code> and <code>input_attention</code>.
- cannot have <code>part_attention</code> with <code>attention</code>, unless <code>secondary_attention</code>!=1</dd>
<dt><code>ValueError</code></dt>
<dd>If <code>replace_stride_with_dilation</code> is not a 3-tuple or None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class resnet(nn.Module):
    def __init__(
        self,
        block: nn.Module = Bottleneck,
        layers: List[int] = [3, 4, 6, 3],
        last_stride: int = 2,
        zero_init_residual: bool = False,
        top_only: bool = True,
        num_classes: bool = 1000,
        groups: int = 1,
        width_per_group: int = 64,
        replace_stride_with_dilation: List[int] = None,
        norm_layer: nn.Module = None,
        attention: str = None,
        input_attention: bool = None,
        secondary_attention: int = None,
        ia_attention: bool = None,
        part_attention: bool = None,
        **kwargs
    ):
        &#34;&#34;&#34;Initializes the resnet model.


        Args:
            block (nn.Module, optional): The block (BasicBlock or Bottleneck) used for the internal Resnet architecture. Defaults to ResnetBottleneck.
            layers (List[int], optional): Number of layers in each block. Defaults to [3, 4, 6, 3].
            last_stride (int, optional): The stride for the last block. Defaults to 2.
            zero_init_residual (bool, optional): Whether to initialize network with only zeros. Unused. Defaults to False.
            top_only (bool, optional): Whether to keep only the feature extractor block. Defaults to True.
            num_classes (bool, optional): Number of classes for the imagenet layer. Unused. Defaults to 1000.
            groups (int, optional): see nn.conv2D. Defaults to 1.
            width_per_group (int, optional): see nn.conv2D. Defaults to 64.
            replace_stride_with_dilation (List[int], optional): Whether to replace stride with dilation for each block. Defaults to None.
            norm_layer (nn.Module, optional): The default normalization layer. If None, uses batchnorm. Defaults to None.
            attention (str, optional): What attention to use, among `cbam`, `dbam`. Defaults to None.
            input_attention (bool, optional): Whether to use input attention at the first resnet block. Defaults to None.
            secondary_attention (int, optional): Whether to use secondary attention to apply `attention` to the specific resnet block only. Defaults to None.
            ia_attention (bool, optional): Whether to use input attention at the first conv layer. Exclusive with `input_attention`. Defaults to None.
            part_attention (bool, optional): Whether to use the local attention module. Defaults to None.

        Raises:
            ValueError: If attention blocks are not self-consistent. Specifically, the following rules:
                - cannot have both `ia_attention` and `input_attention`.
                - cannot have `part_attention` with `attention`, unless `secondary_attention`!=1
            ValueError: If `replace_stride_with_dilation` is not a 3-tuple or None.
        &#34;&#34;&#34;
        super().__init__()

        self.attention = attention
        self.input_attention = input_attention
        self.secondary_attention = secondary_attention
        self.block = block
        self.inplanes = 64
        if norm_layer is None:
            self._norm_layer = nn.BatchNorm2d
        # elif norm_layer == &#34;ln&#34;:
        #    self._norm_layer = nn.LayerNorm
        self.dilation = 1
        if replace_stride_with_dilation is None:
            replace_stride_with_dilation = [False, False, False]
        if len(replace_stride_with_dilation) != 3:
            raise ValueError(
                &#34;replace_stride_with_dilation should be `None` or a 3-element tuple. Got {}&#34;.format(
                    replace_stride_with_dilation
                )
            )
        self.groups = groups
        self.base_width = width_per_group

        self.conv1 = nn.Conv2d(
            3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False
        )
        # if norm_layer == &#34;gn&#34;:
        #    self.bn1 = nn.GroupNorm2d
        self.bn1 = nn.BatchNorm2d(self.inplanes)
        self.relu1 = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

        self.ia_attention = ia_attention
        self.part_attention = part_attention

        # Make sure ia and input_attention do not conflict
        if self.ia_attention is not None and self.input_attention is not None:
            raise ValueError(&#34;Cannot have both ia_attention and input_attention.&#34;)
        if self.part_attention is not None and (
            self.attention is not None and self.secondary_attention is None
        ):
            raise ValueError(&#34;Cannot have part-attention with CBAM everywhere&#34;)
        if self.part_attention is not None and (
            self.attention is not None and self.secondary_attention == 1
        ):
            raise ValueError(&#34;Cannot have part-attention with CBAM-Early&#34;)

        # Create true IA
        if self.ia_attention:
            self.ia_attention = InputAttention(self.inplanes)  # 64, set above
        else:
            self.ia_attention = None

        att = self.attention
        if (
            secondary_attention is not None and secondary_attention != 1
        ):  # leave alone if sec attention not set
            att = None
        self.layer1 = self._make_layer(
            self.block,
            64,
            layers[0],
            attention=att,
            input_attention=self.input_attention,
            part_attention=self.part_attention,
        )
        att = self.attention
        if (
            secondary_attention is not None and secondary_attention != 2
        ):  # leave alone if sec attention not set
            att = None
        self.layer2 = self._make_layer(
            self.block,
            128,
            layers[1],
            stride=2,
            attention=att,
            dilate=replace_stride_with_dilation[0],
        )
        att = self.attention
        if (
            secondary_attention is not None and secondary_attention != 3
        ):  # leave alone if sec attention not set
            att = None
        self.layer3 = self._make_layer(
            self.block,
            256,
            layers[2],
            stride=2,
            attention=att,
            dilate=replace_stride_with_dilation[1],
        )
        att = self.attention
        if (
            secondary_attention is not None and secondary_attention != 4
        ):  # leave alone if sec attention not set
            att = None
        self.layer4 = self._make_layer(
            self.block,
            512,
            layers[3],
            stride=last_stride,
            attention=att,
            dilate=replace_stride_with_dilation[2],
        )

        self.top_only = top_only
        self.avgpool, self.fc = None, None

        if not self.top_only:
            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
            self.fc = nn.Linear(512 * block.expansion, num_classes)

    def _make_layer(
        self,
        block: nn.Module,
        planes: int,
        blocks: int,
        stride: int = 1,
        dilate: bool = False,
        attention: str = None,
        input_attention: bool = False,
        ia_attention: bool = False,
        part_attention: bool = False,
    ) -&gt; nn.Sequential:
        &#34;&#34;&#34;Creates a resnet block

        Args:
            block (nn.Module): The block (BasicBlock or Bottleneck) used for the internal Resnet architecture. Defaults to ResnetBottleneck.
            planes (int): Number of input depth
            blocks (int): Number of blocks in this ResnetBlock
            stride (int, optional): Stride for the conv layers. Defaults to 1.
            dilate (bool, optional): Dilation for the conv layers. Defaults to False.
            attention (str, optional): Which of `cbam`, `dbam` attention to use. Defaults to None.
            input_attention (bool, optional): Whether to use `input_attention`. Defaults to False.
            ia_attention (bool, optional): Whether to use `ia_attention`. Unused. Defaults to False.
            part_attention (bool, optional): Whether to use local attention. Defaults to False.

        Returns:
            nn.Sequential: The layers comprising this Resnet Block as an nn.Sequential
        &#34;&#34;&#34;
        downsample = None
        previous_dilation = self.dilation
        if dilate:
            self.dilation *= stride
            stride = 1
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(
                    self.inplanes,
                    planes * block.expansion,
                    kernel_size=1,
                    stride=stride,
                    bias=False,
                ),
                self._norm_layer(planes * block.expansion),
            )

        layers = []
        layers.append(
            block(
                self.inplanes,
                planes,
                stride,
                downsample,
                groups=self.groups,
                base_width=self.base_width,
                dilation=previous_dilation,
                norm_layer=self._norm_layer,
                attention=attention,
                input_attention=input_attention,
                part_attention=part_attention,
            )
        )
        self.inplanes = planes * block.expansion
        for _ in range(1, blocks):
            layers.append(
                block(
                    self.inplanes,
                    planes,
                    groups=self.groups,
                    base_width=self.base_width,
                    dilation=self.dilation,
                    norm_layer=self._norm_layer,
                    attention=attention,
                )
            )
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv1(x)

        if self.ia_attention is not None:
            x = self.ia_attention(x) * x
        x = self.bn1(x)
        x = self.relu1(x)
        x = self.maxpool(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        if not self.top_only:
            x = self.avgpool(x)
            x = torch.flatten(x, 1)
            x = self.fc(x)
        return x

    def load_param(self, weights_path: str):
        &#34;&#34;&#34;Loads parameters frm saved weights file

        Args:
            weights_path (str): Path to the weights file
        &#34;&#34;&#34;
        param_dict = torch.load(weights_path)
        for i in param_dict:
            if &#34;fc&#34; in i and self.top_only:
                continue
            self.state_dict()[i].copy_(param_dict[i])</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="ednaml.backbones.resnet.resnet.dump_patches"><code class="name">var <span class="ident">dump_patches</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="ednaml.backbones.resnet.resnet.training"><code class="name">var <span class="ident">training</span> :Â bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="ednaml.backbones.resnet.resnet.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) â€‘>Â Callable[...,Â Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    x = self.conv1(x)

    if self.ia_attention is not None:
        x = self.ia_attention(x) * x
    x = self.bn1(x)
    x = self.relu1(x)
    x = self.maxpool(x)

    x = self.layer1(x)
    x = self.layer2(x)
    x = self.layer3(x)
    x = self.layer4(x)

    if not self.top_only:
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)
    return x</code></pre>
</details>
</dd>
<dt id="ednaml.backbones.resnet.resnet.load_param"><code class="name flex">
<span>def <span class="ident">load_param</span></span>(<span>self, weights_path:Â str)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads parameters frm saved weights file</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>weights_path</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to the weights file</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_param(self, weights_path: str):
    &#34;&#34;&#34;Loads parameters frm saved weights file

    Args:
        weights_path (str): Path to the weights file
    &#34;&#34;&#34;
    param_dict = torch.load(weights_path)
    for i in param_dict:
        if &#34;fc&#34; in i and self.top_only:
            continue
        self.state_dict()[i].copy_(param_dict[i])</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="ednaml.backbones" href="index.html">ednaml.backbones</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="ednaml.backbones.resnet.resnet101" href="#ednaml.backbones.resnet.resnet101">resnet101</a></code></li>
<li><code><a title="ednaml.backbones.resnet.resnet152" href="#ednaml.backbones.resnet.resnet152">resnet152</a></code></li>
<li><code><a title="ednaml.backbones.resnet.resnet18" href="#ednaml.backbones.resnet.resnet18">resnet18</a></code></li>
<li><code><a title="ednaml.backbones.resnet.resnet34" href="#ednaml.backbones.resnet.resnet34">resnet34</a></code></li>
<li><code><a title="ednaml.backbones.resnet.resnet50" href="#ednaml.backbones.resnet.resnet50">resnet50</a></code></li>
<li><code><a title="ednaml.backbones.resnet.resnext101_32x8d" href="#ednaml.backbones.resnet.resnext101_32x8d">resnext101_32x8d</a></code></li>
<li><code><a title="ednaml.backbones.resnet.resnext50_32x4d" href="#ednaml.backbones.resnet.resnext50_32x4d">resnext50_32x4d</a></code></li>
<li><code><a title="ednaml.backbones.resnet.wide_resnet101_2" href="#ednaml.backbones.resnet.wide_resnet101_2">wide_resnet101_2</a></code></li>
<li><code><a title="ednaml.backbones.resnet.wide_resnet50_2" href="#ednaml.backbones.resnet.wide_resnet50_2">wide_resnet50_2</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="ednaml.backbones.resnet.resnet" href="#ednaml.backbones.resnet.resnet">resnet</a></code></h4>
<ul class="">
<li><code><a title="ednaml.backbones.resnet.resnet.dump_patches" href="#ednaml.backbones.resnet.resnet.dump_patches">dump_patches</a></code></li>
<li><code><a title="ednaml.backbones.resnet.resnet.forward" href="#ednaml.backbones.resnet.resnet.forward">forward</a></code></li>
<li><code><a title="ednaml.backbones.resnet.resnet.load_param" href="#ednaml.backbones.resnet.resnet.load_param">load_param</a></code></li>
<li><code><a title="ednaml.backbones.resnet.resnet.training" href="#ednaml.backbones.resnet.resnet.training">training</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>