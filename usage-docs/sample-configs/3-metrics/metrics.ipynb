{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3LtQmhmQ1T_8"
      },
      "source": [
        "# Metrics in EdnaML and EdnaDeploy\n",
        "\n",
        "Here, we examine EdnaML's metrics infrastructure. Most metrics are essentially wrappers around their Torchmetrics "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BZOO_ELa0PRM"
      },
      "source": [
        "# Setup Steps (If EdnaML is not already installed)\n",
        " \n",
        "We can install either from source or from PyPi. The appropriate option can be selected from the first cell below.\n",
        "\n",
        "**Very Important**. Due to the way Colab installs certain packages, you will need to restart the runtime after installing EdnaML. Then you can proceed with future steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9n75eBgQXl7p",
        "outputId": "285a4edf-df02-4d0c-83b8-4af1d2b3c88a"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7AZehvZ0i2X"
      },
      "outputs": [],
      "source": [
        "install_from = \"source\" # source | pypi\n",
        "branch = \"suprem-devel-mongo-metrics\"           # DO NOT CHANGE THIS unless you know what you are doing\n",
        "version = \"0.1.5\"           # DO NOT CHANGE THIS unless you know what you are doing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5j3WfN0fpIT"
      },
      "source": [
        "###  Installation steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBWvd4b10spk",
        "outputId": "80382658-b13b-4383-b9ef-b0ec10827ad1"
      },
      "outputs": [],
      "source": [
        "if install_from == \"source\":\n",
        "  ! rm -rf -- EdnaML ||:\n",
        "  ! git clone -b $branch https://github.com/asuprem/EdnaML\n",
        "  ! pip install -e EdnaML/\n",
        "else:\n",
        "  ! python -V\n",
        "  ! pip3 install --pre ednaml==$version"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Restart Runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "  import ednaml\n",
        "except (ImportError, KeyError, ModuleNotFoundError):\n",
        "  print('Stopping RUNTIME. Colaboratory will restart automatically.')\n",
        "  exit()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JY04vJjxCGaD"
      },
      "source": [
        "# Metrics Basics\n",
        "\n",
        "We will run a few experiments using the basic [MNIST configuration](./mnist.yml). See [cnn.ipynb](../0-basics/cnn/cnn.ipynb) for more details on MNIST and EdnaML.\n",
        "\n",
        "We have added a few things here for Metrics on top of the Storage and experimentation components. Specifically, we have added the `METRICS` section with a single metric:\n",
        "\n",
        "```\n",
        "METRICS:                      # Defining metrics to be tracked. This needs to be made top-level\n",
        "  MODEL_METRICS:\n",
        "    - METRIC_NAME: avgacc\n",
        "      METRIC_CLASS: BaseTorchMetric\n",
        "      METRIC_ARGS:\n",
        "        metric_name: Accuracy\n",
        "        aggregate: 100\n",
        "        metric_kwargs:\n",
        "          task: 'multiclass'\n",
        "          num_classes: 10\n",
        "      METRIC_PARAMS: \n",
        "        preds: logits\n",
        "        targets: labels  # basically, key is what the metric expects, value is what WILL be there, e.g. HFTrainer could have labels, not targets, so it would be targets: labels\n",
        "      METRIC_TRIGGER: step\n",
        "      METRIC_STORAGE: null\n",
        "```\n",
        "\n",
        "Here, we have set up a Metric for the model (more details about specific metrics can be found at []). You can add metrics for each component to track different KPIs, e.g. metrics for the Logger, Cnfiguration manager, Deployment, and Code. For example you may want to record code quality of provided custom code through some backend service as a metric. You may also want to record number of error logs, warning logs, or debug logs per X training steps. \n",
        "\n",
        "In this case, we have added an Accuracy Metric. Since we want to use the excellent Torchmetrics package, we will use our internal wrapper `BaseTorchMetric`, and provide the correct arguments (`metric_name` is the specific torchmetric we wish to use, and `metric_kwargs` are the arguments for the above class). \n",
        "\n",
        "Then, we have `METRIC_PARAMS`. Each module publishes a set of parameters internally that metrics have access to. For example, `BaseTrainer` publishes `loss` at each step. `ClassificationTrainer`, which we use for MNIST, publishes loss, as well as `logits`, `labels`, `features` and the current `epoch` and `step`, among others. For accuracy, we require `logits` and `labels`. However, `torchmetrics.Accuracy` takes in `preds` and `targets` as input. So we provide this mapping in `METRIC_PARAMS` so that our metrics can access the correct arguments to compute metrics.\n",
        "\n",
        "Finally, we have `METRIC_TRIGGER`, which can be one of `[once | always | step | batch]`, meaning it is triggered just once at beginning, always whenever some parameter changes inside a module, at the end of each step, or at the end of a batch of steps (batch determined by `LOGGING.STEP_VERBOSE`). We want to compute accuracy at each step. However, we also want to aggregate the accuracy for saving across 100 steps, so we set the `aggregate` parameter in `METRIC_ARGS` to 100.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QzffsEaj1P2e"
      },
      "source": [
        "## 0. Setting up the MNIST model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93KF2dwKk9HO"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wP4BmCySkb85"
      },
      "outputs": [],
      "source": [
        "# Here we define our custom model class\n",
        "from ednaml.models import ModelAbstract\n",
        "from torch import nn\n",
        "import ednaml.core.decorators as edna\n",
        "\n",
        "class MNISTModel(ModelAbstract):\n",
        "  def model_attributes_setup(self, **kwargs):\n",
        "    pass\n",
        "  def model_setup(self, **kwargs):\n",
        "    self.conv1 = nn.Sequential(         \n",
        "        nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2), \n",
        "        nn.ReLU(), \n",
        "        nn.MaxPool2d(kernel_size=2),    \n",
        "    )\n",
        "    self.conv2 = nn.Sequential(         \n",
        "        nn.Conv2d(16, 32, 5, 1, 2), nn.ReLU(), nn.MaxPool2d(2),                \n",
        "    )\n",
        "    # fully connected layer, output 10 classes\n",
        "    self.out = nn.Linear(32 * 7 * 7, 10)\n",
        "    \n",
        "  def forward_impl(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.conv2(x)\n",
        "    # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
        "    x = x.view(x.size(0), -1)       \n",
        "    output = self.out(x)\n",
        "    # A ModelAbstract returns prediction, features, and secondary output (empty list)\n",
        "    return output, x, []    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hgqvDUeTUK6R",
        "outputId": "e2e60aaf-b5d3-4eb0-e5f0-79194f7a4f29"
      },
      "outputs": [],
      "source": [
        "import torch, ednaml\n",
        "from ednaml.core import EdnaML\n",
        "torch.__version__"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Basic MNIST Experiment\n",
        "\n",
        "We first run our basic MNIST experiment, with the default options."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOdO13uRkiyu",
        "outputId": "f817dcbd-d72c-4745-d9db-65aefd7cca89"
      },
      "outputs": [],
      "source": [
        "EdnaML.clear_registrations()\n",
        "cfg = \"./EdnaML/usage-docs/sample-configs/1-storage/mnist.yml\"\n",
        "eml = EdnaML(config=cfg, config_inject = [\n",
        "    (\"MODEL.MODEL_BASE\", \"simple\"),  \n",
        "    (\"SAVE.MODEL_BACKBONE\", \"simple\"),  \n",
        "    (\"TRANSFORMATION.BATCH_SIZE\", 64),   # We will also increase the batch size\n",
        "    (\"LOGGING.INPUT_SIZE\", [64,1,28,28]),   # We will also fix the input size\n",
        "])\n",
        "eml.cfg.MODEL.MODEL_KWARGS = {}       # We delete the old MODEL_KWARGS, because our new model needs no arguments\n",
        "eml.addModelClass(MNISTModel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOoRZIDdmac5",
        "outputId": "87f6f70e-1ce9-4f6f-fc3f-0fcb203bdb01"
      },
      "outputs": [],
      "source": [
        "# These are the default options.\n",
        "eml.apply(  storage_manager_mode = \"strict\",\n",
        "            storage_mode = \"local\",\n",
        "            backup_mode = \"hybrid\",\n",
        "            tracking_run = None,\n",
        "            new_run = False,\n",
        "            skip_storage = False\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djlAOyYdmo9w",
        "outputId": "d58a57ae-3e49-43d0-ba1c-7e7d3c92388e"
      },
      "outputs": [],
      "source": [
        "eml.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiCRi_eBgghQ"
      },
      "outputs": [],
      "source": [
        "eml.eval()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What to note\n",
        "\n",
        "On the file view (if you are on Colab), you should see a directory called `mnist_resnet-v1-simple-mnist` that contains a directory `0`. Inside this, there should be several pytorch files and log files.\n",
        "\n",
        "Here, the name of the experiment is `mnist_resnet-v1-simple-mnist`, inherited from the `MODEL_CORE_NAME`, `MODEL_VERSION`, `MODEL_BACKBONE`, and `MODEL_QUALIFIER` in the `SAVE` section of the [configuration, linked here](mnist.yml#L20).\n",
        "\n",
        "`0` is the run for this experiment. There should be a `metrics.json` with the computed metrics."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. `storage_mode`\n",
        "\n",
        "We will let `storage_mode=\"empty\"` instead of `local`. Here, no local files will be created. Note: we have incremented the version to `2` in `config_inject`, so if `storage_mode` was local, the expected directory would be `mnist_resnet-v2-simple-mnist`. However, it will not be created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOdO13uRkiyu",
        "outputId": "f817dcbd-d72c-4745-d9db-65aefd7cca89"
      },
      "outputs": [],
      "source": [
        "EdnaML.clear_registrations()\n",
        "cfg = \"./EdnaML/usage-docs/sample-configs/1-storage/mnist.yml\"\n",
        "eml = EdnaML(config=cfg, config_inject = [\n",
        "    (\"MODEL.MODEL_VERSION\", 2),  \n",
        "    (\"MODEL.MODEL_BASE\", \"simple\"),  \n",
        "    (\"SAVE.MODEL_BACKBONE\", \"simple\"),  \n",
        "    (\"TRANSFORMATION.BATCH_SIZE\", 64),   # We will also increase the batch size\n",
        "    (\"LOGGING.INPUT_SIZE\", [64,1,28,28]),   # We will also fix the input size\n",
        "])\n",
        "eml.cfg.MODEL.MODEL_KWARGS = {}       # We delete the old MODEL_KWARGS, because our new model needs no arguments\n",
        "eml.addModelClass(MNISTModel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOoRZIDdmac5",
        "outputId": "87f6f70e-1ce9-4f6f-fc3f-0fcb203bdb01"
      },
      "outputs": [],
      "source": [
        "# These are the default options.\n",
        "eml.apply(  storage_manager_mode = \"strict\",\n",
        "            storage_mode = \"empty\",\n",
        "            backup_mode = \"hybrid\",\n",
        "            tracking_run = None,\n",
        "            new_run = False,\n",
        "            skip_storage = False\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djlAOyYdmo9w",
        "outputId": "d58a57ae-3e49-43d0-ba1c-7e7d3c92388e"
      },
      "outputs": [],
      "source": [
        "eml.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiCRi_eBgghQ"
      },
      "outputs": [],
      "source": [
        "eml.eval()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. `new_run`\n",
        "\n",
        "Each EdnaML experiment can have multiple runs. Runs are denoted by integers starting from 0. When `new_run` is set to `True`, the runs are incremented. Here, we will switch back to version 1 of the experiment. Inside the directory for experiment 1 (`mnist_resnet-v1-simple-mnist`), we will get 2 new directories: `0` and `1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOdO13uRkiyu",
        "outputId": "f817dcbd-d72c-4745-d9db-65aefd7cca89"
      },
      "outputs": [],
      "source": [
        "EdnaML.clear_registrations()\n",
        "cfg = \"./EdnaML/usage-docs/sample-configs/1-storage/mnist.yml\"\n",
        "eml = EdnaML(config=cfg, config_inject = [\n",
        "    (\"MODEL.MODEL_VERSION\", 1),  \n",
        "    (\"MODEL.MODEL_BASE\", \"simple\"),  \n",
        "    (\"SAVE.MODEL_BACKBONE\", \"simple\"),  \n",
        "    (\"TRANSFORMATION.BATCH_SIZE\", 64),   # We will also increase the batch size\n",
        "    (\"LOGGING.INPUT_SIZE\", [64,1,28,28]),   # We will also fix the input size\n",
        "])\n",
        "eml.cfg.MODEL.MODEL_KWARGS = {}       # We delete the old MODEL_KWARGS, because our new model needs no arguments\n",
        "eml.addModelClass(MNISTModel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOoRZIDdmac5",
        "outputId": "87f6f70e-1ce9-4f6f-fc3f-0fcb203bdb01"
      },
      "outputs": [],
      "source": [
        "# These are the default options.\n",
        "eml.apply(  storage_manager_mode = \"strict\",\n",
        "            storage_mode = \"local\",\n",
        "            backup_mode = \"hybrid\",\n",
        "            tracking_run = None,\n",
        "            new_run = True,\n",
        "            skip_storage = False\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djlAOyYdmo9w",
        "outputId": "d58a57ae-3e49-43d0-ba1c-7e7d3c92388e"
      },
      "outputs": [],
      "source": [
        "eml.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiCRi_eBgghQ"
      },
      "outputs": [],
      "source": [
        "eml.eval()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. `tracking_run`\n",
        "\n",
        "Each experiment can have multiple runs. `tracking_run` selects a specific run to continue from, if there are multiple, or to force a specific run. \n",
        "\n",
        "If `tracking_run=None` and `new_run=False`, EdnaML will automatically switch to the most recent run. Here, we will go to version 1 of the experiment, and track run 0 instead of automatically following run 1.\n",
        "\n",
        "We will also increment number of execution epochs to 2, so we can continue training from where we left off. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOdO13uRkiyu",
        "outputId": "f817dcbd-d72c-4745-d9db-65aefd7cca89"
      },
      "outputs": [],
      "source": [
        "EdnaML.clear_registrations()\n",
        "cfg = \"./EdnaML/usage-docs/sample-configs/1-storage/mnist.yml\"\n",
        "eml = EdnaML(config=cfg, config_inject = [\n",
        "    (\"MODEL.MODEL_VERSION\", 1),  \n",
        "    (\"EXECUTION.EPOCHS\", 2),\n",
        "    (\"MODEL.MODEL_BASE\", \"simple\"),  \n",
        "    (\"SAVE.MODEL_BACKBONE\", \"simple\"),  \n",
        "    (\"TRANSFORMATION.BATCH_SIZE\", 64),   # We will also increase the batch size\n",
        "    (\"LOGGING.INPUT_SIZE\", [64,1,28,28]),   # We will also fix the input size\n",
        "])\n",
        "eml.cfg.MODEL.MODEL_KWARGS = {}       # We delete the old MODEL_KWARGS, because our new model needs no arguments\n",
        "eml.addModelClass(MNISTModel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOoRZIDdmac5",
        "outputId": "87f6f70e-1ce9-4f6f-fc3f-0fcb203bdb01"
      },
      "outputs": [],
      "source": [
        "# These are the default options.\n",
        "eml.apply(  storage_manager_mode = \"strict\",\n",
        "            storage_mode = \"local\",\n",
        "            backup_mode = \"hybrid\",\n",
        "            tracking_run = 0,\n",
        "            new_run = False,\n",
        "            skip_storage = False\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djlAOyYdmo9w",
        "outputId": "d58a57ae-3e49-43d0-ba1c-7e7d3c92388e"
      },
      "outputs": [],
      "source": [
        "eml.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiCRi_eBgghQ"
      },
      "outputs": [],
      "source": [
        "eml.eval()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Using Storage for backup\n",
        "\n",
        "So far, we have not performed backups, and instead used the local storage to record artifacts.\n",
        "\n",
        "Here we will set up a backup for our models. To keep things simple, we will use an `ednaml.storage.LocalStorage` instance, which simple backs up to a user-defined directory on the same machine. LocalStorage can be used to keep redundant copies, or to transfer models to a NAS or some mapped network drive. For example, if running on Google Colab, one can mount their Google Drive at `/content/drive` and use LocalStorage pointing to `/content/drive` to back up artifacts to Google Drive.\n",
        "\n",
        "In our case, we will backup all artifacts to a directory called `backup`. The Storage options are  provided in `mnist_simple_storage.yml` and will be chained with `mnist.yml`.\n",
        "\n",
        "We will use version 1, with tracking run 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOdO13uRkiyu",
        "outputId": "f817dcbd-d72c-4745-d9db-65aefd7cca89"
      },
      "outputs": [],
      "source": [
        "EdnaML.clear_registrations()\n",
        "cfg = \"./EdnaML/usage-docs/sample-configs/1-storage/mnist.yml\"\n",
        "storage_cfg = \"./EdnaML/usage-docs/sample-configs/1-storage/mnist_simple_storage.yml\"\n",
        "eml = EdnaML(config=[cfg, storage_cfg], config_inject = [\n",
        "    (\"MODEL.MODEL_VERSION\", 1),  \n",
        "    (\"MODEL.MODEL_BASE\", \"simple\"),  \n",
        "    (\"SAVE.MODEL_BACKBONE\", \"simple\"),  \n",
        "    (\"TRANSFORMATION.BATCH_SIZE\", 64),   # We will also increase the batch size\n",
        "    (\"LOGGING.INPUT_SIZE\", [64,1,28,28]),   # We will also fix the input size\n",
        "])\n",
        "eml.cfg.MODEL.MODEL_KWARGS = {}       # We delete the old MODEL_KWARGS, because our new model needs no arguments\n",
        "eml.addModelClass(MNISTModel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOoRZIDdmac5",
        "outputId": "87f6f70e-1ce9-4f6f-fc3f-0fcb203bdb01"
      },
      "outputs": [],
      "source": [
        "# These are the default options.\n",
        "eml.apply(  storage_manager_mode = \"strict\",\n",
        "            storage_mode = \"local\",\n",
        "            backup_mode = \"hybrid\",\n",
        "            tracking_run = 3,\n",
        "            new_run = False,\n",
        "            skip_storage = False\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djlAOyYdmo9w",
        "outputId": "d58a57ae-3e49-43d0-ba1c-7e7d3c92388e"
      },
      "outputs": [],
      "source": [
        "eml.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiCRi_eBgghQ"
      },
      "outputs": [],
      "source": [
        "eml.eval()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Disabling Storage for backup\n",
        "\n",
        "We can also disable any storages we have set up, if we wanted only local storage. We do this by passing `skip_storage=True`.\n",
        "\n",
        "Here, we will use a new run, 4. We will also construct the storages by chaining `mnist_simple_storage.yml`. Finally, we will skip constructing the storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOdO13uRkiyu",
        "outputId": "f817dcbd-d72c-4745-d9db-65aefd7cca89"
      },
      "outputs": [],
      "source": [
        "EdnaML.clear_registrations()\n",
        "cfg = \"./EdnaML/usage-docs/sample-configs/1-storage/mnist.yml\"\n",
        "storage_cfg = \"./EdnaML/usage-docs/sample-configs/1-storage/mnist_simple_storage.yml\"\n",
        "eml = EdnaML(config=[cfg, storage_cfg], config_inject = [\n",
        "    (\"MODEL.MODEL_VERSION\", 1),  \n",
        "    (\"MODEL.MODEL_BASE\", \"simple\"),  \n",
        "    (\"SAVE.MODEL_BACKBONE\", \"simple\"),  \n",
        "    (\"TRANSFORMATION.BATCH_SIZE\", 64),   # We will also increase the batch size\n",
        "    (\"LOGGING.INPUT_SIZE\", [64,1,28,28]),   # We will also fix the input size\n",
        "])\n",
        "eml.cfg.MODEL.MODEL_KWARGS = {}       # We delete the old MODEL_KWARGS, because our new model needs no arguments\n",
        "eml.addModelClass(MNISTModel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOoRZIDdmac5",
        "outputId": "87f6f70e-1ce9-4f6f-fc3f-0fcb203bdb01"
      },
      "outputs": [],
      "source": [
        "# These are the default options.\n",
        "eml.apply(  storage_manager_mode = \"strict\",\n",
        "            storage_mode = \"local\",\n",
        "            backup_mode = \"hybrid\",\n",
        "            tracking_run = 4,\n",
        "            new_run = False,\n",
        "            skip_storage = True\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djlAOyYdmo9w",
        "outputId": "d58a57ae-3e49-43d0-ba1c-7e7d3c92388e"
      },
      "outputs": [],
      "source": [
        "eml.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiCRi_eBgghQ"
      },
      "outputs": [],
      "source": [
        "eml.eval()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. `backup_mode`\n",
        "\n",
        "`backup_mode` controls how each artifact is backed up. An artifact can be backed up in 2 modes:\n",
        "\n",
        "1. `canonical` mode: There is one file for the artifact. Each time an artifact is saved, we override the single `canonical` key for the artifact.\n",
        "2. `ers` mode: EdnaML uses the notion of `<Experiment-Run-Storage>`, or ERS to track every artifact. Additional details can be found in []. In `ers`, a file is created in the remote storage every FREQUENCY and FREQUENCY_STEP.\n",
        "\n",
        "\n",
        "For local saves, EdnaML always uses `ers` mode. You will notice files with the pattern [artifact]_epoch[num]_step[num].[ext]. We can control the backup mode with `backup_mode`. If `backup_mode=ers`, every artifact is saved in `ers` mode. If `backup_mode=canonical`, every artifact is saved in `canonical` mode. \n",
        "\n",
        "If `backup_mode=hybrid`, EdnaML uses intelligent options: models and training artifacts are saved in `ers` mode, while plugins, metrics, logs, and configs are saved in `canonical` mode. You can see this in prior experiments where the backups have a single log file: `log.log`, while the local files contain multiple log files.\n",
        "\n",
        "Here, we will set `backup_mode` to `canonical` in run 5. Usually, one should not mix up `backup_modes` between runs of the same experiment, but for the purposes of this notebook, it suffices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOdO13uRkiyu",
        "outputId": "f817dcbd-d72c-4745-d9db-65aefd7cca89"
      },
      "outputs": [],
      "source": [
        "EdnaML.clear_registrations()\n",
        "cfg = \"./EdnaML/usage-docs/sample-configs/1-storage/mnist.yml\"\n",
        "storage_cfg = \"./EdnaML/usage-docs/sample-configs/1-storage/mnist_simple_storage.yml\"\n",
        "eml = EdnaML(config=[cfg, storage_cfg], config_inject = [\n",
        "    (\"MODEL.MODEL_VERSION\", 1),  \n",
        "    (\"MODEL.MODEL_BASE\", \"simple\"),  \n",
        "    (\"SAVE.MODEL_BACKBONE\", \"simple\"),  \n",
        "    (\"TRANSFORMATION.BATCH_SIZE\", 64),   # We will also increase the batch size\n",
        "    (\"LOGGING.INPUT_SIZE\", [64,1,28,28]),   # We will also fix the input size\n",
        "])\n",
        "eml.cfg.MODEL.MODEL_KWARGS = {}       # We delete the old MODEL_KWARGS, because our new model needs no arguments\n",
        "eml.addModelClass(MNISTModel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOoRZIDdmac5",
        "outputId": "87f6f70e-1ce9-4f6f-fc3f-0fcb203bdb01"
      },
      "outputs": [],
      "source": [
        "# These are the default options.\n",
        "eml.apply(  storage_manager_mode = \"strict\",\n",
        "            storage_mode = \"local\",\n",
        "            backup_mode = \"canonical\",\n",
        "            tracking_run = 5,\n",
        "            new_run = False,\n",
        "            skip_storage = False\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djlAOyYdmo9w",
        "outputId": "d58a57ae-3e49-43d0-ba1c-7e7d3c92388e"
      },
      "outputs": [],
      "source": [
        "eml.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiCRi_eBgghQ"
      },
      "outputs": [],
      "source": [
        "eml.eval()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Multiple Storages\n",
        "\n",
        "Here, we will use multiple storages, and fine-tune backups, with run 6.\n",
        "\n",
        "Specifically, we will save models to a `LocalStorage` pointing to `./backup`. We will save artifacts to a `LocalStorage` pointing to `./artifacts`. We will skip saving logs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOdO13uRkiyu",
        "outputId": "f817dcbd-d72c-4745-d9db-65aefd7cca89"
      },
      "outputs": [],
      "source": [
        "EdnaML.clear_registrations()\n",
        "cfg = \"./EdnaML/usage-docs/sample-configs/1-storage/mnist.yml\"\n",
        "storage_cfg = \"./EdnaML/usage-docs/sample-configs/1-storage/mnist_multiple_storage.yml\"\n",
        "eml = EdnaML(config=[cfg, storage_cfg], config_inject = [\n",
        "    (\"MODEL.MODEL_VERSION\", 1),  \n",
        "    (\"MODEL.MODEL_BASE\", \"simple\"),  \n",
        "    (\"SAVE.MODEL_BACKBONE\", \"simple\"),  \n",
        "    (\"TRANSFORMATION.BATCH_SIZE\", 64),   # We will also increase the batch size\n",
        "    (\"LOGGING.INPUT_SIZE\", [64,1,28,28]),   # We will also fix the input size\n",
        "])\n",
        "eml.cfg.MODEL.MODEL_KWARGS = {}       # We delete the old MODEL_KWARGS, because our new model needs no arguments\n",
        "eml.addModelClass(MNISTModel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOoRZIDdmac5",
        "outputId": "87f6f70e-1ce9-4f6f-fc3f-0fcb203bdb01"
      },
      "outputs": [],
      "source": [
        "# These are the default options.\n",
        "eml.apply(  storage_manager_mode = \"strict\",\n",
        "            storage_mode = \"local\",\n",
        "            backup_mode = \"hybrid\",\n",
        "            tracking_run = 6,\n",
        "            new_run = False,\n",
        "            skip_storage = False\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djlAOyYdmo9w",
        "outputId": "d58a57ae-3e49-43d0-ba1c-7e7d3c92388e"
      },
      "outputs": [],
      "source": [
        "eml.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiCRi_eBgghQ"
      },
      "outputs": [],
      "source": [
        "eml.eval()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JujngALfJhT_"
      },
      "source": [
        "## 9. Deployment\n",
        "\n",
        "Once an experiment is finished, we can deploy it to rerun it on the same training data, or on new data.\n",
        "\n",
        "Here, we use EdnaDeploy instead of EdnaML. EdnaDeploy does not build any training components, such as optimizer, scheduler, or loss functions. Furthermore, EdnaDeploy uses a Deployment instead of a Trainer class to manage the pipeline. As such, any Trainer should have a corresponding Deployment object (but not necessarily vice versa).\n",
        "\n",
        "Here, we will rerun Run 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jp_WAEG_ggG3"
      },
      "outputs": [],
      "source": [
        "from ednaml.core import EdnaDeploy\n",
        "from ednaml.deploy import ClassificationDeploy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4ZZgfnyggES",
        "outputId": "3e5048c5-af97-4313-ac9a-3cad611d0356"
      },
      "outputs": [],
      "source": [
        "EdnaDeploy.clear_registrations()\n",
        "cfg = \"./EdnaML/usage-docs/sample-configs/1-storage/mnist.yml\"\n",
        "deploy_cfg = \"./EdnaML/usage-docs/sample-configs/1-storage/mnist-deploy.yml\"\n",
        "ed = EdnaDeploy(config=[cfg, deploy_cfg], config_inject = [\n",
        "    (\"SAVE.MODEL_VERSION\", 1),            # We switch to version 3 for this experiment\n",
        "    (\"MODEL.MODEL_BASE\", \"simple\"),            # We switch to version 3 for this experiment\n",
        "    (\"TRANSFORMATION.BATCH_SIZE\", 64),   # We will also increase the batch size\n",
        "    (\"LOGGING.INPUT_SIZE\", [256,1,28,28]),   # We will also fix the input size\n",
        "]\n",
        ")\n",
        "ed.cfg.MODEL.MODEL_KWARGS = {}       # We delete the old MODEL_KWARGS, because our new model needs no arguments\n",
        "ed.addModelClass(MNISTModel)\n",
        "ed.addDeploymentClass(ClassificationDeploy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPFoSP41ggB_",
        "outputId": "cfbe167c-8a05-4680-a96f-8a5774d03f10"
      },
      "outputs": [],
      "source": [
        "ed.apply(tracking_run=3, new_run=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwcSwEEyJzkN",
        "outputId": "9719df9c-9c53-45aa-843e-36ce5626e938"
      },
      "outputs": [],
      "source": [
        "ed.deploy()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JujngALfJhT_"
      },
      "source": [
        "## 10. Deployment from specific epoch-step pair\n",
        "\n",
        "Now we will evaluate a specific epoch-step pair from Run 3. Since we want a specific epoch-step pair, and not the latest, we will have to tell EdnaDeploy, in `apply()` to skip loading the latest weights, with `skip_weights=True`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jp_WAEG_ggG3"
      },
      "outputs": [],
      "source": [
        "from ednaml.core import EdnaDeploy\n",
        "from ednaml.deploy import ClassificationDeploy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4ZZgfnyggES",
        "outputId": "3e5048c5-af97-4313-ac9a-3cad611d0356"
      },
      "outputs": [],
      "source": [
        "EdnaDeploy.clear_registrations()\n",
        "cfg = \"./EdnaML/usage-docs/sample-configs/1-storage/mnist.yml\"\n",
        "deploy_cfg = \"./EdnaML/usage-docs/sample-configs/1-storage/mnist-deploy.yml\"\n",
        "ed = EdnaDeploy(config=[cfg, deploy_cfg], config_inject = [\n",
        "    (\"SAVE.MODEL_VERSION\", 1),            # We switch to version 3 for this experiment\n",
        "    (\"MODEL.MODEL_BASE\", \"simple\"),            # We switch to version 3 for this experiment\n",
        "    (\"TRANSFORMATION.BATCH_SIZE\", 64),   # We will also increase the batch size\n",
        "    (\"LOGGING.INPUT_SIZE\", [256,1,28,28]),   # We will also fix the input size\n",
        "]\n",
        ")\n",
        "ed.cfg.MODEL.MODEL_KWARGS = {}       # We delete the old MODEL_KWARGS, because our new model needs no arguments\n",
        "ed.addModelClass(MNISTModel)\n",
        "ed.addDeploymentClass(ClassificationDeploy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPFoSP41ggB_",
        "outputId": "cfbe167c-8a05-4680-a96f-8a5774d03f10"
      },
      "outputs": [],
      "source": [
        "ed.apply(tracking_run=3, new_run=False, skip_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwcSwEEyJzkN",
        "outputId": "9719df9c-9c53-45aa-843e-36ce5626e938"
      },
      "outputs": [],
      "source": [
        "ed.deploy(continue_epoch = 0, continue_step = 600)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JujngALfJhT_"
      },
      "source": [
        "## 11. Deployment on new data\n",
        "\n",
        "Finally, we will run our MNIST model on CIFAR. \n",
        "\n",
        "We need to make a few changes:\n",
        "\n",
        "1. CIFAR is a 3-channel dataset. MNIST is 1 channel. We will adjust the CIFAR data during dataloading to be 1-channel by passing an argument for GENERATOR_KWARGS, by passing a grayscale image\n",
        "2. We will overwrite the data loading parts with a CIFAR-10 configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jp_WAEG_ggG3"
      },
      "outputs": [],
      "source": [
        "from ednaml.core import EdnaDeploy\n",
        "from ednaml.deploy import ClassificationDeploy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4ZZgfnyggES",
        "outputId": "3e5048c5-af97-4313-ac9a-3cad611d0356"
      },
      "outputs": [],
      "source": [
        "EdnaDeploy.clear_registrations()\n",
        "cfg = \"./EdnaML/usage-docs/sample-configs/1-storage/mnist.yml\"\n",
        "deploy_cfg = \"./EdnaML/usage-docs/sample-configs/1-storage/mnist-deploy.yml\"\n",
        "cifar10_data = \"./EdnaML/usage-docs/sample-configs/1-storage/cifar10-data.yml\"\n",
        "ed = EdnaDeploy(config=[cfg, deploy_cfg, cifar10_data], config_inject = [\n",
        "    (\"SAVE.MODEL_VERSION\", 1),            # We switch to version 3 for this experiment\n",
        "    (\"MODEL.MODEL_BASE\", \"simple\"),            # We switch to version 3 for this experiment\n",
        "    (\"TRANSFORMATION.BATCH_SIZE\", 64),   # We will also increase the batch size\n",
        "    (\"LOGGING.INPUT_SIZE\", [256,1,28,28]),   # We will also fix the input size\n",
        "]\n",
        ")\n",
        "ed.cfg.MODEL.MODEL_KWARGS = {}       # We delete the old MODEL_KWARGS, because our new model needs no arguments\n",
        "ed.addModelClass(MNISTModel)\n",
        "ed.addDeploymentClass(ClassificationDeploy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPFoSP41ggB_",
        "outputId": "cfbe167c-8a05-4680-a96f-8a5774d03f10"
      },
      "outputs": [],
      "source": [
        "ed.apply(tracking_run=3, new_run=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwcSwEEyJzkN",
        "outputId": "9719df9c-9c53-45aa-843e-36ce5626e938"
      },
      "outputs": [],
      "source": [
        "ed.deploy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-O-_w3oYNc-"
      },
      "source": [
        "# Separator"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "BZOO_ELa0PRM",
        "MBJsh5aTBzM6",
        "ET2xAQ2Lb9Cs"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "ednaml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.11 (main, Mar 30 2022, 02:45:55) [MSC v.1916 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "19686624b004a6cf552f6ad0efbc4b51ca83ef7ea52cc5f8653ac17021c6f95e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
