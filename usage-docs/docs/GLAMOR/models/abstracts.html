<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>GLAMOR.models.abstracts API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>GLAMOR.models.abstracts</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from torch import nn
import torch.nn.functional as F
import torch


class ReidModel(nn.Module):
    def __init__(self, base, weights=None, normalization=None, embedding_dimensions=None, soft_dimensions=None, **kwargs):
        &#34;&#34;&#34;Basic ReID Resnet model.

        A ReID model performs re-identification by generating embeddings such that the same class&#39;s embeddings are closer together.

        Args:
            base (str): The architecture base for resnet, i.e. resnet50, resnet18
            weights (str, None): Path to weights file for the architecture base ONLY. If not provided, base initialized with random values.
            normalization (str, None): Can be None, where no normalization is used. Else create a normalization layer. Supports: [&#34;bn&#34;, &#34;l2&#34;, &#34;in&#34;, &#34;gn&#34;, &#34;ln&#34;]
            embedding_dimensions (int): Dimensions for the feature embedding. Leave empty if feature dimensions should be same as architecture core output (e.g. resnet50 base model has 2048-dim feature outputs). If providing a value, it should be less than the architecture core&#39;s base feature dimensions.
            soft_dimensions (int, None): Whether to include softmax classification layer. If None, softmax layer is not created in model.
        
        Kwargs (MODEL_KWARGS):
            last_stride (int, 1): The final stride parameter for the architecture core. Should be one of 1 or 2.
            attention (str, None): The attention module to use. Only supports [&#39;cbam&#39;, None]
            input_attention (bool, false): Whether to include the IA module
            ia_attention (bool, false): Whether to include input IA module
            part_attention (bool, false): Whether to include Part-CBAM Mobule
            secondary_attention (int, None): Whether to modify CBAM to apply it to specific Resnet basic blocks. None means CBAM is applied to all. Otherwise, CBAM is applied only to the basic block number provided here.

        Default Kwargs (DO NOT CHANGE OR ADD TO MODEL_KWARGS; set in backbones.resnet):
            zero_init_residual (bool, false): Whether the final layer uses zero initialization
            top_only (bool, true): Whether to keep only the architecture base without imagenet fully-connected layers (1000 classes)
            num_classes (int, 1000): Number of features in final imagenet FC layer
            groups (int, 1): Used during resnet variants construction
            width_per_group (int, 64): Used during resnet variants construction
            replace_stride_with_dilation (bool, None): Well, replace stride with dilation...
            norm_layer (nn.Module, None): The normalization layer within resnet. Internally defaults to nn.BatchNorm2D

        &#34;&#34;&#34;
        super(ReidModel, self).__init__()
        self.base = None
        
        self.embedding_dimensions = embedding_dimensions
        self.soft_dimensions = soft_dimensions
        self.normalization = normalization if normalization != &#39;&#39; else None
        self.build_base(base, weights, **kwargs)    # All kwargs are passed into build_base,, which in turn passes kwargs into _resnet()
        
        self.feat_norm = None
        self.build_normalization(self.normalization)
        
        if self.soft_dimensions is not None:
            self.softmax = nn.Linear(self.embedding_dimensions, self.soft_dimensions, bias=False)
            self.softmax.apply(self.weights_init_softmax)
        else:
            self.softmax = None

    def weights_init_kaiming(self,m):
        classname = m.__class__.__name__
        if classname.find(&#39;Linear&#39;) != -1:
                nn.init.kaiming_normal_(m.weight, a=0, mode=&#39;fan_out&#39;)
                nn.init.constant_(m.bias, 0.0)
        elif classname.find(&#39;Conv&#39;) != -1:
                nn.init.kaiming_normal_(m.weight, a=0, mode=&#39;fan_in&#39;)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0.0)
        elif classname.find(&#39;BatchNorm&#39;) != -1:
                if m.affine:
                    nn.init.constant_(m.weight, 1.0)
                    nn.init.constant_(m.bias, 0.0)
        elif classname.find(&#39;GroupNorm&#39;) != -1:
                if m.affine:
                    nn.init.constant_(m.weight, 1.0)
                    nn.init.constant_(m.bias, 0.0)
        elif classname.find(&#39;LayerNorm&#39;) != -1:
                #if m.affine:
                #    nn.init.constant_(m.weight, 1.0)
                #    nn.init.constant_(m.bias, 0.0)
                pass
        elif classname.find(&#39;InstanceNorm&#39;) != -1:
                if m.affine:
                    nn.init.constant_(m.weight, 1.0)
                    nn.init.constant_(m.bias, 0.0)

    def weights_init_softmax(self, m):
        &#34;&#34;&#34; Initialize linear weights to standard normal. Mean 0. Standard Deviation 0.001 &#34;&#34;&#34;
        classname = m.__class__.__name__
        if classname.find(&#39;Linear&#39;) != -1:
                nn.init.normal_(m.weight, std=0.001)
                if m.bias:
                        nn.init.constant_(m.bias, 0.0)
    
    def partial_load(self,weights_path):
        params = torch.load(weights_path)
        for _key in params:
            if _key not in self.state_dict().keys() or params[_key].shape != self.state_dict()[_key].shape: 
                continue
            self.state_dict()[_key].copy_(params[_key])


    def build_base(self,**kwargs):
        &#34;&#34;&#34;Build the architecture base.        
        &#34;&#34;&#34;
        raise NotImplementedError()
    def build_normalization(self,**kwargs):
        raise NotImplementedError()
    def base_forward(self,**kwargs):
        raise NotImplementedError()
    def forward(self,**kwargs):
        raise NotImplementedError()

    



class CoLabelResnetAbstract(nn.Module):
    def __init__(self, base, weights=None, normalization=None, embedding_dimensions=None, soft_dimensions=None, **kwargs):
        &#34;&#34;&#34;Basic CoLabel Resnet model.

        A CoLabel model performs classification using corroboratively labeled data to generate labels for unlabeed data.

        Args:
            base (str): The architecture base for resnet, i.e. resnet50, resnet18
            weights (str, None): Path to weights file for the architecture base ONLY. If not provided, base initialized with random values.
            normalization (str, None): Can be None, where no normalization is used. Else create a normalization layer. Supports: [&#34;bn&#34;, &#34;l2&#34;, &#34;in&#34;, &#34;gn&#34;, &#34;ln&#34;]
            embedding_dimensions (int): Dimensions for the feature embedding. Leave empty if feature dimensions should be same as architecture core output (e.g. resnet50 base model has 2048-dim feature outputs). If providing a value, it should be less than the architecture core&#39;s base feature dimensions.
            soft_dimensions (int, None): Whether to include softmax classification layer. If None, softmax layer is not created in model.
        
        Kwargs (MODEL_KWARGS):
            last_stride (int, 1): The final stride parameter for the architecture core. Should be one of 1 or 2.
            attention (str, None): The attention module to use. Only supports [&#39;cbam&#39;, None]
            input_attention (bool, false): Whether to include the IA module
            ia_attention (bool, false): Whether to include input IA module
            part_attention (bool, false): Whether to include Part-CBAM Mobule
            secondary_attention (List[int], None): Whether to modify CBAM to apply it to specific Resnet basic blocks. None means CBAM is applied to all. Otherwise, CBAM is applied only to the basic block number provided here.

        Default Kwargs (DO NOT CHANGE OR ADD TO MODEL_KWARGS; set in backbones.resnet):
            zero_init_residual (bool, false): Whether the final layer uses zero initialization
            top_only (bool, true): Whether to keep only the architecture base without imagenet fully-connected layers (1000 classes)
            num_classes (int, 1000): Number of features in final imagenet FC layer
            groups (int, 1): Used during resnet variants construction
            width_per_group (int, 64): Used during resnet variants construction
            replace_stride_with_dilation (bool, None): Well, replace stride with dilation...
            norm_layer (nn.Module, None): The normalization layer within resnet. Internally defaults to nn.BatchNorm2D

        &#34;&#34;&#34;
        super(CoLabelResnetAbstract, self).__init__()
        self.base = None
        
        self.embedding_dimensions = embedding_dimensions
        self.soft_dimensions = soft_dimensions
        self.normalization = normalization if normalization != &#39;&#39; else None
        self.build_base(base, weights, **kwargs)    # All kwargs are passed into build_base,, which in turn passes kwargs into _resnet()
        
        self.feat_norm = None
        self.build_normalization(self.normalization)
        
        if self.soft_dimensions is not None:
            self.softmax = nn.Linear(self.embedding_dimensions, self.soft_dimensions, bias=False)
            self.softmax.apply(self.weights_init_softmax)
        else:
            self.softmax = None

    def weights_init_kaiming(self,m):
        classname = m.__class__.__name__
        if classname.find(&#39;Linear&#39;) != -1:
                nn.init.kaiming_normal_(m.weight, a=0, mode=&#39;fan_out&#39;)
                nn.init.constant_(m.bias, 0.0)
        elif classname.find(&#39;Conv&#39;) != -1:
                nn.init.kaiming_normal_(m.weight, a=0, mode=&#39;fan_in&#39;)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0.0)
        elif classname.find(&#39;BatchNorm&#39;) != -1:
                if m.affine:
                    nn.init.constant_(m.weight, 1.0)
                    nn.init.constant_(m.bias, 0.0)
        elif classname.find(&#39;GroupNorm&#39;) != -1:
                if m.affine:
                    nn.init.constant_(m.weight, 1.0)
                    nn.init.constant_(m.bias, 0.0)
        elif classname.find(&#39;LayerNorm&#39;) != -1:
                #if m.affine:
                #    nn.init.constant_(m.weight, 1.0)
                #    nn.init.constant_(m.bias, 0.0)
                pass
        elif classname.find(&#39;InstanceNorm&#39;) != -1:
                if m.affine:
                    nn.init.constant_(m.weight, 1.0)
                    nn.init.constant_(m.bias, 0.0)

    def weights_init_softmax(self, m):
        &#34;&#34;&#34; Initialize linear weights to standard normal. Mean 0. Standard Deviation 0.001 &#34;&#34;&#34;
        classname = m.__class__.__name__
        if classname.find(&#39;Linear&#39;) != -1:
                nn.init.normal_(m.weight, std=0.001)
                if m.bias:
                        nn.init.constant_(m.bias, 0.0)
    
    def partial_load(self,weights_path):
        params = torch.load(weights_path)
        for _key in params:
            if _key not in self.state_dict().keys() or params[_key].shape != self.state_dict()[_key].shape: 
                continue
            self.state_dict()[_key].copy_(params[_key])


    def build_base(self,**kwargs):
        &#34;&#34;&#34;Build the architecture base.        
        &#34;&#34;&#34;
        raise NotImplementedError()
    def build_normalization(self,**kwargs):
        raise NotImplementedError()
    def base_forward(self,**kwargs):
        raise NotImplementedError()
    def forward(self,**kwargs):
        raise NotImplementedError()



# prework notes -- copying over code to this format -- fix the inconsistencies

class CoLabelInterpretableResnetAbstract(nn.Module):
    def __init__(self, base, weights=None, normalization=None, embedding_dimensions=None, soft_dimensions=None, **kwargs):
        &#34;&#34;&#34;Basic CoLabel Interpretable Resnet model.

        A CoLabel model performs classification using corroboratively labeled data to generate labels for unlabeed data.
        
        assuming branches=3; we generate a resnet model with 3 branches, each with their feature output.

        These feature outputs are fed to a softmax, plus concatenated for final outputs.


        kwargs.branches determines how many complementary feature branches are in this colabel model

        Args:
            base (str): The architecture base for resnet, i.e. resnet50, resnet18
            weights (str, None): Path to weights file for the architecture base ONLY. If not provided, base initialized with random values.
            normalization (str, None): Can be None, where no normalization is used. Else create a normalization layer. Supports: [&#34;bn&#34;, &#34;l2&#34;, &#34;in&#34;, &#34;gn&#34;, &#34;ln&#34;]
            embedding_dimensions (int): Dimensions for the feature embedding. Leave empty if feature dimensions should be same as architecture core output (e.g. resnet50 base model has 2048-dim feature outputs). If providing a value, it should be less than the architecture core&#39;s base feature dimensions.
            soft_dimensions (int, None): Whether to include softmax classification layer. If None, softmax layer is not created in model.
        
        Kwargs (MODEL_KWARGS):
            last_stride (int, 1): The final stride parameter for the architecture core. Should be one of 1 or 2.
            attention (str, None): The attention module to use. Only supports [&#39;cbam&#39;, None]
            input_attention (bool, false): Whether to include the IA module
            ia_attention (bool, false): Whether to include input IA module
            part_attention (bool, false): Whether to include Part-CBAM Mobule
            secondary_attention (List[int], None): Whether to modify CBAM to apply it to specific Resnet basic blocks. None means CBAM is applied to all. Otherwise, CBAM is applied only to the basic block number provided here.

        Default Kwargs (DO NOT CHANGE OR ADD TO MODEL_KWARGS; set in backbones.resnet):
            zero_init_residual (bool, false): Whether the final layer uses zero initialization
            top_only (bool, true): Whether to keep only the architecture base without imagenet fully-connected layers (1000 classes)
            num_classes (int, 1000): Number of features in final imagenet FC layer
            groups (int, 1): Used during resnet variants construction
            width_per_group (int, 64): Used during resnet variants construction
            replace_stride_with_dilation (bool, None): Well, replace stride with dilation...
            norm_layer (nn.Module, None): The normalization layer within resnet. Internally defaults to nn.BatchNorm2D

        &#34;&#34;&#34;
        super(CoLabelInterpretableResnetAbstract, self).__init__()
        self.base = None
        
        self.embedding_dimensions = embedding_dimensions
        self.soft_dimensions = soft_dimensions
        self.normalization = normalization if normalization != &#39;&#39; else None
        #This is the feature extractor
        self.build_base(base, weights, **kwargs)    # All kwargs are passed into build_base,, which in turn passes kwargs into _resnet()
        
        #The specific type of ending feature normalizer
        self.feat_norm = None
        self.build_normalization(self.normalization)
        
        # This is the softmax dimensions...
        self.build_softmax()

    def build_softmax(self):
        if self.soft_dimensions is not None:
            self.softmax = nn.Linear(self.embedding_dimensions, self.soft_dimensions, bias=False)
            self.softmax.apply(self.weights_init_softmax)
        else:
            self.softmax = None

    def weights_init_kaiming(self,m):
        classname = m.__class__.__name__
        if classname.find(&#39;Linear&#39;) != -1:
                nn.init.kaiming_normal_(m.weight, a=0, mode=&#39;fan_out&#39;)
                nn.init.constant_(m.bias, 0.0)
        elif classname.find(&#39;Conv&#39;) != -1:
                nn.init.kaiming_normal_(m.weight, a=0, mode=&#39;fan_in&#39;)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0.0)
        elif classname.find(&#39;BatchNorm&#39;) != -1:
                if m.affine:
                    nn.init.constant_(m.weight, 1.0)
                    nn.init.constant_(m.bias, 0.0)
        elif classname.find(&#39;GroupNorm&#39;) != -1:
                if m.affine:
                    nn.init.constant_(m.weight, 1.0)
                    nn.init.constant_(m.bias, 0.0)
        elif classname.find(&#39;LayerNorm&#39;) != -1:
                #if m.affine:
                #    nn.init.constant_(m.weight, 1.0)
                #    nn.init.constant_(m.bias, 0.0)
                pass
        elif classname.find(&#39;InstanceNorm&#39;) != -1:
                if m.affine:
                    nn.init.constant_(m.weight, 1.0)
                    nn.init.constant_(m.bias, 0.0)

    def weights_init_softmax(self, m):
        &#34;&#34;&#34; Initialize linear weights to standard normal. Mean 0. Standard Deviation 0.001 &#34;&#34;&#34;
        classname = m.__class__.__name__
        if classname.find(&#39;Linear&#39;) != -1:
                nn.init.normal_(m.weight, std=0.001)
                if m.bias:
                        nn.init.constant_(m.bias, 0.0)
    
    def partial_load(self,weights_path):
        params = torch.load(weights_path)
        for _key in params:
            if _key not in self.state_dict().keys() or params[_key].shape != self.state_dict()[_key].shape: 
                continue
            self.state_dict()[_key].copy_(params[_key])


    def build_base(self,**kwargs):
        &#34;&#34;&#34;Build the architecture base.        
        &#34;&#34;&#34;
        raise NotImplementedError()
    def build_normalization(self,**kwargs):
        raise NotImplementedError()
    def base_forward(self,**kwargs):
        raise NotImplementedError()
    def forward(self,**kwargs):
        raise NotImplementedError()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract"><code class="flex name class">
<span>class <span class="ident">CoLabelInterpretableResnetAbstract</span></span>
<span>(</span><span>base, weights=None, normalization=None, embedding_dimensions=None, soft_dimensions=None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Basic CoLabel Interpretable Resnet model.</p>
<p>A CoLabel model performs classification using corroboratively labeled data to generate labels for unlabeed data.</p>
<p>assuming branches=3; we generate a resnet model with 3 branches, each with their feature output.</p>
<p>These feature outputs are fed to a softmax, plus concatenated for final outputs.</p>
<p>kwargs.branches determines how many complementary feature branches are in this colabel model</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>base</code></strong> :&ensp;<code>str</code></dt>
<dd>The architecture base for resnet, i.e. resnet50, resnet18</dd>
<dt><strong><code>weights</code></strong> :&ensp;<code>str, None</code></dt>
<dd>Path to weights file for the architecture base ONLY. If not provided, base initialized with random values.</dd>
<dt><strong><code>normalization</code></strong> :&ensp;<code>str, None</code></dt>
<dd>Can be None, where no normalization is used. Else create a normalization layer. Supports: ["bn", "l2", "in", "gn", "ln"]</dd>
<dt><strong><code>embedding_dimensions</code></strong> :&ensp;<code>int</code></dt>
<dd>Dimensions for the feature embedding. Leave empty if feature dimensions should be same as architecture core output (e.g. resnet50 base model has 2048-dim feature outputs). If providing a value, it should be less than the architecture core's base feature dimensions.</dd>
<dt><strong><code>soft_dimensions</code></strong> :&ensp;<code>int, None</code></dt>
<dd>Whether to include softmax classification layer. If None, softmax layer is not created in model.</dd>
</dl>
<p>Kwargs (MODEL_KWARGS):
last_stride (int, 1): The final stride parameter for the architecture core. Should be one of 1 or 2.
attention (str, None): The attention module to use. Only supports ['cbam', None]
input_attention (bool, false): Whether to include the IA module
ia_attention (bool, false): Whether to include input IA module
part_attention (bool, false): Whether to include Part-CBAM Mobule
secondary_attention (List[int], None): Whether to modify CBAM to apply it to specific Resnet basic blocks. None means CBAM is applied to all. Otherwise, CBAM is applied only to the basic block number provided here.</p>
<p>Default Kwargs (DO NOT CHANGE OR ADD TO MODEL_KWARGS; set in backbones.resnet):
zero_init_residual (bool, false): Whether the final layer uses zero initialization
top_only (bool, true): Whether to keep only the architecture base without imagenet fully-connected layers (1000 classes)
num_classes (int, 1000): Number of features in final imagenet FC layer
groups (int, 1): Used during resnet variants construction
width_per_group (int, 64): Used during resnet variants construction
replace_stride_with_dilation (bool, None): Well, replace stride with dilation&hellip;
norm_layer (nn.Module, None): The normalization layer within resnet. Internally defaults to nn.BatchNorm2D</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CoLabelInterpretableResnetAbstract(nn.Module):
    def __init__(self, base, weights=None, normalization=None, embedding_dimensions=None, soft_dimensions=None, **kwargs):
        &#34;&#34;&#34;Basic CoLabel Interpretable Resnet model.

        A CoLabel model performs classification using corroboratively labeled data to generate labels for unlabeed data.
        
        assuming branches=3; we generate a resnet model with 3 branches, each with their feature output.

        These feature outputs are fed to a softmax, plus concatenated for final outputs.


        kwargs.branches determines how many complementary feature branches are in this colabel model

        Args:
            base (str): The architecture base for resnet, i.e. resnet50, resnet18
            weights (str, None): Path to weights file for the architecture base ONLY. If not provided, base initialized with random values.
            normalization (str, None): Can be None, where no normalization is used. Else create a normalization layer. Supports: [&#34;bn&#34;, &#34;l2&#34;, &#34;in&#34;, &#34;gn&#34;, &#34;ln&#34;]
            embedding_dimensions (int): Dimensions for the feature embedding. Leave empty if feature dimensions should be same as architecture core output (e.g. resnet50 base model has 2048-dim feature outputs). If providing a value, it should be less than the architecture core&#39;s base feature dimensions.
            soft_dimensions (int, None): Whether to include softmax classification layer. If None, softmax layer is not created in model.
        
        Kwargs (MODEL_KWARGS):
            last_stride (int, 1): The final stride parameter for the architecture core. Should be one of 1 or 2.
            attention (str, None): The attention module to use. Only supports [&#39;cbam&#39;, None]
            input_attention (bool, false): Whether to include the IA module
            ia_attention (bool, false): Whether to include input IA module
            part_attention (bool, false): Whether to include Part-CBAM Mobule
            secondary_attention (List[int], None): Whether to modify CBAM to apply it to specific Resnet basic blocks. None means CBAM is applied to all. Otherwise, CBAM is applied only to the basic block number provided here.

        Default Kwargs (DO NOT CHANGE OR ADD TO MODEL_KWARGS; set in backbones.resnet):
            zero_init_residual (bool, false): Whether the final layer uses zero initialization
            top_only (bool, true): Whether to keep only the architecture base without imagenet fully-connected layers (1000 classes)
            num_classes (int, 1000): Number of features in final imagenet FC layer
            groups (int, 1): Used during resnet variants construction
            width_per_group (int, 64): Used during resnet variants construction
            replace_stride_with_dilation (bool, None): Well, replace stride with dilation...
            norm_layer (nn.Module, None): The normalization layer within resnet. Internally defaults to nn.BatchNorm2D

        &#34;&#34;&#34;
        super(CoLabelInterpretableResnetAbstract, self).__init__()
        self.base = None
        
        self.embedding_dimensions = embedding_dimensions
        self.soft_dimensions = soft_dimensions
        self.normalization = normalization if normalization != &#39;&#39; else None
        #This is the feature extractor
        self.build_base(base, weights, **kwargs)    # All kwargs are passed into build_base,, which in turn passes kwargs into _resnet()
        
        #The specific type of ending feature normalizer
        self.feat_norm = None
        self.build_normalization(self.normalization)
        
        # This is the softmax dimensions...
        self.build_softmax()

    def build_softmax(self):
        if self.soft_dimensions is not None:
            self.softmax = nn.Linear(self.embedding_dimensions, self.soft_dimensions, bias=False)
            self.softmax.apply(self.weights_init_softmax)
        else:
            self.softmax = None

    def weights_init_kaiming(self,m):
        classname = m.__class__.__name__
        if classname.find(&#39;Linear&#39;) != -1:
                nn.init.kaiming_normal_(m.weight, a=0, mode=&#39;fan_out&#39;)
                nn.init.constant_(m.bias, 0.0)
        elif classname.find(&#39;Conv&#39;) != -1:
                nn.init.kaiming_normal_(m.weight, a=0, mode=&#39;fan_in&#39;)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0.0)
        elif classname.find(&#39;BatchNorm&#39;) != -1:
                if m.affine:
                    nn.init.constant_(m.weight, 1.0)
                    nn.init.constant_(m.bias, 0.0)
        elif classname.find(&#39;GroupNorm&#39;) != -1:
                if m.affine:
                    nn.init.constant_(m.weight, 1.0)
                    nn.init.constant_(m.bias, 0.0)
        elif classname.find(&#39;LayerNorm&#39;) != -1:
                #if m.affine:
                #    nn.init.constant_(m.weight, 1.0)
                #    nn.init.constant_(m.bias, 0.0)
                pass
        elif classname.find(&#39;InstanceNorm&#39;) != -1:
                if m.affine:
                    nn.init.constant_(m.weight, 1.0)
                    nn.init.constant_(m.bias, 0.0)

    def weights_init_softmax(self, m):
        &#34;&#34;&#34; Initialize linear weights to standard normal. Mean 0. Standard Deviation 0.001 &#34;&#34;&#34;
        classname = m.__class__.__name__
        if classname.find(&#39;Linear&#39;) != -1:
                nn.init.normal_(m.weight, std=0.001)
                if m.bias:
                        nn.init.constant_(m.bias, 0.0)
    
    def partial_load(self,weights_path):
        params = torch.load(weights_path)
        for _key in params:
            if _key not in self.state_dict().keys() or params[_key].shape != self.state_dict()[_key].shape: 
                continue
            self.state_dict()[_key].copy_(params[_key])


    def build_base(self,**kwargs):
        &#34;&#34;&#34;Build the architecture base.        
        &#34;&#34;&#34;
        raise NotImplementedError()
    def build_normalization(self,**kwargs):
        raise NotImplementedError()
    def base_forward(self,**kwargs):
        raise NotImplementedError()
    def forward(self,**kwargs):
        raise NotImplementedError()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="GLAMOR.models.CoLabelInterpretableResNet.CoLabelInterpretableResnet" href="CoLabelInterpretableResNet.html#GLAMOR.models.CoLabelInterpretableResNet.CoLabelInterpretableResnet">CoLabelInterpretableResnet</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract.base_forward"><code class="name flex">
<span>def <span class="ident">base_forward</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def base_forward(self,**kwargs):
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract.build_base"><code class="name flex">
<span>def <span class="ident">build_base</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Build the architecture base.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_base(self,**kwargs):
    &#34;&#34;&#34;Build the architecture base.        
    &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract.build_normalization"><code class="name flex">
<span>def <span class="ident">build_normalization</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_normalization(self,**kwargs):
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract.build_softmax"><code class="name flex">
<span>def <span class="ident">build_softmax</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_softmax(self):
    if self.soft_dimensions is not None:
        self.softmax = nn.Linear(self.embedding_dimensions, self.soft_dimensions, bias=False)
        self.softmax.apply(self.weights_init_softmax)
    else:
        self.softmax = None</code></pre>
</details>
</dd>
<dt id="GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, **kwargs) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self,**kwargs):
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract.partial_load"><code class="name flex">
<span>def <span class="ident">partial_load</span></span>(<span>self, weights_path)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def partial_load(self,weights_path):
    params = torch.load(weights_path)
    for _key in params:
        if _key not in self.state_dict().keys() or params[_key].shape != self.state_dict()[_key].shape: 
            continue
        self.state_dict()[_key].copy_(params[_key])</code></pre>
</details>
</dd>
<dt id="GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract.weights_init_kaiming"><code class="name flex">
<span>def <span class="ident">weights_init_kaiming</span></span>(<span>self, m)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def weights_init_kaiming(self,m):
    classname = m.__class__.__name__
    if classname.find(&#39;Linear&#39;) != -1:
            nn.init.kaiming_normal_(m.weight, a=0, mode=&#39;fan_out&#39;)
            nn.init.constant_(m.bias, 0.0)
    elif classname.find(&#39;Conv&#39;) != -1:
            nn.init.kaiming_normal_(m.weight, a=0, mode=&#39;fan_in&#39;)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0.0)
    elif classname.find(&#39;BatchNorm&#39;) != -1:
            if m.affine:
                nn.init.constant_(m.weight, 1.0)
                nn.init.constant_(m.bias, 0.0)
    elif classname.find(&#39;GroupNorm&#39;) != -1:
            if m.affine:
                nn.init.constant_(m.weight, 1.0)
                nn.init.constant_(m.bias, 0.0)
    elif classname.find(&#39;LayerNorm&#39;) != -1:
            #if m.affine:
            #    nn.init.constant_(m.weight, 1.0)
            #    nn.init.constant_(m.bias, 0.0)
            pass
    elif classname.find(&#39;InstanceNorm&#39;) != -1:
            if m.affine:
                nn.init.constant_(m.weight, 1.0)
                nn.init.constant_(m.bias, 0.0)</code></pre>
</details>
</dd>
<dt id="GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract.weights_init_softmax"><code class="name flex">
<span>def <span class="ident">weights_init_softmax</span></span>(<span>self, m)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize linear weights to standard normal. Mean 0. Standard Deviation 0.001</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def weights_init_softmax(self, m):
    &#34;&#34;&#34; Initialize linear weights to standard normal. Mean 0. Standard Deviation 0.001 &#34;&#34;&#34;
    classname = m.__class__.__name__
    if classname.find(&#39;Linear&#39;) != -1:
            nn.init.normal_(m.weight, std=0.001)
            if m.bias:
                    nn.init.constant_(m.bias, 0.0)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="GLAMOR.models.abstracts.CoLabelResnetAbstract"><code class="flex name class">
<span>class <span class="ident">CoLabelResnetAbstract</span></span>
<span>(</span><span>base, weights=None, normalization=None, embedding_dimensions=None, soft_dimensions=None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Basic CoLabel Resnet model.</p>
<p>A CoLabel model performs classification using corroboratively labeled data to generate labels for unlabeed data.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>base</code></strong> :&ensp;<code>str</code></dt>
<dd>The architecture base for resnet, i.e. resnet50, resnet18</dd>
<dt><strong><code>weights</code></strong> :&ensp;<code>str, None</code></dt>
<dd>Path to weights file for the architecture base ONLY. If not provided, base initialized with random values.</dd>
<dt><strong><code>normalization</code></strong> :&ensp;<code>str, None</code></dt>
<dd>Can be None, where no normalization is used. Else create a normalization layer. Supports: ["bn", "l2", "in", "gn", "ln"]</dd>
<dt><strong><code>embedding_dimensions</code></strong> :&ensp;<code>int</code></dt>
<dd>Dimensions for the feature embedding. Leave empty if feature dimensions should be same as architecture core output (e.g. resnet50 base model has 2048-dim feature outputs). If providing a value, it should be less than the architecture core's base feature dimensions.</dd>
<dt><strong><code>soft_dimensions</code></strong> :&ensp;<code>int, None</code></dt>
<dd>Whether to include softmax classification layer. If None, softmax layer is not created in model.</dd>
</dl>
<p>Kwargs (MODEL_KWARGS):
last_stride (int, 1): The final stride parameter for the architecture core. Should be one of 1 or 2.
attention (str, None): The attention module to use. Only supports ['cbam', None]
input_attention (bool, false): Whether to include the IA module
ia_attention (bool, false): Whether to include input IA module
part_attention (bool, false): Whether to include Part-CBAM Mobule
secondary_attention (List[int], None): Whether to modify CBAM to apply it to specific Resnet basic blocks. None means CBAM is applied to all. Otherwise, CBAM is applied only to the basic block number provided here.</p>
<p>Default Kwargs (DO NOT CHANGE OR ADD TO MODEL_KWARGS; set in backbones.resnet):
zero_init_residual (bool, false): Whether the final layer uses zero initialization
top_only (bool, true): Whether to keep only the architecture base without imagenet fully-connected layers (1000 classes)
num_classes (int, 1000): Number of features in final imagenet FC layer
groups (int, 1): Used during resnet variants construction
width_per_group (int, 64): Used during resnet variants construction
replace_stride_with_dilation (bool, None): Well, replace stride with dilation&hellip;
norm_layer (nn.Module, None): The normalization layer within resnet. Internally defaults to nn.BatchNorm2D</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CoLabelResnetAbstract(nn.Module):
    def __init__(self, base, weights=None, normalization=None, embedding_dimensions=None, soft_dimensions=None, **kwargs):
        &#34;&#34;&#34;Basic CoLabel Resnet model.

        A CoLabel model performs classification using corroboratively labeled data to generate labels for unlabeed data.

        Args:
            base (str): The architecture base for resnet, i.e. resnet50, resnet18
            weights (str, None): Path to weights file for the architecture base ONLY. If not provided, base initialized with random values.
            normalization (str, None): Can be None, where no normalization is used. Else create a normalization layer. Supports: [&#34;bn&#34;, &#34;l2&#34;, &#34;in&#34;, &#34;gn&#34;, &#34;ln&#34;]
            embedding_dimensions (int): Dimensions for the feature embedding. Leave empty if feature dimensions should be same as architecture core output (e.g. resnet50 base model has 2048-dim feature outputs). If providing a value, it should be less than the architecture core&#39;s base feature dimensions.
            soft_dimensions (int, None): Whether to include softmax classification layer. If None, softmax layer is not created in model.
        
        Kwargs (MODEL_KWARGS):
            last_stride (int, 1): The final stride parameter for the architecture core. Should be one of 1 or 2.
            attention (str, None): The attention module to use. Only supports [&#39;cbam&#39;, None]
            input_attention (bool, false): Whether to include the IA module
            ia_attention (bool, false): Whether to include input IA module
            part_attention (bool, false): Whether to include Part-CBAM Mobule
            secondary_attention (List[int], None): Whether to modify CBAM to apply it to specific Resnet basic blocks. None means CBAM is applied to all. Otherwise, CBAM is applied only to the basic block number provided here.

        Default Kwargs (DO NOT CHANGE OR ADD TO MODEL_KWARGS; set in backbones.resnet):
            zero_init_residual (bool, false): Whether the final layer uses zero initialization
            top_only (bool, true): Whether to keep only the architecture base without imagenet fully-connected layers (1000 classes)
            num_classes (int, 1000): Number of features in final imagenet FC layer
            groups (int, 1): Used during resnet variants construction
            width_per_group (int, 64): Used during resnet variants construction
            replace_stride_with_dilation (bool, None): Well, replace stride with dilation...
            norm_layer (nn.Module, None): The normalization layer within resnet. Internally defaults to nn.BatchNorm2D

        &#34;&#34;&#34;
        super(CoLabelResnetAbstract, self).__init__()
        self.base = None
        
        self.embedding_dimensions = embedding_dimensions
        self.soft_dimensions = soft_dimensions
        self.normalization = normalization if normalization != &#39;&#39; else None
        self.build_base(base, weights, **kwargs)    # All kwargs are passed into build_base,, which in turn passes kwargs into _resnet()
        
        self.feat_norm = None
        self.build_normalization(self.normalization)
        
        if self.soft_dimensions is not None:
            self.softmax = nn.Linear(self.embedding_dimensions, self.soft_dimensions, bias=False)
            self.softmax.apply(self.weights_init_softmax)
        else:
            self.softmax = None

    def weights_init_kaiming(self,m):
        classname = m.__class__.__name__
        if classname.find(&#39;Linear&#39;) != -1:
                nn.init.kaiming_normal_(m.weight, a=0, mode=&#39;fan_out&#39;)
                nn.init.constant_(m.bias, 0.0)
        elif classname.find(&#39;Conv&#39;) != -1:
                nn.init.kaiming_normal_(m.weight, a=0, mode=&#39;fan_in&#39;)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0.0)
        elif classname.find(&#39;BatchNorm&#39;) != -1:
                if m.affine:
                    nn.init.constant_(m.weight, 1.0)
                    nn.init.constant_(m.bias, 0.0)
        elif classname.find(&#39;GroupNorm&#39;) != -1:
                if m.affine:
                    nn.init.constant_(m.weight, 1.0)
                    nn.init.constant_(m.bias, 0.0)
        elif classname.find(&#39;LayerNorm&#39;) != -1:
                #if m.affine:
                #    nn.init.constant_(m.weight, 1.0)
                #    nn.init.constant_(m.bias, 0.0)
                pass
        elif classname.find(&#39;InstanceNorm&#39;) != -1:
                if m.affine:
                    nn.init.constant_(m.weight, 1.0)
                    nn.init.constant_(m.bias, 0.0)

    def weights_init_softmax(self, m):
        &#34;&#34;&#34; Initialize linear weights to standard normal. Mean 0. Standard Deviation 0.001 &#34;&#34;&#34;
        classname = m.__class__.__name__
        if classname.find(&#39;Linear&#39;) != -1:
                nn.init.normal_(m.weight, std=0.001)
                if m.bias:
                        nn.init.constant_(m.bias, 0.0)
    
    def partial_load(self,weights_path):
        params = torch.load(weights_path)
        for _key in params:
            if _key not in self.state_dict().keys() or params[_key].shape != self.state_dict()[_key].shape: 
                continue
            self.state_dict()[_key].copy_(params[_key])


    def build_base(self,**kwargs):
        &#34;&#34;&#34;Build the architecture base.        
        &#34;&#34;&#34;
        raise NotImplementedError()
    def build_normalization(self,**kwargs):
        raise NotImplementedError()
    def base_forward(self,**kwargs):
        raise NotImplementedError()
    def forward(self,**kwargs):
        raise NotImplementedError()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="GLAMOR.models.CoLabelResnet.CoLabelResnet" href="CoLabelResnet.html#GLAMOR.models.CoLabelResnet.CoLabelResnet">CoLabelResnet</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="GLAMOR.models.abstracts.CoLabelResnetAbstract.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="GLAMOR.models.abstracts.CoLabelResnetAbstract.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="GLAMOR.models.abstracts.CoLabelResnetAbstract.base_forward"><code class="name flex">
<span>def <span class="ident">base_forward</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def base_forward(self,**kwargs):
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="GLAMOR.models.abstracts.CoLabelResnetAbstract.build_base"><code class="name flex">
<span>def <span class="ident">build_base</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Build the architecture base.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_base(self,**kwargs):
    &#34;&#34;&#34;Build the architecture base.        
    &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="GLAMOR.models.abstracts.CoLabelResnetAbstract.build_normalization"><code class="name flex">
<span>def <span class="ident">build_normalization</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_normalization(self,**kwargs):
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="GLAMOR.models.abstracts.CoLabelResnetAbstract.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, **kwargs) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self,**kwargs):
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="GLAMOR.models.abstracts.CoLabelResnetAbstract.partial_load"><code class="name flex">
<span>def <span class="ident">partial_load</span></span>(<span>self, weights_path)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def partial_load(self,weights_path):
    params = torch.load(weights_path)
    for _key in params:
        if _key not in self.state_dict().keys() or params[_key].shape != self.state_dict()[_key].shape: 
            continue
        self.state_dict()[_key].copy_(params[_key])</code></pre>
</details>
</dd>
<dt id="GLAMOR.models.abstracts.CoLabelResnetAbstract.weights_init_kaiming"><code class="name flex">
<span>def <span class="ident">weights_init_kaiming</span></span>(<span>self, m)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def weights_init_kaiming(self,m):
    classname = m.__class__.__name__
    if classname.find(&#39;Linear&#39;) != -1:
            nn.init.kaiming_normal_(m.weight, a=0, mode=&#39;fan_out&#39;)
            nn.init.constant_(m.bias, 0.0)
    elif classname.find(&#39;Conv&#39;) != -1:
            nn.init.kaiming_normal_(m.weight, a=0, mode=&#39;fan_in&#39;)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0.0)
    elif classname.find(&#39;BatchNorm&#39;) != -1:
            if m.affine:
                nn.init.constant_(m.weight, 1.0)
                nn.init.constant_(m.bias, 0.0)
    elif classname.find(&#39;GroupNorm&#39;) != -1:
            if m.affine:
                nn.init.constant_(m.weight, 1.0)
                nn.init.constant_(m.bias, 0.0)
    elif classname.find(&#39;LayerNorm&#39;) != -1:
            #if m.affine:
            #    nn.init.constant_(m.weight, 1.0)
            #    nn.init.constant_(m.bias, 0.0)
            pass
    elif classname.find(&#39;InstanceNorm&#39;) != -1:
            if m.affine:
                nn.init.constant_(m.weight, 1.0)
                nn.init.constant_(m.bias, 0.0)</code></pre>
</details>
</dd>
<dt id="GLAMOR.models.abstracts.CoLabelResnetAbstract.weights_init_softmax"><code class="name flex">
<span>def <span class="ident">weights_init_softmax</span></span>(<span>self, m)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize linear weights to standard normal. Mean 0. Standard Deviation 0.001</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def weights_init_softmax(self, m):
    &#34;&#34;&#34; Initialize linear weights to standard normal. Mean 0. Standard Deviation 0.001 &#34;&#34;&#34;
    classname = m.__class__.__name__
    if classname.find(&#39;Linear&#39;) != -1:
            nn.init.normal_(m.weight, std=0.001)
            if m.bias:
                    nn.init.constant_(m.bias, 0.0)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="GLAMOR.models.abstracts.ReidModel"><code class="flex name class">
<span>class <span class="ident">ReidModel</span></span>
<span>(</span><span>base, weights=None, normalization=None, embedding_dimensions=None, soft_dimensions=None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Basic ReID Resnet model.</p>
<p>A ReID model performs re-identification by generating embeddings such that the same class's embeddings are closer together.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>base</code></strong> :&ensp;<code>str</code></dt>
<dd>The architecture base for resnet, i.e. resnet50, resnet18</dd>
<dt><strong><code>weights</code></strong> :&ensp;<code>str, None</code></dt>
<dd>Path to weights file for the architecture base ONLY. If not provided, base initialized with random values.</dd>
<dt><strong><code>normalization</code></strong> :&ensp;<code>str, None</code></dt>
<dd>Can be None, where no normalization is used. Else create a normalization layer. Supports: ["bn", "l2", "in", "gn", "ln"]</dd>
<dt><strong><code>embedding_dimensions</code></strong> :&ensp;<code>int</code></dt>
<dd>Dimensions for the feature embedding. Leave empty if feature dimensions should be same as architecture core output (e.g. resnet50 base model has 2048-dim feature outputs). If providing a value, it should be less than the architecture core's base feature dimensions.</dd>
<dt><strong><code>soft_dimensions</code></strong> :&ensp;<code>int, None</code></dt>
<dd>Whether to include softmax classification layer. If None, softmax layer is not created in model.</dd>
</dl>
<p>Kwargs (MODEL_KWARGS):
last_stride (int, 1): The final stride parameter for the architecture core. Should be one of 1 or 2.
attention (str, None): The attention module to use. Only supports ['cbam', None]
input_attention (bool, false): Whether to include the IA module
ia_attention (bool, false): Whether to include input IA module
part_attention (bool, false): Whether to include Part-CBAM Mobule
secondary_attention (int, None): Whether to modify CBAM to apply it to specific Resnet basic blocks. None means CBAM is applied to all. Otherwise, CBAM is applied only to the basic block number provided here.</p>
<p>Default Kwargs (DO NOT CHANGE OR ADD TO MODEL_KWARGS; set in backbones.resnet):
zero_init_residual (bool, false): Whether the final layer uses zero initialization
top_only (bool, true): Whether to keep only the architecture base without imagenet fully-connected layers (1000 classes)
num_classes (int, 1000): Number of features in final imagenet FC layer
groups (int, 1): Used during resnet variants construction
width_per_group (int, 64): Used during resnet variants construction
replace_stride_with_dilation (bool, None): Well, replace stride with dilation&hellip;
norm_layer (nn.Module, None): The normalization layer within resnet. Internally defaults to nn.BatchNorm2D</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ReidModel(nn.Module):
    def __init__(self, base, weights=None, normalization=None, embedding_dimensions=None, soft_dimensions=None, **kwargs):
        &#34;&#34;&#34;Basic ReID Resnet model.

        A ReID model performs re-identification by generating embeddings such that the same class&#39;s embeddings are closer together.

        Args:
            base (str): The architecture base for resnet, i.e. resnet50, resnet18
            weights (str, None): Path to weights file for the architecture base ONLY. If not provided, base initialized with random values.
            normalization (str, None): Can be None, where no normalization is used. Else create a normalization layer. Supports: [&#34;bn&#34;, &#34;l2&#34;, &#34;in&#34;, &#34;gn&#34;, &#34;ln&#34;]
            embedding_dimensions (int): Dimensions for the feature embedding. Leave empty if feature dimensions should be same as architecture core output (e.g. resnet50 base model has 2048-dim feature outputs). If providing a value, it should be less than the architecture core&#39;s base feature dimensions.
            soft_dimensions (int, None): Whether to include softmax classification layer. If None, softmax layer is not created in model.
        
        Kwargs (MODEL_KWARGS):
            last_stride (int, 1): The final stride parameter for the architecture core. Should be one of 1 or 2.
            attention (str, None): The attention module to use. Only supports [&#39;cbam&#39;, None]
            input_attention (bool, false): Whether to include the IA module
            ia_attention (bool, false): Whether to include input IA module
            part_attention (bool, false): Whether to include Part-CBAM Mobule
            secondary_attention (int, None): Whether to modify CBAM to apply it to specific Resnet basic blocks. None means CBAM is applied to all. Otherwise, CBAM is applied only to the basic block number provided here.

        Default Kwargs (DO NOT CHANGE OR ADD TO MODEL_KWARGS; set in backbones.resnet):
            zero_init_residual (bool, false): Whether the final layer uses zero initialization
            top_only (bool, true): Whether to keep only the architecture base without imagenet fully-connected layers (1000 classes)
            num_classes (int, 1000): Number of features in final imagenet FC layer
            groups (int, 1): Used during resnet variants construction
            width_per_group (int, 64): Used during resnet variants construction
            replace_stride_with_dilation (bool, None): Well, replace stride with dilation...
            norm_layer (nn.Module, None): The normalization layer within resnet. Internally defaults to nn.BatchNorm2D

        &#34;&#34;&#34;
        super(ReidModel, self).__init__()
        self.base = None
        
        self.embedding_dimensions = embedding_dimensions
        self.soft_dimensions = soft_dimensions
        self.normalization = normalization if normalization != &#39;&#39; else None
        self.build_base(base, weights, **kwargs)    # All kwargs are passed into build_base,, which in turn passes kwargs into _resnet()
        
        self.feat_norm = None
        self.build_normalization(self.normalization)
        
        if self.soft_dimensions is not None:
            self.softmax = nn.Linear(self.embedding_dimensions, self.soft_dimensions, bias=False)
            self.softmax.apply(self.weights_init_softmax)
        else:
            self.softmax = None

    def weights_init_kaiming(self,m):
        classname = m.__class__.__name__
        if classname.find(&#39;Linear&#39;) != -1:
                nn.init.kaiming_normal_(m.weight, a=0, mode=&#39;fan_out&#39;)
                nn.init.constant_(m.bias, 0.0)
        elif classname.find(&#39;Conv&#39;) != -1:
                nn.init.kaiming_normal_(m.weight, a=0, mode=&#39;fan_in&#39;)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0.0)
        elif classname.find(&#39;BatchNorm&#39;) != -1:
                if m.affine:
                    nn.init.constant_(m.weight, 1.0)
                    nn.init.constant_(m.bias, 0.0)
        elif classname.find(&#39;GroupNorm&#39;) != -1:
                if m.affine:
                    nn.init.constant_(m.weight, 1.0)
                    nn.init.constant_(m.bias, 0.0)
        elif classname.find(&#39;LayerNorm&#39;) != -1:
                #if m.affine:
                #    nn.init.constant_(m.weight, 1.0)
                #    nn.init.constant_(m.bias, 0.0)
                pass
        elif classname.find(&#39;InstanceNorm&#39;) != -1:
                if m.affine:
                    nn.init.constant_(m.weight, 1.0)
                    nn.init.constant_(m.bias, 0.0)

    def weights_init_softmax(self, m):
        &#34;&#34;&#34; Initialize linear weights to standard normal. Mean 0. Standard Deviation 0.001 &#34;&#34;&#34;
        classname = m.__class__.__name__
        if classname.find(&#39;Linear&#39;) != -1:
                nn.init.normal_(m.weight, std=0.001)
                if m.bias:
                        nn.init.constant_(m.bias, 0.0)
    
    def partial_load(self,weights_path):
        params = torch.load(weights_path)
        for _key in params:
            if _key not in self.state_dict().keys() or params[_key].shape != self.state_dict()[_key].shape: 
                continue
            self.state_dict()[_key].copy_(params[_key])


    def build_base(self,**kwargs):
        &#34;&#34;&#34;Build the architecture base.        
        &#34;&#34;&#34;
        raise NotImplementedError()
    def build_normalization(self,**kwargs):
        raise NotImplementedError()
    def base_forward(self,**kwargs):
        raise NotImplementedError()
    def forward(self,**kwargs):
        raise NotImplementedError()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="GLAMOR.models.CarzamResnet.CarzamResnet" href="CarzamResnet.html#GLAMOR.models.CarzamResnet.CarzamResnet">CarzamResnet</a></li>
<li><a title="GLAMOR.models.ResnetBase.ResnetBase" href="ResnetBase.html#GLAMOR.models.ResnetBase.ResnetBase">ResnetBase</a></li>
<li><a title="GLAMOR.models.ShuffleNetBase.ShuffleNetBase" href="ShuffleNetBase.html#GLAMOR.models.ShuffleNetBase.ShuffleNetBase">ShuffleNetBase</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="GLAMOR.models.abstracts.ReidModel.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="GLAMOR.models.abstracts.ReidModel.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="GLAMOR.models.abstracts.ReidModel.base_forward"><code class="name flex">
<span>def <span class="ident">base_forward</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def base_forward(self,**kwargs):
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="GLAMOR.models.abstracts.ReidModel.build_base"><code class="name flex">
<span>def <span class="ident">build_base</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Build the architecture base.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_base(self,**kwargs):
    &#34;&#34;&#34;Build the architecture base.        
    &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="GLAMOR.models.abstracts.ReidModel.build_normalization"><code class="name flex">
<span>def <span class="ident">build_normalization</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_normalization(self,**kwargs):
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="GLAMOR.models.abstracts.ReidModel.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, **kwargs) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self,**kwargs):
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="GLAMOR.models.abstracts.ReidModel.partial_load"><code class="name flex">
<span>def <span class="ident">partial_load</span></span>(<span>self, weights_path)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def partial_load(self,weights_path):
    params = torch.load(weights_path)
    for _key in params:
        if _key not in self.state_dict().keys() or params[_key].shape != self.state_dict()[_key].shape: 
            continue
        self.state_dict()[_key].copy_(params[_key])</code></pre>
</details>
</dd>
<dt id="GLAMOR.models.abstracts.ReidModel.weights_init_kaiming"><code class="name flex">
<span>def <span class="ident">weights_init_kaiming</span></span>(<span>self, m)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def weights_init_kaiming(self,m):
    classname = m.__class__.__name__
    if classname.find(&#39;Linear&#39;) != -1:
            nn.init.kaiming_normal_(m.weight, a=0, mode=&#39;fan_out&#39;)
            nn.init.constant_(m.bias, 0.0)
    elif classname.find(&#39;Conv&#39;) != -1:
            nn.init.kaiming_normal_(m.weight, a=0, mode=&#39;fan_in&#39;)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0.0)
    elif classname.find(&#39;BatchNorm&#39;) != -1:
            if m.affine:
                nn.init.constant_(m.weight, 1.0)
                nn.init.constant_(m.bias, 0.0)
    elif classname.find(&#39;GroupNorm&#39;) != -1:
            if m.affine:
                nn.init.constant_(m.weight, 1.0)
                nn.init.constant_(m.bias, 0.0)
    elif classname.find(&#39;LayerNorm&#39;) != -1:
            #if m.affine:
            #    nn.init.constant_(m.weight, 1.0)
            #    nn.init.constant_(m.bias, 0.0)
            pass
    elif classname.find(&#39;InstanceNorm&#39;) != -1:
            if m.affine:
                nn.init.constant_(m.weight, 1.0)
                nn.init.constant_(m.bias, 0.0)</code></pre>
</details>
</dd>
<dt id="GLAMOR.models.abstracts.ReidModel.weights_init_softmax"><code class="name flex">
<span>def <span class="ident">weights_init_softmax</span></span>(<span>self, m)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize linear weights to standard normal. Mean 0. Standard Deviation 0.001</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def weights_init_softmax(self, m):
    &#34;&#34;&#34; Initialize linear weights to standard normal. Mean 0. Standard Deviation 0.001 &#34;&#34;&#34;
    classname = m.__class__.__name__
    if classname.find(&#39;Linear&#39;) != -1:
            nn.init.normal_(m.weight, std=0.001)
            if m.bias:
                    nn.init.constant_(m.bias, 0.0)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="GLAMOR.models" href="index.html">GLAMOR.models</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract" href="#GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract">CoLabelInterpretableResnetAbstract</a></code></h4>
<ul class="">
<li><code><a title="GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract.base_forward" href="#GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract.base_forward">base_forward</a></code></li>
<li><code><a title="GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract.build_base" href="#GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract.build_base">build_base</a></code></li>
<li><code><a title="GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract.build_normalization" href="#GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract.build_normalization">build_normalization</a></code></li>
<li><code><a title="GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract.build_softmax" href="#GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract.build_softmax">build_softmax</a></code></li>
<li><code><a title="GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract.dump_patches" href="#GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract.dump_patches">dump_patches</a></code></li>
<li><code><a title="GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract.forward" href="#GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract.forward">forward</a></code></li>
<li><code><a title="GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract.partial_load" href="#GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract.partial_load">partial_load</a></code></li>
<li><code><a title="GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract.training" href="#GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract.training">training</a></code></li>
<li><code><a title="GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract.weights_init_kaiming" href="#GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract.weights_init_kaiming">weights_init_kaiming</a></code></li>
<li><code><a title="GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract.weights_init_softmax" href="#GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract.weights_init_softmax">weights_init_softmax</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="GLAMOR.models.abstracts.CoLabelResnetAbstract" href="#GLAMOR.models.abstracts.CoLabelResnetAbstract">CoLabelResnetAbstract</a></code></h4>
<ul class="">
<li><code><a title="GLAMOR.models.abstracts.CoLabelResnetAbstract.base_forward" href="#GLAMOR.models.abstracts.CoLabelResnetAbstract.base_forward">base_forward</a></code></li>
<li><code><a title="GLAMOR.models.abstracts.CoLabelResnetAbstract.build_base" href="#GLAMOR.models.abstracts.CoLabelResnetAbstract.build_base">build_base</a></code></li>
<li><code><a title="GLAMOR.models.abstracts.CoLabelResnetAbstract.build_normalization" href="#GLAMOR.models.abstracts.CoLabelResnetAbstract.build_normalization">build_normalization</a></code></li>
<li><code><a title="GLAMOR.models.abstracts.CoLabelResnetAbstract.dump_patches" href="#GLAMOR.models.abstracts.CoLabelResnetAbstract.dump_patches">dump_patches</a></code></li>
<li><code><a title="GLAMOR.models.abstracts.CoLabelResnetAbstract.forward" href="#GLAMOR.models.abstracts.CoLabelResnetAbstract.forward">forward</a></code></li>
<li><code><a title="GLAMOR.models.abstracts.CoLabelResnetAbstract.partial_load" href="#GLAMOR.models.abstracts.CoLabelResnetAbstract.partial_load">partial_load</a></code></li>
<li><code><a title="GLAMOR.models.abstracts.CoLabelResnetAbstract.training" href="#GLAMOR.models.abstracts.CoLabelResnetAbstract.training">training</a></code></li>
<li><code><a title="GLAMOR.models.abstracts.CoLabelResnetAbstract.weights_init_kaiming" href="#GLAMOR.models.abstracts.CoLabelResnetAbstract.weights_init_kaiming">weights_init_kaiming</a></code></li>
<li><code><a title="GLAMOR.models.abstracts.CoLabelResnetAbstract.weights_init_softmax" href="#GLAMOR.models.abstracts.CoLabelResnetAbstract.weights_init_softmax">weights_init_softmax</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="GLAMOR.models.abstracts.ReidModel" href="#GLAMOR.models.abstracts.ReidModel">ReidModel</a></code></h4>
<ul class="">
<li><code><a title="GLAMOR.models.abstracts.ReidModel.base_forward" href="#GLAMOR.models.abstracts.ReidModel.base_forward">base_forward</a></code></li>
<li><code><a title="GLAMOR.models.abstracts.ReidModel.build_base" href="#GLAMOR.models.abstracts.ReidModel.build_base">build_base</a></code></li>
<li><code><a title="GLAMOR.models.abstracts.ReidModel.build_normalization" href="#GLAMOR.models.abstracts.ReidModel.build_normalization">build_normalization</a></code></li>
<li><code><a title="GLAMOR.models.abstracts.ReidModel.dump_patches" href="#GLAMOR.models.abstracts.ReidModel.dump_patches">dump_patches</a></code></li>
<li><code><a title="GLAMOR.models.abstracts.ReidModel.forward" href="#GLAMOR.models.abstracts.ReidModel.forward">forward</a></code></li>
<li><code><a title="GLAMOR.models.abstracts.ReidModel.partial_load" href="#GLAMOR.models.abstracts.ReidModel.partial_load">partial_load</a></code></li>
<li><code><a title="GLAMOR.models.abstracts.ReidModel.training" href="#GLAMOR.models.abstracts.ReidModel.training">training</a></code></li>
<li><code><a title="GLAMOR.models.abstracts.ReidModel.weights_init_kaiming" href="#GLAMOR.models.abstracts.ReidModel.weights_init_kaiming">weights_init_kaiming</a></code></li>
<li><code><a title="GLAMOR.models.abstracts.ReidModel.weights_init_softmax" href="#GLAMOR.models.abstracts.ReidModel.weights_init_softmax">weights_init_softmax</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>