<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>GLAMOR.models.CoLabelInterpretableResNet API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>GLAMOR.models.CoLabelInterpretableResNet</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import pdb
from torch import nn
from .abstracts import CoLabelInterpretableResnetAbstract
from utils import layers
import torch

# CHANGELOG: secondary attention is List, not a single number

class CoLabelInterpretableResnet(CoLabelInterpretableResnetAbstract):
    &#34;&#34;&#34;Basic CoLabel Resnet model.

    A CoLabel model is a base ResNet, but during prediction, employs additional pieces such as 
    an ensemble voter, a heuristic based on the prediction output probabilities, as well as (if desired), 
    holistic nested side inputs.

    Args: (TODO)
        base (str): The architecture base for resnet, i.e. resnet50, resnet18
        weights (str, None): Path to weights file for the architecture base ONLY. If not provided, base initialized with random values.
        normalization (str, None): Can be None, where it is torch&#39;s normalization. Else create a normalization layer. Supports: [&#34;bn&#34;, &#34;l2&#34;, &#34;in&#34;, &#34;gn&#34;, &#34;ln&#34;]
        embedding_dimensions (int): Dimensions for the feature embedding. Leave empty if feature dimensions should be same as architecture core output (e.g. resnet50 base model has 2048-dim feature outputs). If providing a value, it should be less than the architecture core&#39;s base feature dimensions.
        soft_dimensions (int, None): Whether to include softmax classification layer. If None, softmax layer is not created in model.

    Kwargs (MODEL_KWARGS):
        last_stride (int, 1): The final stride parameter for the architecture core. Should be one of 1 or 2.
        attention (str, None): The attention module to use. Only supports [&#39;cbam&#39;, None]
        input_attention (bool, false): Whether to include the IA module
        secondary_attention (List[int], None): Whether to modify CBAM to apply it to specific Resnet basic blocks. None means CBAM is applied to all. Otherwise, CBAM is applied only to the basic blocks provided here in List.
        branches (int): How many complementary feature branches for this inteprretable model

    Default Kwargs (DO NOT CHANGE OR ADD TO MODEL_KWARGS):
        zero_init_residual (bool, false): Whether the final layer uses zero initialization
        top_only (bool, true): Whether to keep only the architecture base without imagenet fully-connected layers (1000 classes)
        num_classes (int, 1000): Number of features in final imagenet FC layer
        groups (int, 1): Used during resnet variants construction
        width_per_group (int, 64): Used during resnet variants construction
        replace_stride_with_dilation (bool, None): Well, replace stride with dilation...
        norm_layer (nn.Module, None): The normalization layer within resnet. Internally defaults to nn.BatchNorm2D

    Methods: 
        forward: Process a batch

    &#34;&#34;&#34;

    def __init__(self, base = &#39;interpretable_resnet50&#39;, weights=None, normalization=None, embedding_dimensions=None, soft_dimensions = None, **kwargs):
        super(CoLabelInterpretableResnet, self).__init__(base, weights, normalization, embedding_dimensions, soft_dimensions=soft_dimensions, **kwargs)

    def build_base(self,base, weights, **kwargs):
        &#34;&#34;&#34;Build the model base.

        Builds the architecture base/core.
        &#34;&#34;&#34;
        _resnet = __import__(&#34;backbones.interpretableresnet&#34;, fromlist=[&#34;interpretableresnet&#34;])
        _resnet = getattr(_resnet, base)
        # Set up the resnet backbone
        self.base = _resnet(last_stride=1, **kwargs)
        if weights is not None:
            self.base.load_param(weights)
        
        # TODO add branches here...
        for branch_idx in self.branches:
            self.branch_layers[branch_idx][&#34;gap&#34;] = nn.AdaptiveAvgPool2d(1)
            #self.branch_layers[branch_idx][&#34;fc&#34;] = nn.Linear(512 * self.base.block.expansion, self.branch_classes[branch_idx])
                
        self.gap = nn.AdaptiveAvgPool2d(1)

        self.emb_linear = torch.nn.Identity()
        # Refactor this later, but basically, we don&#39;t need a linear embedding layer to convert from featurs to features. We will go directly from features to softmax...
        if self.embedding_dimensions is None:
            self.embedding_dimensions = 512*self.base.block.expansion
        if self.embedding_dimensions &gt; 512 * self.base.block.expansion:
            raise Warning(&#34;You are trying to scale up embedding dimensions from %i to %i. Try using same or less dimensions.&#34;%(512*self.base.block.expansion, self.embedding_dimensions))
        elif self.embedding_dimensions == 512*self.base.block.expansion:
            pass
        else:
            raise Warning(&#34;You are trying to scale down embedding dimensions from %i to %i. Try using same or less dimensions.&#34;%(512*self.base.block.expansion, self.embedding_dimensions))
            #self.emb_linear = nn.Linear(self.base.block.expansion*512, self.embedding_dimensions, bias=False)

    def build_normalization(self, normalization):
        norm_func=nn.Module
        norm_args={}
        norm_div=1
        if self.normalization == &#39;bn&#39;:
            norm_func = nn.BatchNorm1d
            norm_args={&#34;affine&#34;:True}
        elif self.normalization == &#34;in&#34;:
            norm_func = layers.FixedInstanceNorm1d
            norm_args={&#34;affine&#34;:True}
        elif self.normalization == &#34;gn&#34;:
            norm_div=16
            norm_func = nn.GroupNorm
            norm_args={&#34;num_channels&#34;:self.embedding_dimensions, &#34;affine&#34;:True}
        elif self.normalization == &#34;ln&#34;:
            norm_func = nn.LayerNorm
            norm_args={&#34;elementwise_affine&#34;:True}
        elif self.normalization == &#39;l2&#39;:
            norm_func = layers.L2Norm
            norm_args={&#34;scale&#34;:1.0}            
        elif self.normalization is None or self.normalization == &#39;&#39;:
            norm_func = layers.L2Norm
            norm_args={&#34;scale&#34;:1.0}
        else:
            raise NotImplementedError()

        # Not implemented should have been raised by now, so we don&#39;t need to worry about it here...
        self.feat_norm = norm_func(self.embedding_dimensions // norm_div, **norm_args)
        for branch_idx in self.branches:
            self.branch_layers[branch_idx][&#34;feat_norm&#34;] = norm_func(self.embedding_dimensions//norm_div, **norm_args)


        if self.normalization == &#39;l2&#39; or self.normalization == &#39;&#39; or self.normalization is None:
            pass
        elif self.normalization == &#39;bn&#39; or self.normalization == &#39;gn&#39; or self.normalization == &#39;ln&#39; or self.normalization == &#39;in&#39;:
            self.feat_norm.bias.requires_grad_(False)
            self.feat_norm.apply(self.weights_init_kaiming)
            for branch_idx in self.branches:
                self.branch_layers[branch_idx][&#34;feat_norm&#34;].bias.requires_grad_(False)
                self.branch_layers[branch_idx][&#34;feat_norm&#34;].apply(self.weights_init_kaiming)
        else:
            raise NotImplementedError()

    # here, we are updating build_softmax so that we can controlt the different branch outputs as well...
    def build_softmax(self):
        for branch_idx in self.branches:
            self.branch_layers[branch_idx][&#34;softmax&#34;] = nn.Linear(512 * self.base.block.expansion, self.branch_classes[branch_idx])
            self.branch_layers[branch_idx][&#34;softmax&#34;].apply(self.weights_init_softmax)

        if self.soft_dimensions is not None:
            self.softmax = nn.Linear(self.embedding_dimensions, self.soft_dimensions, bias=False)
            self.softmax.apply(self.weights_init_softmax)
        else:
            raise Warning(&#34;soft_dimensions is None. This should be fixed, otherwise there will be no predictions from model&#34;)
            self.softmax = None

    def base_forward(self,x):
        
        concat_features, branch_features = self.base(x)
        
        concat_features = self.gap(concat_features)
        concat_features = concat_features.view(concat_features.shape[0],-1)
        concat_features = self.emb_linear(concat_features)  #identify, for now NOTE

        for branch_idx in self.branches:
            branch_features[branch_idx] = self.branch_layers[branch_idx][&#34;gap&#34;](branch_features[branch_idx])
            branch_features[branch_idx] = branch_features[branch_idx].view(branch_features[branch_idx].shape[0],-1)
            #branch_features[branch_idx] = nn.Identity()...

        return concat_features, branch_features

    def forward(self,x):
        concat_features, branch_features = self.base_forward(x)
        
        #if self.feat_norm is not None: &lt;-- no need, identity
        concat_inference = self.feat_norm(concat_features)
        for branch_idx in self.branches:
            branch_features[branch_idx] = self.branch_layers[branch_idx][&#34;feat_norm&#34;](branch_features[branch_idx])

        soft_logits = None
        branch_logits = [None]*self.branches
        
        soft_logits = self.softmax(concat_inference)
        for branch_idx in self.branches:
            branch_logits[branch_idx] = self.branch_layers[branch_idx][&#34;softmax&#34;](branch_features[branch_idx])
        #if self.softmax:
        #    soft_logits = self.softmax(inference)
        #return soft_logits, inference   # soft logits are the softmax logits we will use to for training. We can use inference to store the historical probability????
        return soft_logits, branch_logits</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="GLAMOR.models.CoLabelInterpretableResNet.CoLabelInterpretableResnet"><code class="flex name class">
<span>class <span class="ident">CoLabelInterpretableResnet</span></span>
<span>(</span><span>base='interpretable_resnet50', weights=None, normalization=None, embedding_dimensions=None, soft_dimensions=None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Basic CoLabel Resnet model.</p>
<p>A CoLabel model is a base ResNet, but during prediction, employs additional pieces such as
an ensemble voter, a heuristic based on the prediction output probabilities, as well as (if desired),
holistic nested side inputs.</p>
<p>Args: (TODO)
base (str): The architecture base for resnet, i.e. resnet50, resnet18
weights (str, None): Path to weights file for the architecture base ONLY. If not provided, base initialized with random values.
normalization (str, None): Can be None, where it is torch's normalization. Else create a normalization layer. Supports: ["bn", "l2", "in", "gn", "ln"]
embedding_dimensions (int): Dimensions for the feature embedding. Leave empty if feature dimensions should be same as architecture core output (e.g. resnet50 base model has 2048-dim feature outputs). If providing a value, it should be less than the architecture core's base feature dimensions.
soft_dimensions (int, None): Whether to include softmax classification layer. If None, softmax layer is not created in model.</p>
<p>Kwargs (MODEL_KWARGS):
last_stride (int, 1): The final stride parameter for the architecture core. Should be one of 1 or 2.
attention (str, None): The attention module to use. Only supports ['cbam', None]
input_attention (bool, false): Whether to include the IA module
secondary_attention (List[int], None): Whether to modify CBAM to apply it to specific Resnet basic blocks. None means CBAM is applied to all. Otherwise, CBAM is applied only to the basic blocks provided here in List.
branches (int): How many complementary feature branches for this inteprretable model</p>
<p>Default Kwargs (DO NOT CHANGE OR ADD TO MODEL_KWARGS):
zero_init_residual (bool, false): Whether the final layer uses zero initialization
top_only (bool, true): Whether to keep only the architecture base without imagenet fully-connected layers (1000 classes)
num_classes (int, 1000): Number of features in final imagenet FC layer
groups (int, 1): Used during resnet variants construction
width_per_group (int, 64): Used during resnet variants construction
replace_stride_with_dilation (bool, None): Well, replace stride with dilation&hellip;
norm_layer (nn.Module, None): The normalization layer within resnet. Internally defaults to nn.BatchNorm2D</p>
<p>Methods:
forward: Process a batch</p>
<p>Basic CoLabel Interpretable Resnet model.</p>
<p>A CoLabel model performs classification using corroboratively labeled data to generate labels for unlabeed data.</p>
<p>assuming branches=3; we generate a resnet model with 3 branches, each with their feature output.</p>
<p>These feature outputs are fed to a softmax, plus concatenated for final outputs.</p>
<p>kwargs.branches determines how many complementary feature branches are in this colabel model</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>base</code></strong> :&ensp;<code>str</code></dt>
<dd>The architecture base for resnet, i.e. resnet50, resnet18</dd>
<dt><strong><code>weights</code></strong> :&ensp;<code>str, None</code></dt>
<dd>Path to weights file for the architecture base ONLY. If not provided, base initialized with random values.</dd>
<dt><strong><code>normalization</code></strong> :&ensp;<code>str, None</code></dt>
<dd>Can be None, where no normalization is used. Else create a normalization layer. Supports: ["bn", "l2", "in", "gn", "ln"]</dd>
<dt><strong><code>embedding_dimensions</code></strong> :&ensp;<code>int</code></dt>
<dd>Dimensions for the feature embedding. Leave empty if feature dimensions should be same as architecture core output (e.g. resnet50 base model has 2048-dim feature outputs). If providing a value, it should be less than the architecture core's base feature dimensions.</dd>
<dt><strong><code>soft_dimensions</code></strong> :&ensp;<code>int, None</code></dt>
<dd>Whether to include softmax classification layer. If None, softmax layer is not created in model.</dd>
</dl>
<p>Kwargs (MODEL_KWARGS):
last_stride (int, 1): The final stride parameter for the architecture core. Should be one of 1 or 2.
attention (str, None): The attention module to use. Only supports ['cbam', None]
input_attention (bool, false): Whether to include the IA module
ia_attention (bool, false): Whether to include input IA module
part_attention (bool, false): Whether to include Part-CBAM Mobule
secondary_attention (List[int], None): Whether to modify CBAM to apply it to specific Resnet basic blocks. None means CBAM is applied to all. Otherwise, CBAM is applied only to the basic block number provided here.</p>
<p>Default Kwargs (DO NOT CHANGE OR ADD TO MODEL_KWARGS; set in backbones.resnet):
zero_init_residual (bool, false): Whether the final layer uses zero initialization
top_only (bool, true): Whether to keep only the architecture base without imagenet fully-connected layers (1000 classes)
num_classes (int, 1000): Number of features in final imagenet FC layer
groups (int, 1): Used during resnet variants construction
width_per_group (int, 64): Used during resnet variants construction
replace_stride_with_dilation (bool, None): Well, replace stride with dilation&hellip;
norm_layer (nn.Module, None): The normalization layer within resnet. Internally defaults to nn.BatchNorm2D</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CoLabelInterpretableResnet(CoLabelInterpretableResnetAbstract):
    &#34;&#34;&#34;Basic CoLabel Resnet model.

    A CoLabel model is a base ResNet, but during prediction, employs additional pieces such as 
    an ensemble voter, a heuristic based on the prediction output probabilities, as well as (if desired), 
    holistic nested side inputs.

    Args: (TODO)
        base (str): The architecture base for resnet, i.e. resnet50, resnet18
        weights (str, None): Path to weights file for the architecture base ONLY. If not provided, base initialized with random values.
        normalization (str, None): Can be None, where it is torch&#39;s normalization. Else create a normalization layer. Supports: [&#34;bn&#34;, &#34;l2&#34;, &#34;in&#34;, &#34;gn&#34;, &#34;ln&#34;]
        embedding_dimensions (int): Dimensions for the feature embedding. Leave empty if feature dimensions should be same as architecture core output (e.g. resnet50 base model has 2048-dim feature outputs). If providing a value, it should be less than the architecture core&#39;s base feature dimensions.
        soft_dimensions (int, None): Whether to include softmax classification layer. If None, softmax layer is not created in model.

    Kwargs (MODEL_KWARGS):
        last_stride (int, 1): The final stride parameter for the architecture core. Should be one of 1 or 2.
        attention (str, None): The attention module to use. Only supports [&#39;cbam&#39;, None]
        input_attention (bool, false): Whether to include the IA module
        secondary_attention (List[int], None): Whether to modify CBAM to apply it to specific Resnet basic blocks. None means CBAM is applied to all. Otherwise, CBAM is applied only to the basic blocks provided here in List.
        branches (int): How many complementary feature branches for this inteprretable model

    Default Kwargs (DO NOT CHANGE OR ADD TO MODEL_KWARGS):
        zero_init_residual (bool, false): Whether the final layer uses zero initialization
        top_only (bool, true): Whether to keep only the architecture base without imagenet fully-connected layers (1000 classes)
        num_classes (int, 1000): Number of features in final imagenet FC layer
        groups (int, 1): Used during resnet variants construction
        width_per_group (int, 64): Used during resnet variants construction
        replace_stride_with_dilation (bool, None): Well, replace stride with dilation...
        norm_layer (nn.Module, None): The normalization layer within resnet. Internally defaults to nn.BatchNorm2D

    Methods: 
        forward: Process a batch

    &#34;&#34;&#34;

    def __init__(self, base = &#39;interpretable_resnet50&#39;, weights=None, normalization=None, embedding_dimensions=None, soft_dimensions = None, **kwargs):
        super(CoLabelInterpretableResnet, self).__init__(base, weights, normalization, embedding_dimensions, soft_dimensions=soft_dimensions, **kwargs)

    def build_base(self,base, weights, **kwargs):
        &#34;&#34;&#34;Build the model base.

        Builds the architecture base/core.
        &#34;&#34;&#34;
        _resnet = __import__(&#34;backbones.interpretableresnet&#34;, fromlist=[&#34;interpretableresnet&#34;])
        _resnet = getattr(_resnet, base)
        # Set up the resnet backbone
        self.base = _resnet(last_stride=1, **kwargs)
        if weights is not None:
            self.base.load_param(weights)
        
        # TODO add branches here...
        for branch_idx in self.branches:
            self.branch_layers[branch_idx][&#34;gap&#34;] = nn.AdaptiveAvgPool2d(1)
            #self.branch_layers[branch_idx][&#34;fc&#34;] = nn.Linear(512 * self.base.block.expansion, self.branch_classes[branch_idx])
                
        self.gap = nn.AdaptiveAvgPool2d(1)

        self.emb_linear = torch.nn.Identity()
        # Refactor this later, but basically, we don&#39;t need a linear embedding layer to convert from featurs to features. We will go directly from features to softmax...
        if self.embedding_dimensions is None:
            self.embedding_dimensions = 512*self.base.block.expansion
        if self.embedding_dimensions &gt; 512 * self.base.block.expansion:
            raise Warning(&#34;You are trying to scale up embedding dimensions from %i to %i. Try using same or less dimensions.&#34;%(512*self.base.block.expansion, self.embedding_dimensions))
        elif self.embedding_dimensions == 512*self.base.block.expansion:
            pass
        else:
            raise Warning(&#34;You are trying to scale down embedding dimensions from %i to %i. Try using same or less dimensions.&#34;%(512*self.base.block.expansion, self.embedding_dimensions))
            #self.emb_linear = nn.Linear(self.base.block.expansion*512, self.embedding_dimensions, bias=False)

    def build_normalization(self, normalization):
        norm_func=nn.Module
        norm_args={}
        norm_div=1
        if self.normalization == &#39;bn&#39;:
            norm_func = nn.BatchNorm1d
            norm_args={&#34;affine&#34;:True}
        elif self.normalization == &#34;in&#34;:
            norm_func = layers.FixedInstanceNorm1d
            norm_args={&#34;affine&#34;:True}
        elif self.normalization == &#34;gn&#34;:
            norm_div=16
            norm_func = nn.GroupNorm
            norm_args={&#34;num_channels&#34;:self.embedding_dimensions, &#34;affine&#34;:True}
        elif self.normalization == &#34;ln&#34;:
            norm_func = nn.LayerNorm
            norm_args={&#34;elementwise_affine&#34;:True}
        elif self.normalization == &#39;l2&#39;:
            norm_func = layers.L2Norm
            norm_args={&#34;scale&#34;:1.0}            
        elif self.normalization is None or self.normalization == &#39;&#39;:
            norm_func = layers.L2Norm
            norm_args={&#34;scale&#34;:1.0}
        else:
            raise NotImplementedError()

        # Not implemented should have been raised by now, so we don&#39;t need to worry about it here...
        self.feat_norm = norm_func(self.embedding_dimensions // norm_div, **norm_args)
        for branch_idx in self.branches:
            self.branch_layers[branch_idx][&#34;feat_norm&#34;] = norm_func(self.embedding_dimensions//norm_div, **norm_args)


        if self.normalization == &#39;l2&#39; or self.normalization == &#39;&#39; or self.normalization is None:
            pass
        elif self.normalization == &#39;bn&#39; or self.normalization == &#39;gn&#39; or self.normalization == &#39;ln&#39; or self.normalization == &#39;in&#39;:
            self.feat_norm.bias.requires_grad_(False)
            self.feat_norm.apply(self.weights_init_kaiming)
            for branch_idx in self.branches:
                self.branch_layers[branch_idx][&#34;feat_norm&#34;].bias.requires_grad_(False)
                self.branch_layers[branch_idx][&#34;feat_norm&#34;].apply(self.weights_init_kaiming)
        else:
            raise NotImplementedError()

    # here, we are updating build_softmax so that we can controlt the different branch outputs as well...
    def build_softmax(self):
        for branch_idx in self.branches:
            self.branch_layers[branch_idx][&#34;softmax&#34;] = nn.Linear(512 * self.base.block.expansion, self.branch_classes[branch_idx])
            self.branch_layers[branch_idx][&#34;softmax&#34;].apply(self.weights_init_softmax)

        if self.soft_dimensions is not None:
            self.softmax = nn.Linear(self.embedding_dimensions, self.soft_dimensions, bias=False)
            self.softmax.apply(self.weights_init_softmax)
        else:
            raise Warning(&#34;soft_dimensions is None. This should be fixed, otherwise there will be no predictions from model&#34;)
            self.softmax = None

    def base_forward(self,x):
        
        concat_features, branch_features = self.base(x)
        
        concat_features = self.gap(concat_features)
        concat_features = concat_features.view(concat_features.shape[0],-1)
        concat_features = self.emb_linear(concat_features)  #identify, for now NOTE

        for branch_idx in self.branches:
            branch_features[branch_idx] = self.branch_layers[branch_idx][&#34;gap&#34;](branch_features[branch_idx])
            branch_features[branch_idx] = branch_features[branch_idx].view(branch_features[branch_idx].shape[0],-1)
            #branch_features[branch_idx] = nn.Identity()...

        return concat_features, branch_features

    def forward(self,x):
        concat_features, branch_features = self.base_forward(x)
        
        #if self.feat_norm is not None: &lt;-- no need, identity
        concat_inference = self.feat_norm(concat_features)
        for branch_idx in self.branches:
            branch_features[branch_idx] = self.branch_layers[branch_idx][&#34;feat_norm&#34;](branch_features[branch_idx])

        soft_logits = None
        branch_logits = [None]*self.branches
        
        soft_logits = self.softmax(concat_inference)
        for branch_idx in self.branches:
            branch_logits[branch_idx] = self.branch_layers[branch_idx][&#34;softmax&#34;](branch_features[branch_idx])
        #if self.softmax:
        #    soft_logits = self.softmax(inference)
        #return soft_logits, inference   # soft logits are the softmax logits we will use to for training. We can use inference to store the historical probability????
        return soft_logits, branch_logits</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract" href="abstracts.html#GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract">CoLabelInterpretableResnetAbstract</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="GLAMOR.models.CoLabelInterpretableResNet.CoLabelInterpretableResnet.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="GLAMOR.models.CoLabelInterpretableResNet.CoLabelInterpretableResnet.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="GLAMOR.models.CoLabelInterpretableResNet.CoLabelInterpretableResnet.base_forward"><code class="name flex">
<span>def <span class="ident">base_forward</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def base_forward(self,x):
    
    concat_features, branch_features = self.base(x)
    
    concat_features = self.gap(concat_features)
    concat_features = concat_features.view(concat_features.shape[0],-1)
    concat_features = self.emb_linear(concat_features)  #identify, for now NOTE

    for branch_idx in self.branches:
        branch_features[branch_idx] = self.branch_layers[branch_idx][&#34;gap&#34;](branch_features[branch_idx])
        branch_features[branch_idx] = branch_features[branch_idx].view(branch_features[branch_idx].shape[0],-1)
        #branch_features[branch_idx] = nn.Identity()...

    return concat_features, branch_features</code></pre>
</details>
</dd>
<dt id="GLAMOR.models.CoLabelInterpretableResNet.CoLabelInterpretableResnet.build_base"><code class="name flex">
<span>def <span class="ident">build_base</span></span>(<span>self, base, weights, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Build the model base.</p>
<p>Builds the architecture base/core.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_base(self,base, weights, **kwargs):
    &#34;&#34;&#34;Build the model base.

    Builds the architecture base/core.
    &#34;&#34;&#34;
    _resnet = __import__(&#34;backbones.interpretableresnet&#34;, fromlist=[&#34;interpretableresnet&#34;])
    _resnet = getattr(_resnet, base)
    # Set up the resnet backbone
    self.base = _resnet(last_stride=1, **kwargs)
    if weights is not None:
        self.base.load_param(weights)
    
    # TODO add branches here...
    for branch_idx in self.branches:
        self.branch_layers[branch_idx][&#34;gap&#34;] = nn.AdaptiveAvgPool2d(1)
        #self.branch_layers[branch_idx][&#34;fc&#34;] = nn.Linear(512 * self.base.block.expansion, self.branch_classes[branch_idx])
            
    self.gap = nn.AdaptiveAvgPool2d(1)

    self.emb_linear = torch.nn.Identity()
    # Refactor this later, but basically, we don&#39;t need a linear embedding layer to convert from featurs to features. We will go directly from features to softmax...
    if self.embedding_dimensions is None:
        self.embedding_dimensions = 512*self.base.block.expansion
    if self.embedding_dimensions &gt; 512 * self.base.block.expansion:
        raise Warning(&#34;You are trying to scale up embedding dimensions from %i to %i. Try using same or less dimensions.&#34;%(512*self.base.block.expansion, self.embedding_dimensions))
    elif self.embedding_dimensions == 512*self.base.block.expansion:
        pass
    else:
        raise Warning(&#34;You are trying to scale down embedding dimensions from %i to %i. Try using same or less dimensions.&#34;%(512*self.base.block.expansion, self.embedding_dimensions))
        #self.emb_linear = nn.Linear(self.base.block.expansion*512, self.embedding_dimensions, bias=False)</code></pre>
</details>
</dd>
<dt id="GLAMOR.models.CoLabelInterpretableResNet.CoLabelInterpretableResnet.build_normalization"><code class="name flex">
<span>def <span class="ident">build_normalization</span></span>(<span>self, normalization)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_normalization(self, normalization):
    norm_func=nn.Module
    norm_args={}
    norm_div=1
    if self.normalization == &#39;bn&#39;:
        norm_func = nn.BatchNorm1d
        norm_args={&#34;affine&#34;:True}
    elif self.normalization == &#34;in&#34;:
        norm_func = layers.FixedInstanceNorm1d
        norm_args={&#34;affine&#34;:True}
    elif self.normalization == &#34;gn&#34;:
        norm_div=16
        norm_func = nn.GroupNorm
        norm_args={&#34;num_channels&#34;:self.embedding_dimensions, &#34;affine&#34;:True}
    elif self.normalization == &#34;ln&#34;:
        norm_func = nn.LayerNorm
        norm_args={&#34;elementwise_affine&#34;:True}
    elif self.normalization == &#39;l2&#39;:
        norm_func = layers.L2Norm
        norm_args={&#34;scale&#34;:1.0}            
    elif self.normalization is None or self.normalization == &#39;&#39;:
        norm_func = layers.L2Norm
        norm_args={&#34;scale&#34;:1.0}
    else:
        raise NotImplementedError()

    # Not implemented should have been raised by now, so we don&#39;t need to worry about it here...
    self.feat_norm = norm_func(self.embedding_dimensions // norm_div, **norm_args)
    for branch_idx in self.branches:
        self.branch_layers[branch_idx][&#34;feat_norm&#34;] = norm_func(self.embedding_dimensions//norm_div, **norm_args)


    if self.normalization == &#39;l2&#39; or self.normalization == &#39;&#39; or self.normalization is None:
        pass
    elif self.normalization == &#39;bn&#39; or self.normalization == &#39;gn&#39; or self.normalization == &#39;ln&#39; or self.normalization == &#39;in&#39;:
        self.feat_norm.bias.requires_grad_(False)
        self.feat_norm.apply(self.weights_init_kaiming)
        for branch_idx in self.branches:
            self.branch_layers[branch_idx][&#34;feat_norm&#34;].bias.requires_grad_(False)
            self.branch_layers[branch_idx][&#34;feat_norm&#34;].apply(self.weights_init_kaiming)
    else:
        raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="GLAMOR.models.CoLabelInterpretableResNet.CoLabelInterpretableResnet.build_softmax"><code class="name flex">
<span>def <span class="ident">build_softmax</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_softmax(self):
    for branch_idx in self.branches:
        self.branch_layers[branch_idx][&#34;softmax&#34;] = nn.Linear(512 * self.base.block.expansion, self.branch_classes[branch_idx])
        self.branch_layers[branch_idx][&#34;softmax&#34;].apply(self.weights_init_softmax)

    if self.soft_dimensions is not None:
        self.softmax = nn.Linear(self.embedding_dimensions, self.soft_dimensions, bias=False)
        self.softmax.apply(self.weights_init_softmax)
    else:
        raise Warning(&#34;soft_dimensions is None. This should be fixed, otherwise there will be no predictions from model&#34;)
        self.softmax = None</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract" href="abstracts.html#GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract">CoLabelInterpretableResnetAbstract</a></b></code>:
<ul class="hlist">
<li><code><a title="GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract.forward" href="abstracts.html#GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract.forward">forward</a></code></li>
<li><code><a title="GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract.weights_init_softmax" href="abstracts.html#GLAMOR.models.abstracts.CoLabelInterpretableResnetAbstract.weights_init_softmax">weights_init_softmax</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="GLAMOR.models" href="index.html">GLAMOR.models</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="GLAMOR.models.CoLabelInterpretableResNet.CoLabelInterpretableResnet" href="#GLAMOR.models.CoLabelInterpretableResNet.CoLabelInterpretableResnet">CoLabelInterpretableResnet</a></code></h4>
<ul class="two-column">
<li><code><a title="GLAMOR.models.CoLabelInterpretableResNet.CoLabelInterpretableResnet.base_forward" href="#GLAMOR.models.CoLabelInterpretableResNet.CoLabelInterpretableResnet.base_forward">base_forward</a></code></li>
<li><code><a title="GLAMOR.models.CoLabelInterpretableResNet.CoLabelInterpretableResnet.build_base" href="#GLAMOR.models.CoLabelInterpretableResNet.CoLabelInterpretableResnet.build_base">build_base</a></code></li>
<li><code><a title="GLAMOR.models.CoLabelInterpretableResNet.CoLabelInterpretableResnet.build_normalization" href="#GLAMOR.models.CoLabelInterpretableResNet.CoLabelInterpretableResnet.build_normalization">build_normalization</a></code></li>
<li><code><a title="GLAMOR.models.CoLabelInterpretableResNet.CoLabelInterpretableResnet.build_softmax" href="#GLAMOR.models.CoLabelInterpretableResNet.CoLabelInterpretableResnet.build_softmax">build_softmax</a></code></li>
<li><code><a title="GLAMOR.models.CoLabelInterpretableResNet.CoLabelInterpretableResnet.dump_patches" href="#GLAMOR.models.CoLabelInterpretableResNet.CoLabelInterpretableResnet.dump_patches">dump_patches</a></code></li>
<li><code><a title="GLAMOR.models.CoLabelInterpretableResNet.CoLabelInterpretableResnet.training" href="#GLAMOR.models.CoLabelInterpretableResNet.CoLabelInterpretableResnet.training">training</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>