<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>GLAMOR.backbones.resnet API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>GLAMOR.backbones.resnet</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from ctypes import Union
import warnings
from torch import nn
import torch
import pdb

class ChannelAttention(nn.Module):
    &#34;&#34;&#34;Channel Attention module that forms part of CBAM attention.
    See the ECCV paper at https://openaccess.thecvf.com/content_ECCV_2018/papers/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.pdf
    &#34;&#34;&#34;
    def __init__(self, in_planes, ratio=16):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)

        self.fc1   = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)
        self.relu1 = nn.ReLU()
        self.fc2   = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)

        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))
        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))
        out = avg_out + max_out
        return self.sigmoid(out)

class SpatialAttention(nn.Module):
    &#34;&#34;&#34;Spatial Attention module that forms part of CBAM attention.
    See the ECCV paper at https://openaccess.thecvf.com/content_ECCV_2018/papers/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.pdf
    &#34;&#34;&#34;
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()

        assert kernel_size in (3, 7), &#39;kernel size must be 3 or 7&#39;
        padding = 3 if kernel_size == 7 else 1

        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        x = torch.cat([avg_out, max_out], dim=1)
        x = self.conv1(x)
        return self.sigmoid(x)

class DenseAttention(nn.Module):    # Like spatial, but for all channels
    &#34;&#34;&#34;Dense attention module for global attention from GLAMOR; see arXiv paper
    at https://arxiv.org/pdf/2002.02256.pdf
    &#34;&#34;&#34;
    def __init__(self, planes):
        super(DenseAttention, self).__init__()
        self.dense_conv1=nn.Conv2d(planes,planes,kernel_size=3,padding=1,bias=False)
        self.dense_relu1=nn.LeakyReLU()
        self.dense_conv2=nn.Conv2d(planes,planes,kernel_size=3,padding=1,bias=False)
        self.dense_sigmoid = nn.Sigmoid()
    def forward(self,x):
        x = self.dense_conv1(x)
        x = self.dense_relu1(x)
        x = self.dense_conv2(x)
        x = self.dense_sigmoid(x)
        return x

class InputAttention(nn.Module):
    &#34;&#34;&#34;Input attention module for global/local attention from GLAMOR; see arXiv paper
    at https://arxiv.org/pdf/2002.02256.pdf
    &#34;&#34;&#34;
    def __init__(self, planes):
        super(InputAttention, self).__init__()
        self.ia_conv1=nn.Conv2d(planes,planes,kernel_size=3,padding=1,bias=False)
        self.ia_relu1=nn.LeakyReLU()
        self.ia_conv2=nn.Conv2d(planes,planes,kernel_size=3,padding=1,bias=False)
        self.ia_sigmoid = nn.Sigmoid()
    def forward(self,x):
        x = self.ia_conv1(x)
        x = self.ia_relu1(x)
        x = self.ia_conv2(x)
        x = self.ia_sigmoid(x)
        return x

class BasicBlock(nn.Module):
    &#34;&#34;&#34;ResNet component block for R18, R34. Useful when there are not too many layers
    to deal with complicated matrix multiplications.

    Raises:
        ValueError: When groups!=1 or base_width!=64
        NotImplementedError: When dilation&gt;1 or attention not cbam or dbam
    &#34;&#34;&#34;
    expansion = 1

    def __init__(self, inplanes: int, planes: int, stride: int=1, downsample=None, groups: int=1,
                 base_width: int =64, dilation: int=1, norm_layer=nn.BatchNorm2d, 
                 attention:str=None, input_attention:bool=False, part_attention:bool=False):
        &#34;&#34;&#34;Sets up the ResNet BasicBlock

        Args:
            inplanes (int): Depth of input
            planes (int): Depth of output
            stride (int, optional): Convolutional stride parameter. Defaults to 1.
            downsample (Union[int,None], optional): Whether to downsample images for skip connection. Defaults to None.
            groups (int, optional): &lt;&gt;. Defaults to 1.
            base_width (int, optional): &lt;&gt;. Defaults to 64.
            dilation (int, optional): Convolutional dilation parameters. Defaults to 1.
            norm_layer (Union[nn.GroupNorm,nn.modules.batchnorm._NormBase,None], optional): Normalization layer throughout Block. Defaults to nn.BatchNorm2d.
            attention (str, optional): Attention type: CBAM or DBAM. Defaults to None.
            input_attention (bool, optional): Whether to include the input attention module. Defaults to False.
            part_attention (bool, optional): Whether to include the part (local) attention module. Defaults to False.
        &#34;&#34;&#34;
        super(BasicBlock, self).__init__()
        # Verify some base parameters
        if groups != 1 or base_width != 64:
            raise ValueError(&#39;BasicBlock only supports groups=1 and base_width=64&#39;)
        if dilation &gt; 1:
            raise NotImplementedError(&#34;Dilation &gt; 1 not supported in BasicBlock&#34;)
        if attention is not None and attention not in [&#34;cbam&#34;,&#34;dbam&#34;]:
            raise ValueError(&#34;attention parameter is unsupported value %s. Use one of &#39;cbam&#39;,&#39;dbam&#39;.&#34;%attention)
        # Both self.conv1 and self.downsample layers downsample the input when stride != 1
        # This builds the core layers in the BasicBlock
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False, groups=1, dilation=1)
        self.bn1 = norm_layer(planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False, groups=1, dilation=1)
        self.bn2 = norm_layer(planes)       
        
        # Sets up input attention
        self.input_attention = None
        if input_attention:
            self.input_attention = InputAttention(planes)
        
        # Sets up CBAM or DBAM attention
        self.ca = None
        self.sa = None
        if attention == &#39;cbam&#39;:
            self.ca = ChannelAttention(planes)
            self.sa = SpatialAttention(kernel_size=3)
        if attention == &#39;dbam&#39;:
            self.ca = ChannelAttention(planes)
            self.sa = DenseAttention(planes)
        
        # Sets up local attention
        self.p_ca = None
        self.p_sa = None
        if part_attention:
            self.p_sa = DenseAttention(planes=planes*self.expansion)
            self.p_ca = ChannelAttention(planes*self.expansion)
            
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        identity = x

        if self.input_attention is not None:
            x = self.input_attention(x) * x
        
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        if self.ca is not None:
            out = self.ca(out) * out
            out = self.sa(out) * out

        if self.downsample is not None:
            identity = self.downsample(x)

        p_out = out
        part_mask = None
        if self.p_ca is not None:   # Get part attention
            p_out = self.p_sa(p_out) * p_out
#            p_out = self.p_ca(p_out) * p_out
            p_out = self.relu(p_out)
            part_mask = self.p_ca(p_out)

        out = out + identity
        out = self.relu(out)

        if self.p_ca is not None:   # Concat part attention
            #out = torch.cat([p_out[:,p_out.shape[1]//2:,:,:],out[:,:p_out.shape[1]//2,:,:]],dim=1)
            out = (part_mask * p_out) + ((1-part_mask)*out)
        return out

class Bottleneck(nn.Module):
    expansion = 4

    def __init__(   self, inplanes, planes, stride=1, downsample=None, groups = 1, base_width = 64, dilation = 1, norm_layer=None, 
                    attention = None, input_attention=False, part_attention=False):
        super(Bottleneck, self).__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        width = int(planes * (base_width / 64.)) * groups
        
        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False,stride=1)
        self.bn1 = norm_layer(width)
        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride, padding=dilation, bias=False, groups=groups, dilation=dilation)
        self.bn2 = norm_layer(width)
        self.conv3 = nn.Conv2d(width, planes * self.expansion, kernel_size=1, bias=False, stride=1)
        self.bn3 = norm_layer(planes * self.expansion)
        self.relu = nn.ReLU(inplace=True)
        
        if input_attention:
            self.input_attention = InputAttention(planes)
        else:
            self.input_attention = None

        if attention is None:
            self.ca = None
            self.sa = None
        elif attention == &#39;cbam&#39;:
            self.sa = SpatialAttention(kernel_size=3)
            self.ca = ChannelAttention(planes*self.expansion)
        elif attention == &#39;dbam&#39;:
            self.ca = ChannelAttention(planes)
            self.sa = DenseAttention(planes)
        else:
            raise NotImplementedError()

        if part_attention:
            self.p_sa = DenseAttention(planes=planes*self.expansion)
            self.p_ca = ChannelAttention(planes*self.expansion)
        else:
            self.p_ca = None
            self.p_sa = None
        
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        identity = x

        if self.input_attention is not None:
            x = self.input_attention(x) * x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.ca is not None:
            out = self.ca(out) * out
            out = self.sa(out) * out

        if self.downsample is not None:
            identity = self.downsample(x)

        p_out = out
        part_mask = None
        if self.p_ca is not None:   # Get part attention
            p_out = self.p_sa(p_out) * p_out
#            p_out = self.p_ca(p_out) * p_out
            p_out = self.relu(p_out)
            part_mask = self.p_ca(p_out)
        
        out = out + identity
        out = self.relu(out)

        if self.p_ca is not None:   # Concat part attention
            #out = torch.cat([p_out[:,p_out.shape[1]//2:,:,:],out[:,:p_out.shape[1]//2,:,:]],dim=1)
            out = (part_mask * p_out) + ((1-part_mask)*out)
        return out

class ResNet(nn.Module):
    def __init__(self, block=Bottleneck, layers=[3, 4, 6, 3], last_stride=2, zero_init_residual=False, \
                    top_only=True, num_classes=1000, groups=1, width_per_group=64, replace_stride_with_dilation=None,norm_layer=None, 
                    attention=None, input_attention = None, secondary_attention=None, ia_attention = None, part_attention = None,
                    **kwargs):
        super().__init__()
        self.attention=attention
        self.input_attention=input_attention
        self.secondary_attention=secondary_attention
        self.block=block
        self.inplanes = 64
        if norm_layer is None:
            self._norm_layer = nn.BatchNorm2d
        #elif norm_layer == &#34;ln&#34;:
        #    self._norm_layer = nn.LayerNorm
        self.dilation = 1
        if replace_stride_with_dilation is None:
            replace_stride_with_dilation = [False, False, False]
        if len(replace_stride_with_dilation) != 3:
            raise ValueError(&#34;replace_stride_with_dilation should be `None` or a 3-element tuple. Got {}&#34;.format(replace_stride_with_dilation))
        self.groups = groups
        self.base_width = width_per_group

        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)
        #if norm_layer == &#34;gn&#34;:
        #    self.bn1 = nn.GroupNorm2d
        self.bn1 = nn.BatchNorm2d(self.inplanes)
        self.relu1 = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

        self.ia_attention = ia_attention
        self.part_attention = part_attention
        
        # Make sure ia and input_attention do not conflict
        if self.ia_attention is not None and self.input_attention is not None:
            raise ValueError(&#34;Cannot have both ia_attention and input_attention.&#34;)
        if self.part_attention is not None and (self.attention is not None and self.secondary_attention is None):
            raise ValueError(&#34;Cannot have part-attention with CBAM everywhere&#34;)
        if self.part_attention is not None and (self.attention is not None and self.secondary_attention==1):
            raise ValueError(&#34;Cannot have part-attention with CBAM-Early&#34;)

        # Create true IA
        if self.ia_attention:
            self.ia_attention = InputAttention(self.inplanes)   # 64, set above
        else:
            self.ia_attention = None

        att = self.attention
        if secondary_attention is not None and secondary_attention != 1: # leave alone if sec attention not set
            att = None
        self.layer1 = self._make_layer(self.block, 64, layers[0], attention = att, input_attention=self.input_attention, part_attention = self.part_attention)
        att = self.attention
        if secondary_attention is not None and secondary_attention != 2: # leave alone if sec attention not set
            att = None
        self.layer2 = self._make_layer(self.block, 128, layers[1], stride=2, attention = att, dilate=replace_stride_with_dilation[0])
        att = self.attention
        if secondary_attention is not None and secondary_attention != 3: # leave alone if sec attention not set
            att = None
        self.layer3 = self._make_layer(self.block, 256, layers[2], stride=2, attention = att, dilate=replace_stride_with_dilation[1])
        att = self.attention
        if secondary_attention is not None and secondary_attention != 4: # leave alone if sec attention not set
            att = None
        self.layer4 = self._make_layer(self.block, 512, layers[3], stride=last_stride, attention = att, dilate=replace_stride_with_dilation[2])
        
        self.top_only = top_only
        self.avgpool, self.fc = None, None

        if not self.top_only:
            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
            self.fc = nn.Linear(512 * block.expansion, num_classes)
    
    def _make_layer(self, block, planes, blocks, stride=1, dilate = False, attention = None, input_attention=False, ia_attention = False, part_attention = False):
        downsample = None
        previous_dilation = self.dilation
        if dilate:
            self.dilation *= stride
            stride = 1
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.inplanes, planes * block.expansion,
                            kernel_size=1, stride=stride, bias=False),
                self._norm_layer(planes * block.expansion),
            )
    
        layers = []
        layers.append(block(self.inplanes, planes, stride, downsample,groups = self.groups, base_width = self.base_width, dilation = previous_dilation, norm_layer=self._norm_layer, attention=attention, input_attention=input_attention, part_attention=part_attention))
        self.inplanes = planes * block.expansion
        for _ in range(1, blocks):
            layers.append(block(self.inplanes, planes, groups = self.groups, base_width = self.base_width, dilation = self.dilation, norm_layer=self._norm_layer, attention=attention))
        return nn.Sequential(*layers)
    
    def forward(self, x):
        x = self.conv1(x)
        
        if self.ia_attention is not None:
            x = self.ia_attention(x) * x
        x = self.bn1(x)
        x = self.relu1(x)
        x = self.maxpool(x)
    
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        if not self.top_only:
            x = self.avgpool(x)
            x = torch.flatten(x,1)
            x = self.fc(x)            
        return x
    
    def load_param(self, weights_path):
        param_dict = torch.load(weights_path)
        for i in param_dict:
            if &#39;fc&#39; in i and self.top_only:
                continue
            self.state_dict()[i].copy_(param_dict[i])
            
            
def _resnet(arch, block, layers, pretrained, progress, **kwargs):
    model = ResNet(block, layers, **kwargs)
    return model


def resnet18(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNet-18 model from
    `&#34;Deep Residual Learning for Image Recognition&#34; &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    return _resnet(&#39;resnet18&#39;, BasicBlock, [2, 2, 2, 2], pretrained, progress,
                   **kwargs)


def resnet34(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNet-34 model from
    `&#34;Deep Residual Learning for Image Recognition&#34; &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    return _resnet(&#39;resnet34&#39;, BasicBlock, [3, 4, 6, 3], pretrained, progress,
                   **kwargs)


def resnet50(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNet-50 model from
    `&#34;Deep Residual Learning for Image Recognition&#34; &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    return _resnet(&#39;resnet50&#39;, Bottleneck, [3, 4, 6, 3], pretrained, progress,
                   **kwargs)


def resnet101(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNet-101 model from
    `&#34;Deep Residual Learning for Image Recognition&#34; &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    return _resnet(&#39;resnet101&#39;, Bottleneck, [3, 4, 23, 3], pretrained, progress,
                   **kwargs)


def resnet152(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNet-152 model from
    `&#34;Deep Residual Learning for Image Recognition&#34; &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    return _resnet(&#39;resnet152&#39;, Bottleneck, [3, 8, 36, 3], pretrained, progress,
                   **kwargs)


def resnext50_32x4d(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNeXt-50 32x4d model from
    `&#34;Aggregated Residual Transformation for Deep Neural Networks&#34; &lt;https://arxiv.org/pdf/1611.05431.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    kwargs[&#39;groups&#39;] = 32
    kwargs[&#39;width_per_group&#39;] = 4
    return _resnet(&#39;resnext50_32x4d&#39;, Bottleneck, [3, 4, 6, 3],
                   pretrained, progress, **kwargs)


def resnext101_32x8d(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNeXt-101 32x8d model from
    `&#34;Aggregated Residual Transformation for Deep Neural Networks&#34; &lt;https://arxiv.org/pdf/1611.05431.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    kwargs[&#39;groups&#39;] = 32
    kwargs[&#39;width_per_group&#39;] = 8
    return _resnet(&#39;resnext101_32x8d&#39;, Bottleneck, [3, 4, 23, 3],
                   pretrained, progress, **kwargs)


def wide_resnet50_2(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;Wide ResNet-50-2 model from
    `&#34;Wide Residual Networks&#34; &lt;https://arxiv.org/pdf/1605.07146.pdf&gt;`_
    The model is the same as ResNet except for the bottleneck number of channels
    which is twice larger in every block. The number of channels in outer 1x1
    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048
    channels, and in Wide ResNet-50-2 has 2048-1024-2048.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    kwargs[&#39;width_per_group&#39;] = 64 * 2
    return _resnet(&#39;wide_resnet50_2&#39;, Bottleneck, [3, 4, 6, 3],
                   pretrained, progress, **kwargs)


def wide_resnet101_2(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;Wide ResNet-101-2 model from
    `&#34;Wide Residual Networks&#34; &lt;https://arxiv.org/pdf/1605.07146.pdf&gt;`_
    The model is the same as ResNet except for the bottleneck number of channels
    which is twice larger in every block. The number of channels in outer 1x1
    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048
    channels, and in Wide ResNet-50-2 has 2048-1024-2048.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    kwargs[&#39;width_per_group&#39;] = 64 * 2
    return _resnet(&#39;wide_resnet101_2&#39;, Bottleneck, [3, 4, 23, 3],
                   pretrained, progress, **kwargs)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="GLAMOR.backbones.resnet.resnet101"><code class="name flex">
<span>def <span class="ident">resnet101</span></span>(<span>pretrained=False, progress=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>ResNet-101 model from
<code>"Deep Residual Learning for Image Recognition" &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;</code>_</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pretrained</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, returns a model pre-trained on ImageNet</dd>
<dt><strong><code>progress</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, displays a progress bar of the download to stderr</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resnet101(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNet-101 model from
    `&#34;Deep Residual Learning for Image Recognition&#34; &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    return _resnet(&#39;resnet101&#39;, Bottleneck, [3, 4, 23, 3], pretrained, progress,
                   **kwargs)</code></pre>
</details>
</dd>
<dt id="GLAMOR.backbones.resnet.resnet152"><code class="name flex">
<span>def <span class="ident">resnet152</span></span>(<span>pretrained=False, progress=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>ResNet-152 model from
<code>"Deep Residual Learning for Image Recognition" &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;</code>_</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pretrained</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, returns a model pre-trained on ImageNet</dd>
<dt><strong><code>progress</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, displays a progress bar of the download to stderr</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resnet152(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNet-152 model from
    `&#34;Deep Residual Learning for Image Recognition&#34; &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    return _resnet(&#39;resnet152&#39;, Bottleneck, [3, 8, 36, 3], pretrained, progress,
                   **kwargs)</code></pre>
</details>
</dd>
<dt id="GLAMOR.backbones.resnet.resnet18"><code class="name flex">
<span>def <span class="ident">resnet18</span></span>(<span>pretrained=False, progress=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>ResNet-18 model from
<code>"Deep Residual Learning for Image Recognition" &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;</code>_</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pretrained</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, returns a model pre-trained on ImageNet</dd>
<dt><strong><code>progress</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, displays a progress bar of the download to stderr</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resnet18(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNet-18 model from
    `&#34;Deep Residual Learning for Image Recognition&#34; &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    return _resnet(&#39;resnet18&#39;, BasicBlock, [2, 2, 2, 2], pretrained, progress,
                   **kwargs)</code></pre>
</details>
</dd>
<dt id="GLAMOR.backbones.resnet.resnet34"><code class="name flex">
<span>def <span class="ident">resnet34</span></span>(<span>pretrained=False, progress=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>ResNet-34 model from
<code>"Deep Residual Learning for Image Recognition" &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;</code>_</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pretrained</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, returns a model pre-trained on ImageNet</dd>
<dt><strong><code>progress</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, displays a progress bar of the download to stderr</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resnet34(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNet-34 model from
    `&#34;Deep Residual Learning for Image Recognition&#34; &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    return _resnet(&#39;resnet34&#39;, BasicBlock, [3, 4, 6, 3], pretrained, progress,
                   **kwargs)</code></pre>
</details>
</dd>
<dt id="GLAMOR.backbones.resnet.resnet50"><code class="name flex">
<span>def <span class="ident">resnet50</span></span>(<span>pretrained=False, progress=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>ResNet-50 model from
<code>"Deep Residual Learning for Image Recognition" &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;</code>_</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pretrained</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, returns a model pre-trained on ImageNet</dd>
<dt><strong><code>progress</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, displays a progress bar of the download to stderr</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resnet50(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNet-50 model from
    `&#34;Deep Residual Learning for Image Recognition&#34; &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    return _resnet(&#39;resnet50&#39;, Bottleneck, [3, 4, 6, 3], pretrained, progress,
                   **kwargs)</code></pre>
</details>
</dd>
<dt id="GLAMOR.backbones.resnet.resnext101_32x8d"><code class="name flex">
<span>def <span class="ident">resnext101_32x8d</span></span>(<span>pretrained=False, progress=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>ResNeXt-101 32x8d model from
<code>"Aggregated Residual Transformation for Deep Neural Networks" &lt;https://arxiv.org/pdf/1611.05431.pdf&gt;</code>_</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pretrained</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, returns a model pre-trained on ImageNet</dd>
<dt><strong><code>progress</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, displays a progress bar of the download to stderr</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resnext101_32x8d(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNeXt-101 32x8d model from
    `&#34;Aggregated Residual Transformation for Deep Neural Networks&#34; &lt;https://arxiv.org/pdf/1611.05431.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    kwargs[&#39;groups&#39;] = 32
    kwargs[&#39;width_per_group&#39;] = 8
    return _resnet(&#39;resnext101_32x8d&#39;, Bottleneck, [3, 4, 23, 3],
                   pretrained, progress, **kwargs)</code></pre>
</details>
</dd>
<dt id="GLAMOR.backbones.resnet.resnext50_32x4d"><code class="name flex">
<span>def <span class="ident">resnext50_32x4d</span></span>(<span>pretrained=False, progress=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>ResNeXt-50 32x4d model from
<code>"Aggregated Residual Transformation for Deep Neural Networks" &lt;https://arxiv.org/pdf/1611.05431.pdf&gt;</code>_</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pretrained</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, returns a model pre-trained on ImageNet</dd>
<dt><strong><code>progress</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, displays a progress bar of the download to stderr</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def resnext50_32x4d(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNeXt-50 32x4d model from
    `&#34;Aggregated Residual Transformation for Deep Neural Networks&#34; &lt;https://arxiv.org/pdf/1611.05431.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    kwargs[&#39;groups&#39;] = 32
    kwargs[&#39;width_per_group&#39;] = 4
    return _resnet(&#39;resnext50_32x4d&#39;, Bottleneck, [3, 4, 6, 3],
                   pretrained, progress, **kwargs)</code></pre>
</details>
</dd>
<dt id="GLAMOR.backbones.resnet.wide_resnet101_2"><code class="name flex">
<span>def <span class="ident">wide_resnet101_2</span></span>(<span>pretrained=False, progress=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Wide ResNet-101-2 model from
<code>"Wide Residual Networks" &lt;https://arxiv.org/pdf/1605.07146.pdf&gt;</code>_
The model is the same as ResNet except for the bottleneck number of channels
which is twice larger in every block. The number of channels in outer 1x1
convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048
channels, and in Wide ResNet-50-2 has 2048-1024-2048.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pretrained</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, returns a model pre-trained on ImageNet</dd>
<dt><strong><code>progress</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, displays a progress bar of the download to stderr</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wide_resnet101_2(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;Wide ResNet-101-2 model from
    `&#34;Wide Residual Networks&#34; &lt;https://arxiv.org/pdf/1605.07146.pdf&gt;`_
    The model is the same as ResNet except for the bottleneck number of channels
    which is twice larger in every block. The number of channels in outer 1x1
    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048
    channels, and in Wide ResNet-50-2 has 2048-1024-2048.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    kwargs[&#39;width_per_group&#39;] = 64 * 2
    return _resnet(&#39;wide_resnet101_2&#39;, Bottleneck, [3, 4, 23, 3],
                   pretrained, progress, **kwargs)</code></pre>
</details>
</dd>
<dt id="GLAMOR.backbones.resnet.wide_resnet50_2"><code class="name flex">
<span>def <span class="ident">wide_resnet50_2</span></span>(<span>pretrained=False, progress=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Wide ResNet-50-2 model from
<code>"Wide Residual Networks" &lt;https://arxiv.org/pdf/1605.07146.pdf&gt;</code>_
The model is the same as ResNet except for the bottleneck number of channels
which is twice larger in every block. The number of channels in outer 1x1
convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048
channels, and in Wide ResNet-50-2 has 2048-1024-2048.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pretrained</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, returns a model pre-trained on ImageNet</dd>
<dt><strong><code>progress</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, displays a progress bar of the download to stderr</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wide_resnet50_2(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;Wide ResNet-50-2 model from
    `&#34;Wide Residual Networks&#34; &lt;https://arxiv.org/pdf/1605.07146.pdf&gt;`_
    The model is the same as ResNet except for the bottleneck number of channels
    which is twice larger in every block. The number of channels in outer 1x1
    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048
    channels, and in Wide ResNet-50-2 has 2048-1024-2048.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    kwargs[&#39;width_per_group&#39;] = 64 * 2
    return _resnet(&#39;wide_resnet50_2&#39;, Bottleneck, [3, 4, 6, 3],
                   pretrained, progress, **kwargs)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="GLAMOR.backbones.resnet.BasicBlock"><code class="flex name class">
<span>class <span class="ident">BasicBlock</span></span>
<span>(</span><span>inplanes: int, planes: int, stride: int = 1, downsample=None, groups: int = 1, base_width: int = 64, dilation: int = 1, norm_layer=torch.nn.modules.batchnorm.BatchNorm2d, attention: str = None, input_attention: bool = False, part_attention: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>ResNet component block for R18, R34. Useful when there are not too many layers
to deal with complicated matrix multiplications.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>When groups!=1 or base_width!=64</dd>
<dt><code>NotImplementedError</code></dt>
<dd>When dilation&gt;1 or attention not cbam or dbam</dd>
</dl>
<p>Sets up the ResNet BasicBlock</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>inplanes</code></strong> :&ensp;<code>int</code></dt>
<dd>Depth of input</dd>
<dt><strong><code>planes</code></strong> :&ensp;<code>int</code></dt>
<dd>Depth of output</dd>
<dt><strong><code>stride</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Convolutional stride parameter. Defaults to 1.</dd>
<dt><strong><code>downsample</code></strong> :&ensp;<code>Union[int,None]</code>, optional</dt>
<dd>Whether to downsample images for skip connection. Defaults to None.</dd>
<dt><strong><code>groups</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>&lt;&gt;. Defaults to 1.</dd>
<dt><strong><code>base_width</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>&lt;&gt;. Defaults to 64.</dd>
<dt><strong><code>dilation</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Convolutional dilation parameters. Defaults to 1.</dd>
<dt><strong><code>norm_layer</code></strong> :&ensp;<code>Union[nn.GroupNorm,nn.modules.batchnorm._NormBase,None]</code>, optional</dt>
<dd>Normalization layer throughout Block. Defaults to nn.BatchNorm2d.</dd>
<dt><strong><code>attention</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Attention type: CBAM or DBAM. Defaults to None.</dd>
<dt><strong><code>input_attention</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to include the input attention module. Defaults to False.</dd>
<dt><strong><code>part_attention</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to include the part (local) attention module. Defaults to False.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BasicBlock(nn.Module):
    &#34;&#34;&#34;ResNet component block for R18, R34. Useful when there are not too many layers
    to deal with complicated matrix multiplications.

    Raises:
        ValueError: When groups!=1 or base_width!=64
        NotImplementedError: When dilation&gt;1 or attention not cbam or dbam
    &#34;&#34;&#34;
    expansion = 1

    def __init__(self, inplanes: int, planes: int, stride: int=1, downsample=None, groups: int=1,
                 base_width: int =64, dilation: int=1, norm_layer=nn.BatchNorm2d, 
                 attention:str=None, input_attention:bool=False, part_attention:bool=False):
        &#34;&#34;&#34;Sets up the ResNet BasicBlock

        Args:
            inplanes (int): Depth of input
            planes (int): Depth of output
            stride (int, optional): Convolutional stride parameter. Defaults to 1.
            downsample (Union[int,None], optional): Whether to downsample images for skip connection. Defaults to None.
            groups (int, optional): &lt;&gt;. Defaults to 1.
            base_width (int, optional): &lt;&gt;. Defaults to 64.
            dilation (int, optional): Convolutional dilation parameters. Defaults to 1.
            norm_layer (Union[nn.GroupNorm,nn.modules.batchnorm._NormBase,None], optional): Normalization layer throughout Block. Defaults to nn.BatchNorm2d.
            attention (str, optional): Attention type: CBAM or DBAM. Defaults to None.
            input_attention (bool, optional): Whether to include the input attention module. Defaults to False.
            part_attention (bool, optional): Whether to include the part (local) attention module. Defaults to False.
        &#34;&#34;&#34;
        super(BasicBlock, self).__init__()
        # Verify some base parameters
        if groups != 1 or base_width != 64:
            raise ValueError(&#39;BasicBlock only supports groups=1 and base_width=64&#39;)
        if dilation &gt; 1:
            raise NotImplementedError(&#34;Dilation &gt; 1 not supported in BasicBlock&#34;)
        if attention is not None and attention not in [&#34;cbam&#34;,&#34;dbam&#34;]:
            raise ValueError(&#34;attention parameter is unsupported value %s. Use one of &#39;cbam&#39;,&#39;dbam&#39;.&#34;%attention)
        # Both self.conv1 and self.downsample layers downsample the input when stride != 1
        # This builds the core layers in the BasicBlock
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False, groups=1, dilation=1)
        self.bn1 = norm_layer(planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False, groups=1, dilation=1)
        self.bn2 = norm_layer(planes)       
        
        # Sets up input attention
        self.input_attention = None
        if input_attention:
            self.input_attention = InputAttention(planes)
        
        # Sets up CBAM or DBAM attention
        self.ca = None
        self.sa = None
        if attention == &#39;cbam&#39;:
            self.ca = ChannelAttention(planes)
            self.sa = SpatialAttention(kernel_size=3)
        if attention == &#39;dbam&#39;:
            self.ca = ChannelAttention(planes)
            self.sa = DenseAttention(planes)
        
        # Sets up local attention
        self.p_ca = None
        self.p_sa = None
        if part_attention:
            self.p_sa = DenseAttention(planes=planes*self.expansion)
            self.p_ca = ChannelAttention(planes*self.expansion)
            
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        identity = x

        if self.input_attention is not None:
            x = self.input_attention(x) * x
        
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        if self.ca is not None:
            out = self.ca(out) * out
            out = self.sa(out) * out

        if self.downsample is not None:
            identity = self.downsample(x)

        p_out = out
        part_mask = None
        if self.p_ca is not None:   # Get part attention
            p_out = self.p_sa(p_out) * p_out
#            p_out = self.p_ca(p_out) * p_out
            p_out = self.relu(p_out)
            part_mask = self.p_ca(p_out)

        out = out + identity
        out = self.relu(out)

        if self.p_ca is not None:   # Concat part attention
            #out = torch.cat([p_out[:,p_out.shape[1]//2:,:,:],out[:,:p_out.shape[1]//2,:,:]],dim=1)
            out = (part_mask * p_out) + ((1-part_mask)*out)
        return out</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="GLAMOR.backbones.resnet.BasicBlock.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="GLAMOR.backbones.resnet.BasicBlock.expansion"><code class="name">var <span class="ident">expansion</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="GLAMOR.backbones.resnet.BasicBlock.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="GLAMOR.backbones.resnet.BasicBlock.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">    def forward(self, x):
        identity = x

        if self.input_attention is not None:
            x = self.input_attention(x) * x
        
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        if self.ca is not None:
            out = self.ca(out) * out
            out = self.sa(out) * out

        if self.downsample is not None:
            identity = self.downsample(x)

        p_out = out
        part_mask = None
        if self.p_ca is not None:   # Get part attention
            p_out = self.p_sa(p_out) * p_out
#            p_out = self.p_ca(p_out) * p_out
            p_out = self.relu(p_out)
            part_mask = self.p_ca(p_out)

        out = out + identity
        out = self.relu(out)

        if self.p_ca is not None:   # Concat part attention
            #out = torch.cat([p_out[:,p_out.shape[1]//2:,:,:],out[:,:p_out.shape[1]//2,:,:]],dim=1)
            out = (part_mask * p_out) + ((1-part_mask)*out)
        return out</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="GLAMOR.backbones.resnet.Bottleneck"><code class="flex name class">
<span>class <span class="ident">Bottleneck</span></span>
<span>(</span><span>inplanes, planes, stride=1, downsample=None, groups=1, base_width=64, dilation=1, norm_layer=None, attention=None, input_attention=False, part_attention=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Bottleneck(nn.Module):
    expansion = 4

    def __init__(   self, inplanes, planes, stride=1, downsample=None, groups = 1, base_width = 64, dilation = 1, norm_layer=None, 
                    attention = None, input_attention=False, part_attention=False):
        super(Bottleneck, self).__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        width = int(planes * (base_width / 64.)) * groups
        
        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False,stride=1)
        self.bn1 = norm_layer(width)
        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride, padding=dilation, bias=False, groups=groups, dilation=dilation)
        self.bn2 = norm_layer(width)
        self.conv3 = nn.Conv2d(width, planes * self.expansion, kernel_size=1, bias=False, stride=1)
        self.bn3 = norm_layer(planes * self.expansion)
        self.relu = nn.ReLU(inplace=True)
        
        if input_attention:
            self.input_attention = InputAttention(planes)
        else:
            self.input_attention = None

        if attention is None:
            self.ca = None
            self.sa = None
        elif attention == &#39;cbam&#39;:
            self.sa = SpatialAttention(kernel_size=3)
            self.ca = ChannelAttention(planes*self.expansion)
        elif attention == &#39;dbam&#39;:
            self.ca = ChannelAttention(planes)
            self.sa = DenseAttention(planes)
        else:
            raise NotImplementedError()

        if part_attention:
            self.p_sa = DenseAttention(planes=planes*self.expansion)
            self.p_ca = ChannelAttention(planes*self.expansion)
        else:
            self.p_ca = None
            self.p_sa = None
        
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        identity = x

        if self.input_attention is not None:
            x = self.input_attention(x) * x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.ca is not None:
            out = self.ca(out) * out
            out = self.sa(out) * out

        if self.downsample is not None:
            identity = self.downsample(x)

        p_out = out
        part_mask = None
        if self.p_ca is not None:   # Get part attention
            p_out = self.p_sa(p_out) * p_out
#            p_out = self.p_ca(p_out) * p_out
            p_out = self.relu(p_out)
            part_mask = self.p_ca(p_out)
        
        out = out + identity
        out = self.relu(out)

        if self.p_ca is not None:   # Concat part attention
            #out = torch.cat([p_out[:,p_out.shape[1]//2:,:,:],out[:,:p_out.shape[1]//2,:,:]],dim=1)
            out = (part_mask * p_out) + ((1-part_mask)*out)
        return out</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="GLAMOR.backbones.resnet.Bottleneck.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="GLAMOR.backbones.resnet.Bottleneck.expansion"><code class="name">var <span class="ident">expansion</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="GLAMOR.backbones.resnet.Bottleneck.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="GLAMOR.backbones.resnet.Bottleneck.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">    def forward(self, x):
        identity = x

        if self.input_attention is not None:
            x = self.input_attention(x) * x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.ca is not None:
            out = self.ca(out) * out
            out = self.sa(out) * out

        if self.downsample is not None:
            identity = self.downsample(x)

        p_out = out
        part_mask = None
        if self.p_ca is not None:   # Get part attention
            p_out = self.p_sa(p_out) * p_out
#            p_out = self.p_ca(p_out) * p_out
            p_out = self.relu(p_out)
            part_mask = self.p_ca(p_out)
        
        out = out + identity
        out = self.relu(out)

        if self.p_ca is not None:   # Concat part attention
            #out = torch.cat([p_out[:,p_out.shape[1]//2:,:,:],out[:,:p_out.shape[1]//2,:,:]],dim=1)
            out = (part_mask * p_out) + ((1-part_mask)*out)
        return out</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="GLAMOR.backbones.resnet.ChannelAttention"><code class="flex name class">
<span>class <span class="ident">ChannelAttention</span></span>
<span>(</span><span>in_planes, ratio=16)</span>
</code></dt>
<dd>
<div class="desc"><p>Channel Attention module that forms part of CBAM attention.
See the ECCV paper at <a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.pdf">https://openaccess.thecvf.com/content_ECCV_2018/papers/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.pdf</a></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ChannelAttention(nn.Module):
    &#34;&#34;&#34;Channel Attention module that forms part of CBAM attention.
    See the ECCV paper at https://openaccess.thecvf.com/content_ECCV_2018/papers/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.pdf
    &#34;&#34;&#34;
    def __init__(self, in_planes, ratio=16):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)

        self.fc1   = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)
        self.relu1 = nn.ReLU()
        self.fc2   = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)

        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))
        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))
        out = avg_out + max_out
        return self.sigmoid(out)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="GLAMOR.backbones.resnet.ChannelAttention.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="GLAMOR.backbones.resnet.ChannelAttention.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="GLAMOR.backbones.resnet.ChannelAttention.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))
    max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))
    out = avg_out + max_out
    return self.sigmoid(out)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="GLAMOR.backbones.resnet.DenseAttention"><code class="flex name class">
<span>class <span class="ident">DenseAttention</span></span>
<span>(</span><span>planes)</span>
</code></dt>
<dd>
<div class="desc"><p>Dense attention module for global attention from GLAMOR; see arXiv paper
at <a href="https://arxiv.org/pdf/2002.02256.pdf">https://arxiv.org/pdf/2002.02256.pdf</a></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DenseAttention(nn.Module):    # Like spatial, but for all channels
    &#34;&#34;&#34;Dense attention module for global attention from GLAMOR; see arXiv paper
    at https://arxiv.org/pdf/2002.02256.pdf
    &#34;&#34;&#34;
    def __init__(self, planes):
        super(DenseAttention, self).__init__()
        self.dense_conv1=nn.Conv2d(planes,planes,kernel_size=3,padding=1,bias=False)
        self.dense_relu1=nn.LeakyReLU()
        self.dense_conv2=nn.Conv2d(planes,planes,kernel_size=3,padding=1,bias=False)
        self.dense_sigmoid = nn.Sigmoid()
    def forward(self,x):
        x = self.dense_conv1(x)
        x = self.dense_relu1(x)
        x = self.dense_conv2(x)
        x = self.dense_sigmoid(x)
        return x</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="GLAMOR.backbones.resnet.DenseAttention.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="GLAMOR.backbones.resnet.DenseAttention.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="GLAMOR.backbones.resnet.DenseAttention.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self,x):
    x = self.dense_conv1(x)
    x = self.dense_relu1(x)
    x = self.dense_conv2(x)
    x = self.dense_sigmoid(x)
    return x</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="GLAMOR.backbones.resnet.InputAttention"><code class="flex name class">
<span>class <span class="ident">InputAttention</span></span>
<span>(</span><span>planes)</span>
</code></dt>
<dd>
<div class="desc"><p>Input attention module for global/local attention from GLAMOR; see arXiv paper
at <a href="https://arxiv.org/pdf/2002.02256.pdf">https://arxiv.org/pdf/2002.02256.pdf</a></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class InputAttention(nn.Module):
    &#34;&#34;&#34;Input attention module for global/local attention from GLAMOR; see arXiv paper
    at https://arxiv.org/pdf/2002.02256.pdf
    &#34;&#34;&#34;
    def __init__(self, planes):
        super(InputAttention, self).__init__()
        self.ia_conv1=nn.Conv2d(planes,planes,kernel_size=3,padding=1,bias=False)
        self.ia_relu1=nn.LeakyReLU()
        self.ia_conv2=nn.Conv2d(planes,planes,kernel_size=3,padding=1,bias=False)
        self.ia_sigmoid = nn.Sigmoid()
    def forward(self,x):
        x = self.ia_conv1(x)
        x = self.ia_relu1(x)
        x = self.ia_conv2(x)
        x = self.ia_sigmoid(x)
        return x</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="GLAMOR.backbones.resnet.InputAttention.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="GLAMOR.backbones.resnet.InputAttention.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="GLAMOR.backbones.resnet.InputAttention.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self,x):
    x = self.ia_conv1(x)
    x = self.ia_relu1(x)
    x = self.ia_conv2(x)
    x = self.ia_sigmoid(x)
    return x</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="GLAMOR.backbones.resnet.ResNet"><code class="flex name class">
<span>class <span class="ident">ResNet</span></span>
<span>(</span><span>block=GLAMOR.backbones.resnet.Bottleneck, layers=[3, 4, 6, 3], last_stride=2, zero_init_residual=False, top_only=True, num_classes=1000, groups=1, width_per_group=64, replace_stride_with_dilation=None, norm_layer=None, attention=None, input_attention=None, secondary_attention=None, ia_attention=None, part_attention=None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ResNet(nn.Module):
    def __init__(self, block=Bottleneck, layers=[3, 4, 6, 3], last_stride=2, zero_init_residual=False, \
                    top_only=True, num_classes=1000, groups=1, width_per_group=64, replace_stride_with_dilation=None,norm_layer=None, 
                    attention=None, input_attention = None, secondary_attention=None, ia_attention = None, part_attention = None,
                    **kwargs):
        super().__init__()
        self.attention=attention
        self.input_attention=input_attention
        self.secondary_attention=secondary_attention
        self.block=block
        self.inplanes = 64
        if norm_layer is None:
            self._norm_layer = nn.BatchNorm2d
        #elif norm_layer == &#34;ln&#34;:
        #    self._norm_layer = nn.LayerNorm
        self.dilation = 1
        if replace_stride_with_dilation is None:
            replace_stride_with_dilation = [False, False, False]
        if len(replace_stride_with_dilation) != 3:
            raise ValueError(&#34;replace_stride_with_dilation should be `None` or a 3-element tuple. Got {}&#34;.format(replace_stride_with_dilation))
        self.groups = groups
        self.base_width = width_per_group

        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)
        #if norm_layer == &#34;gn&#34;:
        #    self.bn1 = nn.GroupNorm2d
        self.bn1 = nn.BatchNorm2d(self.inplanes)
        self.relu1 = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

        self.ia_attention = ia_attention
        self.part_attention = part_attention
        
        # Make sure ia and input_attention do not conflict
        if self.ia_attention is not None and self.input_attention is not None:
            raise ValueError(&#34;Cannot have both ia_attention and input_attention.&#34;)
        if self.part_attention is not None and (self.attention is not None and self.secondary_attention is None):
            raise ValueError(&#34;Cannot have part-attention with CBAM everywhere&#34;)
        if self.part_attention is not None and (self.attention is not None and self.secondary_attention==1):
            raise ValueError(&#34;Cannot have part-attention with CBAM-Early&#34;)

        # Create true IA
        if self.ia_attention:
            self.ia_attention = InputAttention(self.inplanes)   # 64, set above
        else:
            self.ia_attention = None

        att = self.attention
        if secondary_attention is not None and secondary_attention != 1: # leave alone if sec attention not set
            att = None
        self.layer1 = self._make_layer(self.block, 64, layers[0], attention = att, input_attention=self.input_attention, part_attention = self.part_attention)
        att = self.attention
        if secondary_attention is not None and secondary_attention != 2: # leave alone if sec attention not set
            att = None
        self.layer2 = self._make_layer(self.block, 128, layers[1], stride=2, attention = att, dilate=replace_stride_with_dilation[0])
        att = self.attention
        if secondary_attention is not None and secondary_attention != 3: # leave alone if sec attention not set
            att = None
        self.layer3 = self._make_layer(self.block, 256, layers[2], stride=2, attention = att, dilate=replace_stride_with_dilation[1])
        att = self.attention
        if secondary_attention is not None and secondary_attention != 4: # leave alone if sec attention not set
            att = None
        self.layer4 = self._make_layer(self.block, 512, layers[3], stride=last_stride, attention = att, dilate=replace_stride_with_dilation[2])
        
        self.top_only = top_only
        self.avgpool, self.fc = None, None

        if not self.top_only:
            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
            self.fc = nn.Linear(512 * block.expansion, num_classes)
    
    def _make_layer(self, block, planes, blocks, stride=1, dilate = False, attention = None, input_attention=False, ia_attention = False, part_attention = False):
        downsample = None
        previous_dilation = self.dilation
        if dilate:
            self.dilation *= stride
            stride = 1
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.inplanes, planes * block.expansion,
                            kernel_size=1, stride=stride, bias=False),
                self._norm_layer(planes * block.expansion),
            )
    
        layers = []
        layers.append(block(self.inplanes, planes, stride, downsample,groups = self.groups, base_width = self.base_width, dilation = previous_dilation, norm_layer=self._norm_layer, attention=attention, input_attention=input_attention, part_attention=part_attention))
        self.inplanes = planes * block.expansion
        for _ in range(1, blocks):
            layers.append(block(self.inplanes, planes, groups = self.groups, base_width = self.base_width, dilation = self.dilation, norm_layer=self._norm_layer, attention=attention))
        return nn.Sequential(*layers)
    
    def forward(self, x):
        x = self.conv1(x)
        
        if self.ia_attention is not None:
            x = self.ia_attention(x) * x
        x = self.bn1(x)
        x = self.relu1(x)
        x = self.maxpool(x)
    
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        if not self.top_only:
            x = self.avgpool(x)
            x = torch.flatten(x,1)
            x = self.fc(x)            
        return x
    
    def load_param(self, weights_path):
        param_dict = torch.load(weights_path)
        for i in param_dict:
            if &#39;fc&#39; in i and self.top_only:
                continue
            self.state_dict()[i].copy_(param_dict[i])</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="GLAMOR.backbones.resnet.ResNet.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="GLAMOR.backbones.resnet.ResNet.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="GLAMOR.backbones.resnet.ResNet.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    x = self.conv1(x)
    
    if self.ia_attention is not None:
        x = self.ia_attention(x) * x
    x = self.bn1(x)
    x = self.relu1(x)
    x = self.maxpool(x)

    x = self.layer1(x)
    x = self.layer2(x)
    x = self.layer3(x)
    x = self.layer4(x)

    if not self.top_only:
        x = self.avgpool(x)
        x = torch.flatten(x,1)
        x = self.fc(x)            
    return x</code></pre>
</details>
</dd>
<dt id="GLAMOR.backbones.resnet.ResNet.load_param"><code class="name flex">
<span>def <span class="ident">load_param</span></span>(<span>self, weights_path)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_param(self, weights_path):
    param_dict = torch.load(weights_path)
    for i in param_dict:
        if &#39;fc&#39; in i and self.top_only:
            continue
        self.state_dict()[i].copy_(param_dict[i])</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="GLAMOR.backbones.resnet.SpatialAttention"><code class="flex name class">
<span>class <span class="ident">SpatialAttention</span></span>
<span>(</span><span>kernel_size=7)</span>
</code></dt>
<dd>
<div class="desc"><p>Spatial Attention module that forms part of CBAM attention.
See the ECCV paper at <a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.pdf">https://openaccess.thecvf.com/content_ECCV_2018/papers/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.pdf</a></p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SpatialAttention(nn.Module):
    &#34;&#34;&#34;Spatial Attention module that forms part of CBAM attention.
    See the ECCV paper at https://openaccess.thecvf.com/content_ECCV_2018/papers/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.pdf
    &#34;&#34;&#34;
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()

        assert kernel_size in (3, 7), &#39;kernel size must be 3 or 7&#39;
        padding = 3 if kernel_size == 7 else 1

        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        x = torch.cat([avg_out, max_out], dim=1)
        x = self.conv1(x)
        return self.sigmoid(x)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="GLAMOR.backbones.resnet.SpatialAttention.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="GLAMOR.backbones.resnet.SpatialAttention.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="GLAMOR.backbones.resnet.SpatialAttention.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    avg_out = torch.mean(x, dim=1, keepdim=True)
    max_out, _ = torch.max(x, dim=1, keepdim=True)
    x = torch.cat([avg_out, max_out], dim=1)
    x = self.conv1(x)
    return self.sigmoid(x)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="GLAMOR.backbones" href="index.html">GLAMOR.backbones</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="GLAMOR.backbones.resnet.resnet101" href="#GLAMOR.backbones.resnet.resnet101">resnet101</a></code></li>
<li><code><a title="GLAMOR.backbones.resnet.resnet152" href="#GLAMOR.backbones.resnet.resnet152">resnet152</a></code></li>
<li><code><a title="GLAMOR.backbones.resnet.resnet18" href="#GLAMOR.backbones.resnet.resnet18">resnet18</a></code></li>
<li><code><a title="GLAMOR.backbones.resnet.resnet34" href="#GLAMOR.backbones.resnet.resnet34">resnet34</a></code></li>
<li><code><a title="GLAMOR.backbones.resnet.resnet50" href="#GLAMOR.backbones.resnet.resnet50">resnet50</a></code></li>
<li><code><a title="GLAMOR.backbones.resnet.resnext101_32x8d" href="#GLAMOR.backbones.resnet.resnext101_32x8d">resnext101_32x8d</a></code></li>
<li><code><a title="GLAMOR.backbones.resnet.resnext50_32x4d" href="#GLAMOR.backbones.resnet.resnext50_32x4d">resnext50_32x4d</a></code></li>
<li><code><a title="GLAMOR.backbones.resnet.wide_resnet101_2" href="#GLAMOR.backbones.resnet.wide_resnet101_2">wide_resnet101_2</a></code></li>
<li><code><a title="GLAMOR.backbones.resnet.wide_resnet50_2" href="#GLAMOR.backbones.resnet.wide_resnet50_2">wide_resnet50_2</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="GLAMOR.backbones.resnet.BasicBlock" href="#GLAMOR.backbones.resnet.BasicBlock">BasicBlock</a></code></h4>
<ul class="">
<li><code><a title="GLAMOR.backbones.resnet.BasicBlock.dump_patches" href="#GLAMOR.backbones.resnet.BasicBlock.dump_patches">dump_patches</a></code></li>
<li><code><a title="GLAMOR.backbones.resnet.BasicBlock.expansion" href="#GLAMOR.backbones.resnet.BasicBlock.expansion">expansion</a></code></li>
<li><code><a title="GLAMOR.backbones.resnet.BasicBlock.forward" href="#GLAMOR.backbones.resnet.BasicBlock.forward">forward</a></code></li>
<li><code><a title="GLAMOR.backbones.resnet.BasicBlock.training" href="#GLAMOR.backbones.resnet.BasicBlock.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="GLAMOR.backbones.resnet.Bottleneck" href="#GLAMOR.backbones.resnet.Bottleneck">Bottleneck</a></code></h4>
<ul class="">
<li><code><a title="GLAMOR.backbones.resnet.Bottleneck.dump_patches" href="#GLAMOR.backbones.resnet.Bottleneck.dump_patches">dump_patches</a></code></li>
<li><code><a title="GLAMOR.backbones.resnet.Bottleneck.expansion" href="#GLAMOR.backbones.resnet.Bottleneck.expansion">expansion</a></code></li>
<li><code><a title="GLAMOR.backbones.resnet.Bottleneck.forward" href="#GLAMOR.backbones.resnet.Bottleneck.forward">forward</a></code></li>
<li><code><a title="GLAMOR.backbones.resnet.Bottleneck.training" href="#GLAMOR.backbones.resnet.Bottleneck.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="GLAMOR.backbones.resnet.ChannelAttention" href="#GLAMOR.backbones.resnet.ChannelAttention">ChannelAttention</a></code></h4>
<ul class="">
<li><code><a title="GLAMOR.backbones.resnet.ChannelAttention.dump_patches" href="#GLAMOR.backbones.resnet.ChannelAttention.dump_patches">dump_patches</a></code></li>
<li><code><a title="GLAMOR.backbones.resnet.ChannelAttention.forward" href="#GLAMOR.backbones.resnet.ChannelAttention.forward">forward</a></code></li>
<li><code><a title="GLAMOR.backbones.resnet.ChannelAttention.training" href="#GLAMOR.backbones.resnet.ChannelAttention.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="GLAMOR.backbones.resnet.DenseAttention" href="#GLAMOR.backbones.resnet.DenseAttention">DenseAttention</a></code></h4>
<ul class="">
<li><code><a title="GLAMOR.backbones.resnet.DenseAttention.dump_patches" href="#GLAMOR.backbones.resnet.DenseAttention.dump_patches">dump_patches</a></code></li>
<li><code><a title="GLAMOR.backbones.resnet.DenseAttention.forward" href="#GLAMOR.backbones.resnet.DenseAttention.forward">forward</a></code></li>
<li><code><a title="GLAMOR.backbones.resnet.DenseAttention.training" href="#GLAMOR.backbones.resnet.DenseAttention.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="GLAMOR.backbones.resnet.InputAttention" href="#GLAMOR.backbones.resnet.InputAttention">InputAttention</a></code></h4>
<ul class="">
<li><code><a title="GLAMOR.backbones.resnet.InputAttention.dump_patches" href="#GLAMOR.backbones.resnet.InputAttention.dump_patches">dump_patches</a></code></li>
<li><code><a title="GLAMOR.backbones.resnet.InputAttention.forward" href="#GLAMOR.backbones.resnet.InputAttention.forward">forward</a></code></li>
<li><code><a title="GLAMOR.backbones.resnet.InputAttention.training" href="#GLAMOR.backbones.resnet.InputAttention.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="GLAMOR.backbones.resnet.ResNet" href="#GLAMOR.backbones.resnet.ResNet">ResNet</a></code></h4>
<ul class="">
<li><code><a title="GLAMOR.backbones.resnet.ResNet.dump_patches" href="#GLAMOR.backbones.resnet.ResNet.dump_patches">dump_patches</a></code></li>
<li><code><a title="GLAMOR.backbones.resnet.ResNet.forward" href="#GLAMOR.backbones.resnet.ResNet.forward">forward</a></code></li>
<li><code><a title="GLAMOR.backbones.resnet.ResNet.load_param" href="#GLAMOR.backbones.resnet.ResNet.load_param">load_param</a></code></li>
<li><code><a title="GLAMOR.backbones.resnet.ResNet.training" href="#GLAMOR.backbones.resnet.ResNet.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="GLAMOR.backbones.resnet.SpatialAttention" href="#GLAMOR.backbones.resnet.SpatialAttention">SpatialAttention</a></code></h4>
<ul class="">
<li><code><a title="GLAMOR.backbones.resnet.SpatialAttention.dump_patches" href="#GLAMOR.backbones.resnet.SpatialAttention.dump_patches">dump_patches</a></code></li>
<li><code><a title="GLAMOR.backbones.resnet.SpatialAttention.forward" href="#GLAMOR.backbones.resnet.SpatialAttention.forward">forward</a></code></li>
<li><code><a title="GLAMOR.backbones.resnet.SpatialAttention.training" href="#GLAMOR.backbones.resnet.SpatialAttention.training">training</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>