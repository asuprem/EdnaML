<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>GLAMOR.backbones.interpretableresnet API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>GLAMOR.backbones.interpretableresnet</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from torch import nn
import torch
import pdb

class ChannelAttention(nn.Module):
    def __init__(self, in_planes, ratio=16):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)

        self.fc1   = nn.Conv2d(in_planes, in_planes // 16, 1, bias=False)
        self.relu1 = nn.ReLU()
        self.fc2   = nn.Conv2d(in_planes // 16, in_planes, 1, bias=False)

        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))
        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))
        out = avg_out + max_out
        return self.sigmoid(out)

class SpatialAttention(nn.Module):
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()

        assert kernel_size in (3, 7), &#39;kernel size must be 3 or 7&#39;
        padding = 3 if kernel_size == 7 else 1

        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        x = torch.cat([avg_out, max_out], dim=1)
        x = self.conv1(x)
        return self.sigmoid(x)

class DenseAttention(nn.Module):    # Like spatial, but for all channels
    def __init__(self, planes):
        super(DenseAttention, self).__init__()
        self.dense_conv1=nn.Conv2d(planes,planes,kernel_size=3,padding=1,bias=False)
        self.dense_relu1=nn.LeakyReLU()
        self.dense_conv2=nn.Conv2d(planes,planes,kernel_size=3,padding=1,bias=False)
        self.dense_sigmoid = nn.Sigmoid()
    def forward(self,x):
        x = self.dense_conv1(x)
        x = self.dense_relu1(x)
        x = self.dense_conv2(x)
        x = self.dense_sigmoid(x)
        return x

class InputAttention(nn.Module):
    def __init__(self, planes):
        super(InputAttention, self).__init__()
        self.ia_conv1=nn.Conv2d(planes,planes,kernel_size=3,padding=1,bias=False)
        self.ia_relu1=nn.LeakyReLU()
        self.ia_conv2=nn.Conv2d(planes,planes,kernel_size=3,padding=1,bias=False)
        self.ia_sigmoid = nn.Sigmoid()
    def forward(self,x):
        x = self.ia_conv1(x)
        x = self.ia_relu1(x)
        x = self.ia_conv2(x)
        x = self.ia_sigmoid(x)
        return x

class BasicBlock(nn.Module):
    expansion = 1

    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,
                 base_width=64, dilation=1, norm_layer=None, 
                 attention=None, input_attention=False, part_attention=False):
        super(BasicBlock, self).__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        if groups != 1 or base_width != 64:
            raise ValueError(&#39;BasicBlock only supports groups=1 and base_width=64&#39;)
        if dilation &gt; 1:
            raise NotImplementedError(&#34;Dilation &gt; 1 not supported in BasicBlock&#34;)
        # Both self.conv1 and self.downsample layers downsample the input when stride != 1
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False, groups=1, dilation=1)
        self.bn1 = norm_layer(planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False, groups=1, dilation=1)
        self.bn2 = norm_layer(planes)       
        
        if input_attention:
            self.input_attention = InputAttention(planes)
        else:
            self.input_attention = None

        if attention is None:
            self.ca = None
            self.sa = None
        elif attention == &#39;cbam&#39;:
            self.ca = ChannelAttention(planes)
            self.sa = SpatialAttention(kernel_size=3)
        elif attention == &#39;dbam&#39;:
            self.ca = ChannelAttention(planes)
            self.sa = DenseAttention(planes)
        else:
            raise NotImplementedError()

        if part_attention:
            self.p_sa = DenseAttention(planes=planes*self.expansion)
            self.p_ca = ChannelAttention(planes*self.expansion)
        else:
            self.p_ca = None
            self.p_sa = None

        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        identity = x

        if self.input_attention is not None:
            x = self.input_attention(x) * x
        
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        if self.ca is not None:
            out = self.ca(out) * out
            out = self.sa(out) * out

        if self.downsample is not None:
            identity = self.downsample(x)

        p_out = out
        part_mask = None
        if self.p_ca is not None:   # Get part attention
            p_out = self.p_sa(p_out) * p_out
#            p_out = self.p_ca(p_out) * p_out
            p_out = self.relu(p_out)
            part_mask = self.p_ca(p_out)

        out = out + identity
        out = self.relu(out)

        if self.p_ca is not None:   # Concat part attention
            #out = torch.cat([p_out[:,p_out.shape[1]//2:,:,:],out[:,:p_out.shape[1]//2,:,:]],dim=1)
            out = (part_mask * p_out) + ((1-part_mask)*out)
        return out

class Bottleneck(nn.Module):
    expansion = 4

    def __init__(   self, inplanes, planes, stride=1, downsample=None, groups = 1, base_width = 64, dilation = 1, norm_layer=None, 
                    attention = None, input_attention=False, part_attention=False):
        super(Bottleneck, self).__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        width = int(planes * (base_width / 64.)) * groups
        
        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False,stride=1)
        self.bn1 = norm_layer(width)
        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride, padding=dilation, bias=False, groups=groups, dilation=dilation)
        self.bn2 = norm_layer(width)
        self.conv3 = nn.Conv2d(width, planes * self.expansion, kernel_size=1, bias=False, stride=1)
        self.bn3 = norm_layer(planes * self.expansion)
        self.relu = nn.ReLU(inplace=True)
        
        if input_attention:
            self.input_attention = InputAttention(planes)
        else:
            self.input_attention = None

        if attention is None:
            self.ca = None
            self.sa = None
        elif attention == &#39;cbam&#39;:
            self.sa = SpatialAttention(kernel_size=3)
            self.ca = ChannelAttention(planes*self.expansion)
        elif attention == &#39;dbam&#39;:
            self.ca = ChannelAttention(planes)
            self.sa = DenseAttention(planes)
        else:
            raise NotImplementedError()

        if part_attention:
            self.p_sa = DenseAttention(planes=planes*self.expansion)
            self.p_ca = ChannelAttention(planes*self.expansion)
        else:
            self.p_ca = None
            self.p_sa = None
        
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        identity = x

        if self.input_attention is not None:
            x = self.input_attention(x) * x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.ca is not None:
            out = self.ca(out) * out
            out = self.sa(out) * out

        if self.downsample is not None:
            identity = self.downsample(x)

        p_out = out
        part_mask = None
        if self.p_ca is not None:   # Get part attention
            p_out = self.p_sa(p_out) * p_out
#            p_out = self.p_ca(p_out) * p_out
            p_out = self.relu(p_out)
            part_mask = self.p_ca(p_out)
        
        out = out + identity
        out = self.relu(out)

        if self.p_ca is not None:   # Concat part attention
            #out = torch.cat([p_out[:,p_out.shape[1]//2:,:,:],out[:,:p_out.shape[1]//2,:,:]],dim=1)
            out = (part_mask * p_out) + ((1-part_mask)*out)
        return out

class InterpretableResNet(nn.Module):
    def __init__(self, block=Bottleneck, layers=[3, 4, 6, 3], last_stride=2, zero_init_residual=False, \
                    top_only=True, num_classes=1000, groups=1, width_per_group=64, replace_stride_with_dilation=None,norm_layer=None, 
                    attention=None, input_attention = None, secondary_attention=None, ia_attention = None, part_attention = None,
                    branches = 2, branch_classes = [10,10],
                    **kwargs):
        super().__init__()
        self.attention=attention
        self.input_attention=input_attention
        self.secondary_attention=secondary_attention
        self.block=block
        self.inplanes = 64

        if top_only is False:
            raise Warning(&#34;top_only parameter should be true. Otherwise, there could be ininteded performance results from model, and it might not compile.&#34;)

        if branches &lt;2:
            raise ValueError(&#34;Must have &gt;1 branch in interpretable model&#34;)
        if len(branch_classes)!= branches:
            raise ValueError(&#34;Number of entries in branch_classes much equal number of branches.&#34;)
        self.branches = branches
        self.branch_classes = branch_classes

        if norm_layer is None:
            self._norm_layer = nn.BatchNorm2d
        #elif norm_layer == &#34;ln&#34;:
        #    self._norm_layer = nn.LayerNorm
        self.dilation = 1
        if replace_stride_with_dilation is None:
            replace_stride_with_dilation = [False, False, False]
        if len(replace_stride_with_dilation) != 3:
            raise ValueError(&#34;replace_stride_with_dilation should be `None` or a 3-element tuple. Got {}&#34;.format(replace_stride_with_dilation))
        self.groups = groups
        self.base_width = width_per_group

        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)
        #if norm_layer == &#34;gn&#34;:
        #    self.bn1 = nn.GroupNorm2d
        self.bn1 = nn.BatchNorm2d(self.inplanes)
        self.relu1 = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

        self.ia_attention = ia_attention
        self.part_attention = part_attention
        
        # Make sure ia and input_attention do not conflict
        if self.ia_attention is not None and self.input_attention is not None:
            raise ValueError(&#34;Cannot have both ia_attention and input_attention.&#34;)
        if self.part_attention is not None and (self.attention is not None and self.secondary_attention is None):
            raise ValueError(&#34;Cannot have part-attention with CBAM everywhere&#34;)
        if self.part_attention is not None and (self.attention is not None and self.secondary_attention==1):
            raise ValueError(&#34;Cannot have part-attention with CBAM-Early&#34;)

        # Create true IA
        if self.ia_attention:
            self.ia_attention = InputAttention(self.inplanes)   # 64, set above
        else:
            self.ia_attention = None

        att = self.attention
        if secondary_attention is not None and secondary_attention != 1: # leave alone if sec attention not set
            att = None
        self.layer1 = self._make_layer(self.block, 64, layers[0], attention = att, input_attention=self.input_attention, part_attention = self.part_attention)



        self.branch_layers = [None]*self.branches

        for branch_idx in self.branches:
            self.branch_layers[branch_idx] = {}   # for layer 2,3,4
            att = self.attention
            if secondary_attention is not None and secondary_attention != 2: # leave alone if sec attention not set
                att = None
            self.branch_layers[branch_idx][&#34;layer2&#34;] = self._make_layer(self.block, 128, layers[1], stride=2, attention = att, dilate=replace_stride_with_dilation[0])
            att = self.attention
            if secondary_attention is not None and secondary_attention != 3: # leave alone if sec attention not set
                att = None
            self.branch_layers[branch_idx][&#34;layer3&#34;] = self._make_layer(self.block, 256, layers[2], stride=2, attention = att, dilate=replace_stride_with_dilation[1])
            att = self.attention
            if secondary_attention is not None and secondary_attention != 4: # leave alone if sec attention not set
                att = None
            self.branch_layers[branch_idx][&#34;layer4&#34;] = self._make_layer(self.block, 512, layers[3], stride=last_stride, attention = att, dilate=replace_stride_with_dilation[2])

            # NOTE TODO: top only is not used -- possibly in future when we no longer use the interpretable stuff?????
            #if not top_only:
                #self.branch_layers[branch_idx][&#34;avgpool&#34;] = nn.AdaptiveAvgPool2d((1, 1))
                #self.branch_layers[branch_idx][&#34;fc&#34;] = nn.Linear(512 * block.expansion, self.branch_classes[branch_idx])



        self.top_only = top_only
        self.avgpool, self.fc = None, None

        #if not self.top_only:
        #    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        #    self.fc = nn.Linear(512 * block.expansion * self.branches, num_classes)
    
    def _make_layer(self, block, planes, blocks, stride=1, dilate = False, attention = None, input_attention=False, ia_attention = False, part_attention = False):
        downsample = None
        previous_dilation = self.dilation
        if dilate:
            self.dilation *= stride
            stride = 1
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.inplanes, planes * block.expansion,
                            kernel_size=1, stride=stride, bias=False),
                self._norm_layer(planes * block.expansion),
            )
    
        layers = []
        layers.append(block(self.inplanes, planes, stride, downsample,groups = self.groups, base_width = self.base_width, dilation = previous_dilation, norm_layer=self._norm_layer, attention=attention, input_attention=input_attention, part_attention=part_attention))
        self.inplanes = planes * block.expansion
        for _ in range(1, blocks):
            layers.append(block(self.inplanes, planes, groups = self.groups, base_width = self.base_width, dilation = self.dilation, norm_layer=self._norm_layer, attention=attention))
        return nn.Sequential(*layers)
    
    def forward(self, x):
        branch_outs = [None]*self.branches
        branch_features = [None]*self.branches
        x = self.conv1(x)
        
        if self.ia_attention is not None:
            x = self.ia_attention(x) * x
        x = self.bn1(x)
        x = self.relu1(x)
        x = self.maxpool(x)
    
        x = self.layer1(x)

        for branch_idx in self.branches:
            branch_outs[branch_idx] = self.branch_layers[branch_idx][&#34;layer2&#34;](x)
            branch_outs[branch_idx] = self.branch_layers[branch_idx][&#34;layer3&#34;](branch_outs[branch_idx])
            branch_outs[branch_idx] = self.branch_layers[branch_idx][&#34;layer4&#34;](branch_outs[branch_idx])
            #branch_features[branch_idx]=nn.Identity(branch_outs[branch_idx])

            #if not self.top_only:
            #    branch_outs[branch_idx] = self.branch_layers[branch_idx][&#34;avgpool&#34;](branch_outs[branch_idx])
            #    branch_outs[branch_idx] = torch.flatten(branch_outs[branch_idx],1)
            #    branch_outs[branch_idx] = self.branch_layers[branch_idx][&#34;fc&#34;](branch_outs[branch_idx])


        #x = self.layer2(x)
        #x = self.layer3(x)
        #x = self.layer4(x)
        # NOTE: top_only will never be false
        x = torch.concat(branch_features,1)

        #if not self.top_only:
        #    x = self.avgpool(x)
        #    x = torch.flatten(x,1)
        #    x = self.fc(x)
        return x, branch_outs
    
    def load_param(self, weights_path):
        # need to verify this TODO
        param_dict = torch.load(weights_path)
        for i in param_dict:
            if &#39;fc&#39; in i and self.top_only:
                continue
            self.state_dict()[i].copy_(param_dict[i])
            
            
def _interpretableresnet(arch, block, layers, pretrained, progress, **kwargs):
    # kwargs includes --&gt; branches
    model = InterpretableResNet(block, layers, **kwargs)
    return model


def interpretable_resnet18(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNet-18 model from
    `&#34;Deep Residual Learning for Image Recognition&#34; &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    return _interpretableresnet(&#39;resnet18&#39;, BasicBlock, [2, 2, 2, 2], pretrained, progress,
                   **kwargs)


def interpretable_resnet34(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNet-34 model from
    `&#34;Deep Residual Learning for Image Recognition&#34; &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    return _interpretableresnet(&#39;resnet34&#39;, BasicBlock, [3, 4, 6, 3], pretrained, progress,
                   **kwargs)


def interpretable_resnet50(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNet-50 model from
    `&#34;Deep Residual Learning for Image Recognition&#34; &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    return _interpretableresnet(&#39;resnet50&#39;, Bottleneck, [3, 4, 6, 3], pretrained, progress,
                   **kwargs)


def interpretable_resnet101(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNet-101 model from
    `&#34;Deep Residual Learning for Image Recognition&#34; &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    return _interpretableresnet(&#39;resnet101&#39;, Bottleneck, [3, 4, 23, 3], pretrained, progress,
                   **kwargs)


def interpretable_resnet152(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNet-152 model from
    `&#34;Deep Residual Learning for Image Recognition&#34; &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    return _interpretableresnet(&#39;resnet152&#39;, Bottleneck, [3, 8, 36, 3], pretrained, progress,
                   **kwargs)


def interpretable_resnext50_32x4d(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNeXt-50 32x4d model from
    `&#34;Aggregated Residual Transformation for Deep Neural Networks&#34; &lt;https://arxiv.org/pdf/1611.05431.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    kwargs[&#39;groups&#39;] = 32
    kwargs[&#39;width_per_group&#39;] = 4
    return _interpretableresnet(&#39;resnext50_32x4d&#39;, Bottleneck, [3, 4, 6, 3],
                   pretrained, progress, **kwargs)


def interpretable_resnext101_32x8d(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNeXt-101 32x8d model from
    `&#34;Aggregated Residual Transformation for Deep Neural Networks&#34; &lt;https://arxiv.org/pdf/1611.05431.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    kwargs[&#39;groups&#39;] = 32
    kwargs[&#39;width_per_group&#39;] = 8
    return _interpretableresnet(&#39;resnext101_32x8d&#39;, Bottleneck, [3, 4, 23, 3],
                   pretrained, progress, **kwargs)


def interpretable_wide_resnet50_2(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;Wide ResNet-50-2 model from
    `&#34;Wide Residual Networks&#34; &lt;https://arxiv.org/pdf/1605.07146.pdf&gt;`_
    The model is the same as ResNet except for the bottleneck number of channels
    which is twice larger in every block. The number of channels in outer 1x1
    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048
    channels, and in Wide ResNet-50-2 has 2048-1024-2048.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    kwargs[&#39;width_per_group&#39;] = 64 * 2
    return _interpretableresnet(&#39;wide_resnet50_2&#39;, Bottleneck, [3, 4, 6, 3],
                   pretrained, progress, **kwargs)


def interpretable_wide_resnet101_2(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;Wide ResNet-101-2 model from
    `&#34;Wide Residual Networks&#34; &lt;https://arxiv.org/pdf/1605.07146.pdf&gt;`_
    The model is the same as ResNet except for the bottleneck number of channels
    which is twice larger in every block. The number of channels in outer 1x1
    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048
    channels, and in Wide ResNet-50-2 has 2048-1024-2048.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    kwargs[&#39;width_per_group&#39;] = 64 * 2
    return _interpretableresnet(&#39;wide_resnet101_2&#39;, Bottleneck, [3, 4, 23, 3],
                   pretrained, progress, **kwargs)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="GLAMOR.backbones.interpretableresnet.interpretable_resnet101"><code class="name flex">
<span>def <span class="ident">interpretable_resnet101</span></span>(<span>pretrained=False, progress=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>ResNet-101 model from
<code>"Deep Residual Learning for Image Recognition" &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;</code>_</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pretrained</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, returns a model pre-trained on ImageNet</dd>
<dt><strong><code>progress</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, displays a progress bar of the download to stderr</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def interpretable_resnet101(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNet-101 model from
    `&#34;Deep Residual Learning for Image Recognition&#34; &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    return _interpretableresnet(&#39;resnet101&#39;, Bottleneck, [3, 4, 23, 3], pretrained, progress,
                   **kwargs)</code></pre>
</details>
</dd>
<dt id="GLAMOR.backbones.interpretableresnet.interpretable_resnet152"><code class="name flex">
<span>def <span class="ident">interpretable_resnet152</span></span>(<span>pretrained=False, progress=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>ResNet-152 model from
<code>"Deep Residual Learning for Image Recognition" &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;</code>_</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pretrained</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, returns a model pre-trained on ImageNet</dd>
<dt><strong><code>progress</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, displays a progress bar of the download to stderr</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def interpretable_resnet152(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNet-152 model from
    `&#34;Deep Residual Learning for Image Recognition&#34; &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    return _interpretableresnet(&#39;resnet152&#39;, Bottleneck, [3, 8, 36, 3], pretrained, progress,
                   **kwargs)</code></pre>
</details>
</dd>
<dt id="GLAMOR.backbones.interpretableresnet.interpretable_resnet18"><code class="name flex">
<span>def <span class="ident">interpretable_resnet18</span></span>(<span>pretrained=False, progress=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>ResNet-18 model from
<code>"Deep Residual Learning for Image Recognition" &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;</code>_</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pretrained</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, returns a model pre-trained on ImageNet</dd>
<dt><strong><code>progress</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, displays a progress bar of the download to stderr</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def interpretable_resnet18(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNet-18 model from
    `&#34;Deep Residual Learning for Image Recognition&#34; &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    return _interpretableresnet(&#39;resnet18&#39;, BasicBlock, [2, 2, 2, 2], pretrained, progress,
                   **kwargs)</code></pre>
</details>
</dd>
<dt id="GLAMOR.backbones.interpretableresnet.interpretable_resnet34"><code class="name flex">
<span>def <span class="ident">interpretable_resnet34</span></span>(<span>pretrained=False, progress=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>ResNet-34 model from
<code>"Deep Residual Learning for Image Recognition" &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;</code>_</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pretrained</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, returns a model pre-trained on ImageNet</dd>
<dt><strong><code>progress</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, displays a progress bar of the download to stderr</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def interpretable_resnet34(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNet-34 model from
    `&#34;Deep Residual Learning for Image Recognition&#34; &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    return _interpretableresnet(&#39;resnet34&#39;, BasicBlock, [3, 4, 6, 3], pretrained, progress,
                   **kwargs)</code></pre>
</details>
</dd>
<dt id="GLAMOR.backbones.interpretableresnet.interpretable_resnet50"><code class="name flex">
<span>def <span class="ident">interpretable_resnet50</span></span>(<span>pretrained=False, progress=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>ResNet-50 model from
<code>"Deep Residual Learning for Image Recognition" &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;</code>_</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pretrained</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, returns a model pre-trained on ImageNet</dd>
<dt><strong><code>progress</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, displays a progress bar of the download to stderr</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def interpretable_resnet50(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNet-50 model from
    `&#34;Deep Residual Learning for Image Recognition&#34; &lt;https://arxiv.org/pdf/1512.03385.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    return _interpretableresnet(&#39;resnet50&#39;, Bottleneck, [3, 4, 6, 3], pretrained, progress,
                   **kwargs)</code></pre>
</details>
</dd>
<dt id="GLAMOR.backbones.interpretableresnet.interpretable_resnext101_32x8d"><code class="name flex">
<span>def <span class="ident">interpretable_resnext101_32x8d</span></span>(<span>pretrained=False, progress=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>ResNeXt-101 32x8d model from
<code>"Aggregated Residual Transformation for Deep Neural Networks" &lt;https://arxiv.org/pdf/1611.05431.pdf&gt;</code>_</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pretrained</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, returns a model pre-trained on ImageNet</dd>
<dt><strong><code>progress</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, displays a progress bar of the download to stderr</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def interpretable_resnext101_32x8d(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNeXt-101 32x8d model from
    `&#34;Aggregated Residual Transformation for Deep Neural Networks&#34; &lt;https://arxiv.org/pdf/1611.05431.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    kwargs[&#39;groups&#39;] = 32
    kwargs[&#39;width_per_group&#39;] = 8
    return _interpretableresnet(&#39;resnext101_32x8d&#39;, Bottleneck, [3, 4, 23, 3],
                   pretrained, progress, **kwargs)</code></pre>
</details>
</dd>
<dt id="GLAMOR.backbones.interpretableresnet.interpretable_resnext50_32x4d"><code class="name flex">
<span>def <span class="ident">interpretable_resnext50_32x4d</span></span>(<span>pretrained=False, progress=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>ResNeXt-50 32x4d model from
<code>"Aggregated Residual Transformation for Deep Neural Networks" &lt;https://arxiv.org/pdf/1611.05431.pdf&gt;</code>_</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pretrained</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, returns a model pre-trained on ImageNet</dd>
<dt><strong><code>progress</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, displays a progress bar of the download to stderr</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def interpretable_resnext50_32x4d(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;ResNeXt-50 32x4d model from
    `&#34;Aggregated Residual Transformation for Deep Neural Networks&#34; &lt;https://arxiv.org/pdf/1611.05431.pdf&gt;`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    kwargs[&#39;groups&#39;] = 32
    kwargs[&#39;width_per_group&#39;] = 4
    return _interpretableresnet(&#39;resnext50_32x4d&#39;, Bottleneck, [3, 4, 6, 3],
                   pretrained, progress, **kwargs)</code></pre>
</details>
</dd>
<dt id="GLAMOR.backbones.interpretableresnet.interpretable_wide_resnet101_2"><code class="name flex">
<span>def <span class="ident">interpretable_wide_resnet101_2</span></span>(<span>pretrained=False, progress=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Wide ResNet-101-2 model from
<code>"Wide Residual Networks" &lt;https://arxiv.org/pdf/1605.07146.pdf&gt;</code>_
The model is the same as ResNet except for the bottleneck number of channels
which is twice larger in every block. The number of channels in outer 1x1
convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048
channels, and in Wide ResNet-50-2 has 2048-1024-2048.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pretrained</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, returns a model pre-trained on ImageNet</dd>
<dt><strong><code>progress</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, displays a progress bar of the download to stderr</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def interpretable_wide_resnet101_2(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;Wide ResNet-101-2 model from
    `&#34;Wide Residual Networks&#34; &lt;https://arxiv.org/pdf/1605.07146.pdf&gt;`_
    The model is the same as ResNet except for the bottleneck number of channels
    which is twice larger in every block. The number of channels in outer 1x1
    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048
    channels, and in Wide ResNet-50-2 has 2048-1024-2048.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    kwargs[&#39;width_per_group&#39;] = 64 * 2
    return _interpretableresnet(&#39;wide_resnet101_2&#39;, Bottleneck, [3, 4, 23, 3],
                   pretrained, progress, **kwargs)</code></pre>
</details>
</dd>
<dt id="GLAMOR.backbones.interpretableresnet.interpretable_wide_resnet50_2"><code class="name flex">
<span>def <span class="ident">interpretable_wide_resnet50_2</span></span>(<span>pretrained=False, progress=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Wide ResNet-50-2 model from
<code>"Wide Residual Networks" &lt;https://arxiv.org/pdf/1605.07146.pdf&gt;</code>_
The model is the same as ResNet except for the bottleneck number of channels
which is twice larger in every block. The number of channels in outer 1x1
convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048
channels, and in Wide ResNet-50-2 has 2048-1024-2048.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pretrained</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, returns a model pre-trained on ImageNet</dd>
<dt><strong><code>progress</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, displays a progress bar of the download to stderr</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def interpretable_wide_resnet50_2(pretrained=False, progress=True, **kwargs):
    r&#34;&#34;&#34;Wide ResNet-50-2 model from
    `&#34;Wide Residual Networks&#34; &lt;https://arxiv.org/pdf/1605.07146.pdf&gt;`_
    The model is the same as ResNet except for the bottleneck number of channels
    which is twice larger in every block. The number of channels in outer 1x1
    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048
    channels, and in Wide ResNet-50-2 has 2048-1024-2048.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    &#34;&#34;&#34;
    kwargs[&#39;width_per_group&#39;] = 64 * 2
    return _interpretableresnet(&#39;wide_resnet50_2&#39;, Bottleneck, [3, 4, 6, 3],
                   pretrained, progress, **kwargs)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="GLAMOR.backbones.interpretableresnet.BasicBlock"><code class="flex name class">
<span>class <span class="ident">BasicBlock</span></span>
<span>(</span><span>inplanes, planes, stride=1, downsample=None, groups=1, base_width=64, dilation=1, norm_layer=None, attention=None, input_attention=False, part_attention=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BasicBlock(nn.Module):
    expansion = 1

    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,
                 base_width=64, dilation=1, norm_layer=None, 
                 attention=None, input_attention=False, part_attention=False):
        super(BasicBlock, self).__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        if groups != 1 or base_width != 64:
            raise ValueError(&#39;BasicBlock only supports groups=1 and base_width=64&#39;)
        if dilation &gt; 1:
            raise NotImplementedError(&#34;Dilation &gt; 1 not supported in BasicBlock&#34;)
        # Both self.conv1 and self.downsample layers downsample the input when stride != 1
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False, groups=1, dilation=1)
        self.bn1 = norm_layer(planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False, groups=1, dilation=1)
        self.bn2 = norm_layer(planes)       
        
        if input_attention:
            self.input_attention = InputAttention(planes)
        else:
            self.input_attention = None

        if attention is None:
            self.ca = None
            self.sa = None
        elif attention == &#39;cbam&#39;:
            self.ca = ChannelAttention(planes)
            self.sa = SpatialAttention(kernel_size=3)
        elif attention == &#39;dbam&#39;:
            self.ca = ChannelAttention(planes)
            self.sa = DenseAttention(planes)
        else:
            raise NotImplementedError()

        if part_attention:
            self.p_sa = DenseAttention(planes=planes*self.expansion)
            self.p_ca = ChannelAttention(planes*self.expansion)
        else:
            self.p_ca = None
            self.p_sa = None

        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        identity = x

        if self.input_attention is not None:
            x = self.input_attention(x) * x
        
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        if self.ca is not None:
            out = self.ca(out) * out
            out = self.sa(out) * out

        if self.downsample is not None:
            identity = self.downsample(x)

        p_out = out
        part_mask = None
        if self.p_ca is not None:   # Get part attention
            p_out = self.p_sa(p_out) * p_out
#            p_out = self.p_ca(p_out) * p_out
            p_out = self.relu(p_out)
            part_mask = self.p_ca(p_out)

        out = out + identity
        out = self.relu(out)

        if self.p_ca is not None:   # Concat part attention
            #out = torch.cat([p_out[:,p_out.shape[1]//2:,:,:],out[:,:p_out.shape[1]//2,:,:]],dim=1)
            out = (part_mask * p_out) + ((1-part_mask)*out)
        return out</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="GLAMOR.backbones.interpretableresnet.BasicBlock.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="GLAMOR.backbones.interpretableresnet.BasicBlock.expansion"><code class="name">var <span class="ident">expansion</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="GLAMOR.backbones.interpretableresnet.BasicBlock.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="GLAMOR.backbones.interpretableresnet.BasicBlock.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">    def forward(self, x):
        identity = x

        if self.input_attention is not None:
            x = self.input_attention(x) * x
        
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        if self.ca is not None:
            out = self.ca(out) * out
            out = self.sa(out) * out

        if self.downsample is not None:
            identity = self.downsample(x)

        p_out = out
        part_mask = None
        if self.p_ca is not None:   # Get part attention
            p_out = self.p_sa(p_out) * p_out
#            p_out = self.p_ca(p_out) * p_out
            p_out = self.relu(p_out)
            part_mask = self.p_ca(p_out)

        out = out + identity
        out = self.relu(out)

        if self.p_ca is not None:   # Concat part attention
            #out = torch.cat([p_out[:,p_out.shape[1]//2:,:,:],out[:,:p_out.shape[1]//2,:,:]],dim=1)
            out = (part_mask * p_out) + ((1-part_mask)*out)
        return out</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="GLAMOR.backbones.interpretableresnet.Bottleneck"><code class="flex name class">
<span>class <span class="ident">Bottleneck</span></span>
<span>(</span><span>inplanes, planes, stride=1, downsample=None, groups=1, base_width=64, dilation=1, norm_layer=None, attention=None, input_attention=False, part_attention=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Bottleneck(nn.Module):
    expansion = 4

    def __init__(   self, inplanes, planes, stride=1, downsample=None, groups = 1, base_width = 64, dilation = 1, norm_layer=None, 
                    attention = None, input_attention=False, part_attention=False):
        super(Bottleneck, self).__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        width = int(planes * (base_width / 64.)) * groups
        
        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False,stride=1)
        self.bn1 = norm_layer(width)
        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride, padding=dilation, bias=False, groups=groups, dilation=dilation)
        self.bn2 = norm_layer(width)
        self.conv3 = nn.Conv2d(width, planes * self.expansion, kernel_size=1, bias=False, stride=1)
        self.bn3 = norm_layer(planes * self.expansion)
        self.relu = nn.ReLU(inplace=True)
        
        if input_attention:
            self.input_attention = InputAttention(planes)
        else:
            self.input_attention = None

        if attention is None:
            self.ca = None
            self.sa = None
        elif attention == &#39;cbam&#39;:
            self.sa = SpatialAttention(kernel_size=3)
            self.ca = ChannelAttention(planes*self.expansion)
        elif attention == &#39;dbam&#39;:
            self.ca = ChannelAttention(planes)
            self.sa = DenseAttention(planes)
        else:
            raise NotImplementedError()

        if part_attention:
            self.p_sa = DenseAttention(planes=planes*self.expansion)
            self.p_ca = ChannelAttention(planes*self.expansion)
        else:
            self.p_ca = None
            self.p_sa = None
        
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        identity = x

        if self.input_attention is not None:
            x = self.input_attention(x) * x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.ca is not None:
            out = self.ca(out) * out
            out = self.sa(out) * out

        if self.downsample is not None:
            identity = self.downsample(x)

        p_out = out
        part_mask = None
        if self.p_ca is not None:   # Get part attention
            p_out = self.p_sa(p_out) * p_out
#            p_out = self.p_ca(p_out) * p_out
            p_out = self.relu(p_out)
            part_mask = self.p_ca(p_out)
        
        out = out + identity
        out = self.relu(out)

        if self.p_ca is not None:   # Concat part attention
            #out = torch.cat([p_out[:,p_out.shape[1]//2:,:,:],out[:,:p_out.shape[1]//2,:,:]],dim=1)
            out = (part_mask * p_out) + ((1-part_mask)*out)
        return out</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="GLAMOR.backbones.interpretableresnet.Bottleneck.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="GLAMOR.backbones.interpretableresnet.Bottleneck.expansion"><code class="name">var <span class="ident">expansion</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="GLAMOR.backbones.interpretableresnet.Bottleneck.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="GLAMOR.backbones.interpretableresnet.Bottleneck.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">    def forward(self, x):
        identity = x

        if self.input_attention is not None:
            x = self.input_attention(x) * x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.ca is not None:
            out = self.ca(out) * out
            out = self.sa(out) * out

        if self.downsample is not None:
            identity = self.downsample(x)

        p_out = out
        part_mask = None
        if self.p_ca is not None:   # Get part attention
            p_out = self.p_sa(p_out) * p_out
#            p_out = self.p_ca(p_out) * p_out
            p_out = self.relu(p_out)
            part_mask = self.p_ca(p_out)
        
        out = out + identity
        out = self.relu(out)

        if self.p_ca is not None:   # Concat part attention
            #out = torch.cat([p_out[:,p_out.shape[1]//2:,:,:],out[:,:p_out.shape[1]//2,:,:]],dim=1)
            out = (part_mask * p_out) + ((1-part_mask)*out)
        return out</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="GLAMOR.backbones.interpretableresnet.ChannelAttention"><code class="flex name class">
<span>class <span class="ident">ChannelAttention</span></span>
<span>(</span><span>in_planes, ratio=16)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ChannelAttention(nn.Module):
    def __init__(self, in_planes, ratio=16):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)

        self.fc1   = nn.Conv2d(in_planes, in_planes // 16, 1, bias=False)
        self.relu1 = nn.ReLU()
        self.fc2   = nn.Conv2d(in_planes // 16, in_planes, 1, bias=False)

        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))
        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))
        out = avg_out + max_out
        return self.sigmoid(out)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="GLAMOR.backbones.interpretableresnet.ChannelAttention.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="GLAMOR.backbones.interpretableresnet.ChannelAttention.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="GLAMOR.backbones.interpretableresnet.ChannelAttention.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))
    max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))
    out = avg_out + max_out
    return self.sigmoid(out)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="GLAMOR.backbones.interpretableresnet.DenseAttention"><code class="flex name class">
<span>class <span class="ident">DenseAttention</span></span>
<span>(</span><span>planes)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DenseAttention(nn.Module):    # Like spatial, but for all channels
    def __init__(self, planes):
        super(DenseAttention, self).__init__()
        self.dense_conv1=nn.Conv2d(planes,planes,kernel_size=3,padding=1,bias=False)
        self.dense_relu1=nn.LeakyReLU()
        self.dense_conv2=nn.Conv2d(planes,planes,kernel_size=3,padding=1,bias=False)
        self.dense_sigmoid = nn.Sigmoid()
    def forward(self,x):
        x = self.dense_conv1(x)
        x = self.dense_relu1(x)
        x = self.dense_conv2(x)
        x = self.dense_sigmoid(x)
        return x</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="GLAMOR.backbones.interpretableresnet.DenseAttention.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="GLAMOR.backbones.interpretableresnet.DenseAttention.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="GLAMOR.backbones.interpretableresnet.DenseAttention.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self,x):
    x = self.dense_conv1(x)
    x = self.dense_relu1(x)
    x = self.dense_conv2(x)
    x = self.dense_sigmoid(x)
    return x</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="GLAMOR.backbones.interpretableresnet.InputAttention"><code class="flex name class">
<span>class <span class="ident">InputAttention</span></span>
<span>(</span><span>planes)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class InputAttention(nn.Module):
    def __init__(self, planes):
        super(InputAttention, self).__init__()
        self.ia_conv1=nn.Conv2d(planes,planes,kernel_size=3,padding=1,bias=False)
        self.ia_relu1=nn.LeakyReLU()
        self.ia_conv2=nn.Conv2d(planes,planes,kernel_size=3,padding=1,bias=False)
        self.ia_sigmoid = nn.Sigmoid()
    def forward(self,x):
        x = self.ia_conv1(x)
        x = self.ia_relu1(x)
        x = self.ia_conv2(x)
        x = self.ia_sigmoid(x)
        return x</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="GLAMOR.backbones.interpretableresnet.InputAttention.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="GLAMOR.backbones.interpretableresnet.InputAttention.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="GLAMOR.backbones.interpretableresnet.InputAttention.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self,x):
    x = self.ia_conv1(x)
    x = self.ia_relu1(x)
    x = self.ia_conv2(x)
    x = self.ia_sigmoid(x)
    return x</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="GLAMOR.backbones.interpretableresnet.InterpretableResNet"><code class="flex name class">
<span>class <span class="ident">InterpretableResNet</span></span>
<span>(</span><span>block=GLAMOR.backbones.interpretableresnet.Bottleneck, layers=[3, 4, 6, 3], last_stride=2, zero_init_residual=False, top_only=True, num_classes=1000, groups=1, width_per_group=64, replace_stride_with_dilation=None, norm_layer=None, attention=None, input_attention=None, secondary_attention=None, ia_attention=None, part_attention=None, branches=2, branch_classes=[10, 10], **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class InterpretableResNet(nn.Module):
    def __init__(self, block=Bottleneck, layers=[3, 4, 6, 3], last_stride=2, zero_init_residual=False, \
                    top_only=True, num_classes=1000, groups=1, width_per_group=64, replace_stride_with_dilation=None,norm_layer=None, 
                    attention=None, input_attention = None, secondary_attention=None, ia_attention = None, part_attention = None,
                    branches = 2, branch_classes = [10,10],
                    **kwargs):
        super().__init__()
        self.attention=attention
        self.input_attention=input_attention
        self.secondary_attention=secondary_attention
        self.block=block
        self.inplanes = 64

        if top_only is False:
            raise Warning(&#34;top_only parameter should be true. Otherwise, there could be ininteded performance results from model, and it might not compile.&#34;)

        if branches &lt;2:
            raise ValueError(&#34;Must have &gt;1 branch in interpretable model&#34;)
        if len(branch_classes)!= branches:
            raise ValueError(&#34;Number of entries in branch_classes much equal number of branches.&#34;)
        self.branches = branches
        self.branch_classes = branch_classes

        if norm_layer is None:
            self._norm_layer = nn.BatchNorm2d
        #elif norm_layer == &#34;ln&#34;:
        #    self._norm_layer = nn.LayerNorm
        self.dilation = 1
        if replace_stride_with_dilation is None:
            replace_stride_with_dilation = [False, False, False]
        if len(replace_stride_with_dilation) != 3:
            raise ValueError(&#34;replace_stride_with_dilation should be `None` or a 3-element tuple. Got {}&#34;.format(replace_stride_with_dilation))
        self.groups = groups
        self.base_width = width_per_group

        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)
        #if norm_layer == &#34;gn&#34;:
        #    self.bn1 = nn.GroupNorm2d
        self.bn1 = nn.BatchNorm2d(self.inplanes)
        self.relu1 = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

        self.ia_attention = ia_attention
        self.part_attention = part_attention
        
        # Make sure ia and input_attention do not conflict
        if self.ia_attention is not None and self.input_attention is not None:
            raise ValueError(&#34;Cannot have both ia_attention and input_attention.&#34;)
        if self.part_attention is not None and (self.attention is not None and self.secondary_attention is None):
            raise ValueError(&#34;Cannot have part-attention with CBAM everywhere&#34;)
        if self.part_attention is not None and (self.attention is not None and self.secondary_attention==1):
            raise ValueError(&#34;Cannot have part-attention with CBAM-Early&#34;)

        # Create true IA
        if self.ia_attention:
            self.ia_attention = InputAttention(self.inplanes)   # 64, set above
        else:
            self.ia_attention = None

        att = self.attention
        if secondary_attention is not None and secondary_attention != 1: # leave alone if sec attention not set
            att = None
        self.layer1 = self._make_layer(self.block, 64, layers[0], attention = att, input_attention=self.input_attention, part_attention = self.part_attention)



        self.branch_layers = [None]*self.branches

        for branch_idx in self.branches:
            self.branch_layers[branch_idx] = {}   # for layer 2,3,4
            att = self.attention
            if secondary_attention is not None and secondary_attention != 2: # leave alone if sec attention not set
                att = None
            self.branch_layers[branch_idx][&#34;layer2&#34;] = self._make_layer(self.block, 128, layers[1], stride=2, attention = att, dilate=replace_stride_with_dilation[0])
            att = self.attention
            if secondary_attention is not None and secondary_attention != 3: # leave alone if sec attention not set
                att = None
            self.branch_layers[branch_idx][&#34;layer3&#34;] = self._make_layer(self.block, 256, layers[2], stride=2, attention = att, dilate=replace_stride_with_dilation[1])
            att = self.attention
            if secondary_attention is not None and secondary_attention != 4: # leave alone if sec attention not set
                att = None
            self.branch_layers[branch_idx][&#34;layer4&#34;] = self._make_layer(self.block, 512, layers[3], stride=last_stride, attention = att, dilate=replace_stride_with_dilation[2])

            # NOTE TODO: top only is not used -- possibly in future when we no longer use the interpretable stuff?????
            #if not top_only:
                #self.branch_layers[branch_idx][&#34;avgpool&#34;] = nn.AdaptiveAvgPool2d((1, 1))
                #self.branch_layers[branch_idx][&#34;fc&#34;] = nn.Linear(512 * block.expansion, self.branch_classes[branch_idx])



        self.top_only = top_only
        self.avgpool, self.fc = None, None

        #if not self.top_only:
        #    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        #    self.fc = nn.Linear(512 * block.expansion * self.branches, num_classes)
    
    def _make_layer(self, block, planes, blocks, stride=1, dilate = False, attention = None, input_attention=False, ia_attention = False, part_attention = False):
        downsample = None
        previous_dilation = self.dilation
        if dilate:
            self.dilation *= stride
            stride = 1
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.inplanes, planes * block.expansion,
                            kernel_size=1, stride=stride, bias=False),
                self._norm_layer(planes * block.expansion),
            )
    
        layers = []
        layers.append(block(self.inplanes, planes, stride, downsample,groups = self.groups, base_width = self.base_width, dilation = previous_dilation, norm_layer=self._norm_layer, attention=attention, input_attention=input_attention, part_attention=part_attention))
        self.inplanes = planes * block.expansion
        for _ in range(1, blocks):
            layers.append(block(self.inplanes, planes, groups = self.groups, base_width = self.base_width, dilation = self.dilation, norm_layer=self._norm_layer, attention=attention))
        return nn.Sequential(*layers)
    
    def forward(self, x):
        branch_outs = [None]*self.branches
        branch_features = [None]*self.branches
        x = self.conv1(x)
        
        if self.ia_attention is not None:
            x = self.ia_attention(x) * x
        x = self.bn1(x)
        x = self.relu1(x)
        x = self.maxpool(x)
    
        x = self.layer1(x)

        for branch_idx in self.branches:
            branch_outs[branch_idx] = self.branch_layers[branch_idx][&#34;layer2&#34;](x)
            branch_outs[branch_idx] = self.branch_layers[branch_idx][&#34;layer3&#34;](branch_outs[branch_idx])
            branch_outs[branch_idx] = self.branch_layers[branch_idx][&#34;layer4&#34;](branch_outs[branch_idx])
            #branch_features[branch_idx]=nn.Identity(branch_outs[branch_idx])

            #if not self.top_only:
            #    branch_outs[branch_idx] = self.branch_layers[branch_idx][&#34;avgpool&#34;](branch_outs[branch_idx])
            #    branch_outs[branch_idx] = torch.flatten(branch_outs[branch_idx],1)
            #    branch_outs[branch_idx] = self.branch_layers[branch_idx][&#34;fc&#34;](branch_outs[branch_idx])


        #x = self.layer2(x)
        #x = self.layer3(x)
        #x = self.layer4(x)
        # NOTE: top_only will never be false
        x = torch.concat(branch_features,1)

        #if not self.top_only:
        #    x = self.avgpool(x)
        #    x = torch.flatten(x,1)
        #    x = self.fc(x)
        return x, branch_outs
    
    def load_param(self, weights_path):
        # need to verify this TODO
        param_dict = torch.load(weights_path)
        for i in param_dict:
            if &#39;fc&#39; in i and self.top_only:
                continue
            self.state_dict()[i].copy_(param_dict[i])</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="GLAMOR.backbones.interpretableresnet.InterpretableResNet.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="GLAMOR.backbones.interpretableresnet.InterpretableResNet.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="GLAMOR.backbones.interpretableresnet.InterpretableResNet.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    branch_outs = [None]*self.branches
    branch_features = [None]*self.branches
    x = self.conv1(x)
    
    if self.ia_attention is not None:
        x = self.ia_attention(x) * x
    x = self.bn1(x)
    x = self.relu1(x)
    x = self.maxpool(x)

    x = self.layer1(x)

    for branch_idx in self.branches:
        branch_outs[branch_idx] = self.branch_layers[branch_idx][&#34;layer2&#34;](x)
        branch_outs[branch_idx] = self.branch_layers[branch_idx][&#34;layer3&#34;](branch_outs[branch_idx])
        branch_outs[branch_idx] = self.branch_layers[branch_idx][&#34;layer4&#34;](branch_outs[branch_idx])
        #branch_features[branch_idx]=nn.Identity(branch_outs[branch_idx])

        #if not self.top_only:
        #    branch_outs[branch_idx] = self.branch_layers[branch_idx][&#34;avgpool&#34;](branch_outs[branch_idx])
        #    branch_outs[branch_idx] = torch.flatten(branch_outs[branch_idx],1)
        #    branch_outs[branch_idx] = self.branch_layers[branch_idx][&#34;fc&#34;](branch_outs[branch_idx])


    #x = self.layer2(x)
    #x = self.layer3(x)
    #x = self.layer4(x)
    # NOTE: top_only will never be false
    x = torch.concat(branch_features,1)

    #if not self.top_only:
    #    x = self.avgpool(x)
    #    x = torch.flatten(x,1)
    #    x = self.fc(x)
    return x, branch_outs</code></pre>
</details>
</dd>
<dt id="GLAMOR.backbones.interpretableresnet.InterpretableResNet.load_param"><code class="name flex">
<span>def <span class="ident">load_param</span></span>(<span>self, weights_path)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_param(self, weights_path):
    # need to verify this TODO
    param_dict = torch.load(weights_path)
    for i in param_dict:
        if &#39;fc&#39; in i and self.top_only:
            continue
        self.state_dict()[i].copy_(param_dict[i])</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="GLAMOR.backbones.interpretableresnet.SpatialAttention"><code class="flex name class">
<span>class <span class="ident">SpatialAttention</span></span>
<span>(</span><span>kernel_size=7)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SpatialAttention(nn.Module):
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()

        assert kernel_size in (3, 7), &#39;kernel size must be 3 or 7&#39;
        padding = 3 if kernel_size == 7 else 1

        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        x = torch.cat([avg_out, max_out], dim=1)
        x = self.conv1(x)
        return self.sigmoid(x)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="GLAMOR.backbones.interpretableresnet.SpatialAttention.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="GLAMOR.backbones.interpretableresnet.SpatialAttention.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="GLAMOR.backbones.interpretableresnet.SpatialAttention.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    avg_out = torch.mean(x, dim=1, keepdim=True)
    max_out, _ = torch.max(x, dim=1, keepdim=True)
    x = torch.cat([avg_out, max_out], dim=1)
    x = self.conv1(x)
    return self.sigmoid(x)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="GLAMOR.backbones" href="index.html">GLAMOR.backbones</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="GLAMOR.backbones.interpretableresnet.interpretable_resnet101" href="#GLAMOR.backbones.interpretableresnet.interpretable_resnet101">interpretable_resnet101</a></code></li>
<li><code><a title="GLAMOR.backbones.interpretableresnet.interpretable_resnet152" href="#GLAMOR.backbones.interpretableresnet.interpretable_resnet152">interpretable_resnet152</a></code></li>
<li><code><a title="GLAMOR.backbones.interpretableresnet.interpretable_resnet18" href="#GLAMOR.backbones.interpretableresnet.interpretable_resnet18">interpretable_resnet18</a></code></li>
<li><code><a title="GLAMOR.backbones.interpretableresnet.interpretable_resnet34" href="#GLAMOR.backbones.interpretableresnet.interpretable_resnet34">interpretable_resnet34</a></code></li>
<li><code><a title="GLAMOR.backbones.interpretableresnet.interpretable_resnet50" href="#GLAMOR.backbones.interpretableresnet.interpretable_resnet50">interpretable_resnet50</a></code></li>
<li><code><a title="GLAMOR.backbones.interpretableresnet.interpretable_resnext101_32x8d" href="#GLAMOR.backbones.interpretableresnet.interpretable_resnext101_32x8d">interpretable_resnext101_32x8d</a></code></li>
<li><code><a title="GLAMOR.backbones.interpretableresnet.interpretable_resnext50_32x4d" href="#GLAMOR.backbones.interpretableresnet.interpretable_resnext50_32x4d">interpretable_resnext50_32x4d</a></code></li>
<li><code><a title="GLAMOR.backbones.interpretableresnet.interpretable_wide_resnet101_2" href="#GLAMOR.backbones.interpretableresnet.interpretable_wide_resnet101_2">interpretable_wide_resnet101_2</a></code></li>
<li><code><a title="GLAMOR.backbones.interpretableresnet.interpretable_wide_resnet50_2" href="#GLAMOR.backbones.interpretableresnet.interpretable_wide_resnet50_2">interpretable_wide_resnet50_2</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="GLAMOR.backbones.interpretableresnet.BasicBlock" href="#GLAMOR.backbones.interpretableresnet.BasicBlock">BasicBlock</a></code></h4>
<ul class="">
<li><code><a title="GLAMOR.backbones.interpretableresnet.BasicBlock.dump_patches" href="#GLAMOR.backbones.interpretableresnet.BasicBlock.dump_patches">dump_patches</a></code></li>
<li><code><a title="GLAMOR.backbones.interpretableresnet.BasicBlock.expansion" href="#GLAMOR.backbones.interpretableresnet.BasicBlock.expansion">expansion</a></code></li>
<li><code><a title="GLAMOR.backbones.interpretableresnet.BasicBlock.forward" href="#GLAMOR.backbones.interpretableresnet.BasicBlock.forward">forward</a></code></li>
<li><code><a title="GLAMOR.backbones.interpretableresnet.BasicBlock.training" href="#GLAMOR.backbones.interpretableresnet.BasicBlock.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="GLAMOR.backbones.interpretableresnet.Bottleneck" href="#GLAMOR.backbones.interpretableresnet.Bottleneck">Bottleneck</a></code></h4>
<ul class="">
<li><code><a title="GLAMOR.backbones.interpretableresnet.Bottleneck.dump_patches" href="#GLAMOR.backbones.interpretableresnet.Bottleneck.dump_patches">dump_patches</a></code></li>
<li><code><a title="GLAMOR.backbones.interpretableresnet.Bottleneck.expansion" href="#GLAMOR.backbones.interpretableresnet.Bottleneck.expansion">expansion</a></code></li>
<li><code><a title="GLAMOR.backbones.interpretableresnet.Bottleneck.forward" href="#GLAMOR.backbones.interpretableresnet.Bottleneck.forward">forward</a></code></li>
<li><code><a title="GLAMOR.backbones.interpretableresnet.Bottleneck.training" href="#GLAMOR.backbones.interpretableresnet.Bottleneck.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="GLAMOR.backbones.interpretableresnet.ChannelAttention" href="#GLAMOR.backbones.interpretableresnet.ChannelAttention">ChannelAttention</a></code></h4>
<ul class="">
<li><code><a title="GLAMOR.backbones.interpretableresnet.ChannelAttention.dump_patches" href="#GLAMOR.backbones.interpretableresnet.ChannelAttention.dump_patches">dump_patches</a></code></li>
<li><code><a title="GLAMOR.backbones.interpretableresnet.ChannelAttention.forward" href="#GLAMOR.backbones.interpretableresnet.ChannelAttention.forward">forward</a></code></li>
<li><code><a title="GLAMOR.backbones.interpretableresnet.ChannelAttention.training" href="#GLAMOR.backbones.interpretableresnet.ChannelAttention.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="GLAMOR.backbones.interpretableresnet.DenseAttention" href="#GLAMOR.backbones.interpretableresnet.DenseAttention">DenseAttention</a></code></h4>
<ul class="">
<li><code><a title="GLAMOR.backbones.interpretableresnet.DenseAttention.dump_patches" href="#GLAMOR.backbones.interpretableresnet.DenseAttention.dump_patches">dump_patches</a></code></li>
<li><code><a title="GLAMOR.backbones.interpretableresnet.DenseAttention.forward" href="#GLAMOR.backbones.interpretableresnet.DenseAttention.forward">forward</a></code></li>
<li><code><a title="GLAMOR.backbones.interpretableresnet.DenseAttention.training" href="#GLAMOR.backbones.interpretableresnet.DenseAttention.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="GLAMOR.backbones.interpretableresnet.InputAttention" href="#GLAMOR.backbones.interpretableresnet.InputAttention">InputAttention</a></code></h4>
<ul class="">
<li><code><a title="GLAMOR.backbones.interpretableresnet.InputAttention.dump_patches" href="#GLAMOR.backbones.interpretableresnet.InputAttention.dump_patches">dump_patches</a></code></li>
<li><code><a title="GLAMOR.backbones.interpretableresnet.InputAttention.forward" href="#GLAMOR.backbones.interpretableresnet.InputAttention.forward">forward</a></code></li>
<li><code><a title="GLAMOR.backbones.interpretableresnet.InputAttention.training" href="#GLAMOR.backbones.interpretableresnet.InputAttention.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="GLAMOR.backbones.interpretableresnet.InterpretableResNet" href="#GLAMOR.backbones.interpretableresnet.InterpretableResNet">InterpretableResNet</a></code></h4>
<ul class="">
<li><code><a title="GLAMOR.backbones.interpretableresnet.InterpretableResNet.dump_patches" href="#GLAMOR.backbones.interpretableresnet.InterpretableResNet.dump_patches">dump_patches</a></code></li>
<li><code><a title="GLAMOR.backbones.interpretableresnet.InterpretableResNet.forward" href="#GLAMOR.backbones.interpretableresnet.InterpretableResNet.forward">forward</a></code></li>
<li><code><a title="GLAMOR.backbones.interpretableresnet.InterpretableResNet.load_param" href="#GLAMOR.backbones.interpretableresnet.InterpretableResNet.load_param">load_param</a></code></li>
<li><code><a title="GLAMOR.backbones.interpretableresnet.InterpretableResNet.training" href="#GLAMOR.backbones.interpretableresnet.InterpretableResNet.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="GLAMOR.backbones.interpretableresnet.SpatialAttention" href="#GLAMOR.backbones.interpretableresnet.SpatialAttention">SpatialAttention</a></code></h4>
<ul class="">
<li><code><a title="GLAMOR.backbones.interpretableresnet.SpatialAttention.dump_patches" href="#GLAMOR.backbones.interpretableresnet.SpatialAttention.dump_patches">dump_patches</a></code></li>
<li><code><a title="GLAMOR.backbones.interpretableresnet.SpatialAttention.forward" href="#GLAMOR.backbones.interpretableresnet.SpatialAttention.forward">forward</a></code></li>
<li><code><a title="GLAMOR.backbones.interpretableresnet.SpatialAttention.training" href="#GLAMOR.backbones.interpretableresnet.SpatialAttention.training">training</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>