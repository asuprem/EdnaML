<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>GLAMOR.loss.TripletLoss API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>GLAMOR.loss.TripletLoss</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import pdb
import torch
from torch import nn
from . import Loss

# Adapted from https://github.com/NegatioN/OnlineMiningTripletLoss/blob/master/triplet_loss.py
class TripletLoss(Loss):
  &#34;&#34;&#34;Standard triplet loss

  Calculates the triplet loss of a mini-batch.

  Args (kwargs only):
      margin (float, 0.3): Margin constraint to use in triplet loss. If not provided,
      mine (str): Mining method. Default &#39;hard&#39;. Supports [&#39;hard&#39;, &#39;all&#39;]. 

  Methods: 
      __call__: Returns loss given features and labels.
  &#34;&#34;&#34;
  def __init__(self, **kwargs):
    super(TripletLoss, self).__init__()

    self.margin = kwargs.get(&#39;margin&#39;, 0.3)
    mine = kwargs.get(&#34;mine&#34;, &#34;hard&#34;)

    if mine == &#34;hard&#34;:
      self.loss_fn = self.hard_mining
    elif mine == &#34;all&#34;:
      self.loss_fn = self.all_mining
    else:
      raise NotImplementedError()
    
  def forward(self, features, labels):
    &#34;&#34;&#34; Returns the triplet loss with either batch hard mining or batch all mining.
    Args:
        features: features matrix with shape (batch_size, emb_dim)
        labels: ground truth labels with shape (batch_size)
    &#34;&#34;&#34;

    return self.loss_fn(features, labels, self.margin)


  def hard_mining(self, features, labels, margin, squared=False, device=&#39;cuda&#39;):
    &#34;&#34;&#34;Build the triplet loss over a batch of features.

    For each anchor, we get the hardest positive and hardest negative to form a triplet.
    
    Args:
        labels: labels of the batch, of size (batch_size,)
        features: tensor of shape (batch_size, embed_dim)
        margin: margin for triplet loss
        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.
                 If false, output is the pairwise euclidean distance matrix.
    
    Returns:
        triplet_loss: scalar tensor containing the triplet loss
        
    &#34;&#34;&#34;
    # Get the pairwise distance matrix
    pairwise_dist = self._pairwise_distances(features, squared=squared)

    # For each anchor, get the hardest positive
    # First, we need to get a mask for every valid positive (they should have same label)
    mask_anchor_positive = self._get_anchor_positive_triplet_mask(labels, device).float()

    # We put to 0 any element where (a, p) is not valid (valid if a != p and label(a) == label(p))
    anchor_positive_dist = mask_anchor_positive * pairwise_dist

    # shape (batch_size, 1)
    hardest_positive_dist, _ = anchor_positive_dist.max(1, keepdim=True)

    # For each anchor, get the hardest negative
    # First, we need to get a mask for every valid negative (they should have different labels)
    mask_anchor_negative = self._get_anchor_negative_triplet_mask(labels).float()

    # We add the maximum value in each row to the invalid negatives (label(a) == label(n))
    max_anchor_negative_dist, _ = pairwise_dist.max(1, keepdim=True)
    anchor_negative_dist = pairwise_dist + max_anchor_negative_dist * (1.0 - mask_anchor_negative)

    # shape (batch_size,)
    hardest_negative_dist, _ = anchor_negative_dist.min(1, keepdim=True)

    # Combine biggest d(a, p) and smallest d(a, n) into final triplet loss
    tl = hardest_positive_dist - hardest_negative_dist + margin
    tl[tl &lt; 0] = 0
    triplet_loss = tl.mean()

    return triplet_loss

  def all_mining(self, features, labels, margin, squared=False):
    &#34;&#34;&#34;Build the triplet loss over a batch of features.

    We generate all the valid triplets and average the loss over the positive ones.
    
    Args:
        labels: labels of the batch, of size (batch_size,)
        features: tensor of shape (batch_size, embed_dim)
        margin: margin for triplet loss
        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.
                 If false, output is the pairwise euclidean distance matrix.

    Returns:
        triplet_loss: scalar tensor containing the triplet loss

    &#34;&#34;&#34;
    # Get the pairwise distance matrix
    pairwise_dist = self._pairwise_distances(features, squared=squared)

    anchor_positive_dist = pairwise_dist.unsqueeze(2)
    anchor_negative_dist = pairwise_dist.unsqueeze(1)

    # Compute a 3D tensor of size (batch_size, batch_size, batch_size)
    # triplet_loss[i, j, k] will contain the triplet loss of anchor=i, positive=j, negative=k
    # Uses broadcasting where the 1st argument has shape (batch_size, batch_size, 1)
    # and the 2nd (batch_size, 1, batch_size)
    triplet_loss = anchor_positive_dist - anchor_negative_dist + margin

    # Put to zero the invalid triplets
    # (where label(a) != label(p) or label(n) == label(a) or a == p)
    mask = self._get_triplet_mask(labels)
    triplet_loss = mask.float() * triplet_loss

    # Remove negative losses (i.e. the easy triplets)
    triplet_loss[triplet_loss &lt; 0] = 0

    # Count number of positive triplets (where triplet_loss &gt; 0)
    valid_triplets = triplet_loss[triplet_loss &gt; 1e-16]
    num_positive_triplets = valid_triplets.size(0)
    num_valid_triplets = mask.sum()

    fraction_positive_triplets = num_positive_triplets / (num_valid_triplets.float() + 1e-16)

    # Get final mean triplet loss over the positive valid triplets
    triplet_loss = triplet_loss.sum() / (num_positive_triplets + 1e-16)

    return triplet_loss #, fraction_positive_triplets


  def _pairwise_distances(self,features, squared=False):
    &#34;&#34;&#34;Compute the 2D matrix of distances between all the features.
    Args:
        features: tensor of shape (batch_size, embed_dim)
        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.
                  If false, output is the pairwise euclidean distance matrix.
    Returns:
        pairwise_distances: tensor of shape (batch_size, batch_size)
    &#34;&#34;&#34;
    dot_product = torch.matmul(features, features.t())

    # Get squared L2 norm for each features. We can just take the diagonal of `dot_product`.
    # This also provides more numerical stability (the diagonal of the result will be exactly 0).
    # shape (batch_size,)
    square_norm = torch.diag(dot_product)

    # Compute the pairwise distance matrix as we have:
    # ||a - b||^2 = ||a||^2  - 2 &lt;a, b&gt; + ||b||^2
    # shape (batch_size, batch_size)
    distances = square_norm.unsqueeze(0) - 2.0 * dot_product + square_norm.unsqueeze(1)

    # Because of computation errors, some distances might be negative so we put everything &gt;= 0.0
    distances[distances &lt; 0] = 0

    if not squared:
        # Because the gradient of sqrt is infinite when distances == 0.0 (ex: on the diagonal)
        # we need to add a small epsilon where distances == 0.0
        mask = distances.eq(0).float()
        distances = distances + mask * 1e-16

        distances = (1.0 -mask) * torch.sqrt(distances)

    return distances

  def _get_triplet_mask(self,labels):
    &#34;&#34;&#34;Return a 3D mask where mask[a, p, n] is True iff the triplet (a, p, n) is valid.
    A triplet (i, j, k) is valid if:
        - i, j, k are distinct
        - labels[i] == labels[j] and labels[i] != labels[k]
    Args:
        labels: tf.int32 `Tensor` with shape [batch_size]
    &#34;&#34;&#34;
    # Check that i, j and k are distinct
    indices_equal = torch.eye(labels.size(0)).bool()
    indices_not_equal = ~indices_equal
    i_not_equal_j = indices_not_equal.unsqueeze(2)
    i_not_equal_k = indices_not_equal.unsqueeze(1)
    j_not_equal_k = indices_not_equal.unsqueeze(0)

    distinct_indices = (i_not_equal_j &amp; i_not_equal_k) &amp; j_not_equal_k


    label_equal = labels.unsqueeze(0) == labels.unsqueeze(1)
    i_equal_j = label_equal.unsqueeze(2)
    i_equal_k = label_equal.unsqueeze(1)

    valid_labels = ~i_equal_k &amp; i_equal_j

    return valid_labels &amp; distinct_indices


  def _get_anchor_positive_triplet_mask(self, labels, device):
    &#34;&#34;&#34;Return a 2D mask where mask[a, p] is True iff a and p are distinct and have same label.
    Args:
        labels: tf.int32 `Tensor` with shape [batch_size]
    Returns:
        mask: tf.bool `Tensor` with shape [batch_size, batch_size]
    &#34;&#34;&#34;
    # Check that i and j are distinct
    indices_equal = torch.eye(labels.size(0)).bool().to(device)
    indices_not_equal = ~indices_equal

    # Check if labels[i] == labels[j]
    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)
    labels_equal = labels.unsqueeze(0) == labels.unsqueeze(1)
    return labels_equal &amp; indices_not_equal


  def _get_anchor_negative_triplet_mask(self,labels):
    &#34;&#34;&#34;Return a 2D mask where mask[a, n] is True iff a and n have distinct labels.
    Args:
        labels: tf.int32 `Tensor` with shape [batch_size]
    Returns:
        mask: tf.bool `Tensor` with shape [batch_size, batch_size]
    &#34;&#34;&#34;
    # Check if labels[i] != labels[k]
    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)

    return ~(labels.unsqueeze(0) == labels.unsqueeze(1))</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="GLAMOR.loss.TripletLoss.TripletLoss"><code class="flex name class">
<span>class <span class="ident">TripletLoss</span></span>
<span>(</span><span>**kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Standard triplet loss</p>
<p>Calculates the triplet loss of a mini-batch.</p>
<p>Args (kwargs only):
margin (float, 0.3): Margin constraint to use in triplet loss. If not provided,
mine (str): Mining method. Default 'hard'. Supports ['hard', 'all']. </p>
<p>Methods:
<strong>call</strong>: Returns loss given features and labels.</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TripletLoss(Loss):
  &#34;&#34;&#34;Standard triplet loss

  Calculates the triplet loss of a mini-batch.

  Args (kwargs only):
      margin (float, 0.3): Margin constraint to use in triplet loss. If not provided,
      mine (str): Mining method. Default &#39;hard&#39;. Supports [&#39;hard&#39;, &#39;all&#39;]. 

  Methods: 
      __call__: Returns loss given features and labels.
  &#34;&#34;&#34;
  def __init__(self, **kwargs):
    super(TripletLoss, self).__init__()

    self.margin = kwargs.get(&#39;margin&#39;, 0.3)
    mine = kwargs.get(&#34;mine&#34;, &#34;hard&#34;)

    if mine == &#34;hard&#34;:
      self.loss_fn = self.hard_mining
    elif mine == &#34;all&#34;:
      self.loss_fn = self.all_mining
    else:
      raise NotImplementedError()
    
  def forward(self, features, labels):
    &#34;&#34;&#34; Returns the triplet loss with either batch hard mining or batch all mining.
    Args:
        features: features matrix with shape (batch_size, emb_dim)
        labels: ground truth labels with shape (batch_size)
    &#34;&#34;&#34;

    return self.loss_fn(features, labels, self.margin)


  def hard_mining(self, features, labels, margin, squared=False, device=&#39;cuda&#39;):
    &#34;&#34;&#34;Build the triplet loss over a batch of features.

    For each anchor, we get the hardest positive and hardest negative to form a triplet.
    
    Args:
        labels: labels of the batch, of size (batch_size,)
        features: tensor of shape (batch_size, embed_dim)
        margin: margin for triplet loss
        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.
                 If false, output is the pairwise euclidean distance matrix.
    
    Returns:
        triplet_loss: scalar tensor containing the triplet loss
        
    &#34;&#34;&#34;
    # Get the pairwise distance matrix
    pairwise_dist = self._pairwise_distances(features, squared=squared)

    # For each anchor, get the hardest positive
    # First, we need to get a mask for every valid positive (they should have same label)
    mask_anchor_positive = self._get_anchor_positive_triplet_mask(labels, device).float()

    # We put to 0 any element where (a, p) is not valid (valid if a != p and label(a) == label(p))
    anchor_positive_dist = mask_anchor_positive * pairwise_dist

    # shape (batch_size, 1)
    hardest_positive_dist, _ = anchor_positive_dist.max(1, keepdim=True)

    # For each anchor, get the hardest negative
    # First, we need to get a mask for every valid negative (they should have different labels)
    mask_anchor_negative = self._get_anchor_negative_triplet_mask(labels).float()

    # We add the maximum value in each row to the invalid negatives (label(a) == label(n))
    max_anchor_negative_dist, _ = pairwise_dist.max(1, keepdim=True)
    anchor_negative_dist = pairwise_dist + max_anchor_negative_dist * (1.0 - mask_anchor_negative)

    # shape (batch_size,)
    hardest_negative_dist, _ = anchor_negative_dist.min(1, keepdim=True)

    # Combine biggest d(a, p) and smallest d(a, n) into final triplet loss
    tl = hardest_positive_dist - hardest_negative_dist + margin
    tl[tl &lt; 0] = 0
    triplet_loss = tl.mean()

    return triplet_loss

  def all_mining(self, features, labels, margin, squared=False):
    &#34;&#34;&#34;Build the triplet loss over a batch of features.

    We generate all the valid triplets and average the loss over the positive ones.
    
    Args:
        labels: labels of the batch, of size (batch_size,)
        features: tensor of shape (batch_size, embed_dim)
        margin: margin for triplet loss
        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.
                 If false, output is the pairwise euclidean distance matrix.

    Returns:
        triplet_loss: scalar tensor containing the triplet loss

    &#34;&#34;&#34;
    # Get the pairwise distance matrix
    pairwise_dist = self._pairwise_distances(features, squared=squared)

    anchor_positive_dist = pairwise_dist.unsqueeze(2)
    anchor_negative_dist = pairwise_dist.unsqueeze(1)

    # Compute a 3D tensor of size (batch_size, batch_size, batch_size)
    # triplet_loss[i, j, k] will contain the triplet loss of anchor=i, positive=j, negative=k
    # Uses broadcasting where the 1st argument has shape (batch_size, batch_size, 1)
    # and the 2nd (batch_size, 1, batch_size)
    triplet_loss = anchor_positive_dist - anchor_negative_dist + margin

    # Put to zero the invalid triplets
    # (where label(a) != label(p) or label(n) == label(a) or a == p)
    mask = self._get_triplet_mask(labels)
    triplet_loss = mask.float() * triplet_loss

    # Remove negative losses (i.e. the easy triplets)
    triplet_loss[triplet_loss &lt; 0] = 0

    # Count number of positive triplets (where triplet_loss &gt; 0)
    valid_triplets = triplet_loss[triplet_loss &gt; 1e-16]
    num_positive_triplets = valid_triplets.size(0)
    num_valid_triplets = mask.sum()

    fraction_positive_triplets = num_positive_triplets / (num_valid_triplets.float() + 1e-16)

    # Get final mean triplet loss over the positive valid triplets
    triplet_loss = triplet_loss.sum() / (num_positive_triplets + 1e-16)

    return triplet_loss #, fraction_positive_triplets


  def _pairwise_distances(self,features, squared=False):
    &#34;&#34;&#34;Compute the 2D matrix of distances between all the features.
    Args:
        features: tensor of shape (batch_size, embed_dim)
        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.
                  If false, output is the pairwise euclidean distance matrix.
    Returns:
        pairwise_distances: tensor of shape (batch_size, batch_size)
    &#34;&#34;&#34;
    dot_product = torch.matmul(features, features.t())

    # Get squared L2 norm for each features. We can just take the diagonal of `dot_product`.
    # This also provides more numerical stability (the diagonal of the result will be exactly 0).
    # shape (batch_size,)
    square_norm = torch.diag(dot_product)

    # Compute the pairwise distance matrix as we have:
    # ||a - b||^2 = ||a||^2  - 2 &lt;a, b&gt; + ||b||^2
    # shape (batch_size, batch_size)
    distances = square_norm.unsqueeze(0) - 2.0 * dot_product + square_norm.unsqueeze(1)

    # Because of computation errors, some distances might be negative so we put everything &gt;= 0.0
    distances[distances &lt; 0] = 0

    if not squared:
        # Because the gradient of sqrt is infinite when distances == 0.0 (ex: on the diagonal)
        # we need to add a small epsilon where distances == 0.0
        mask = distances.eq(0).float()
        distances = distances + mask * 1e-16

        distances = (1.0 -mask) * torch.sqrt(distances)

    return distances

  def _get_triplet_mask(self,labels):
    &#34;&#34;&#34;Return a 3D mask where mask[a, p, n] is True iff the triplet (a, p, n) is valid.
    A triplet (i, j, k) is valid if:
        - i, j, k are distinct
        - labels[i] == labels[j] and labels[i] != labels[k]
    Args:
        labels: tf.int32 `Tensor` with shape [batch_size]
    &#34;&#34;&#34;
    # Check that i, j and k are distinct
    indices_equal = torch.eye(labels.size(0)).bool()
    indices_not_equal = ~indices_equal
    i_not_equal_j = indices_not_equal.unsqueeze(2)
    i_not_equal_k = indices_not_equal.unsqueeze(1)
    j_not_equal_k = indices_not_equal.unsqueeze(0)

    distinct_indices = (i_not_equal_j &amp; i_not_equal_k) &amp; j_not_equal_k


    label_equal = labels.unsqueeze(0) == labels.unsqueeze(1)
    i_equal_j = label_equal.unsqueeze(2)
    i_equal_k = label_equal.unsqueeze(1)

    valid_labels = ~i_equal_k &amp; i_equal_j

    return valid_labels &amp; distinct_indices


  def _get_anchor_positive_triplet_mask(self, labels, device):
    &#34;&#34;&#34;Return a 2D mask where mask[a, p] is True iff a and p are distinct and have same label.
    Args:
        labels: tf.int32 `Tensor` with shape [batch_size]
    Returns:
        mask: tf.bool `Tensor` with shape [batch_size, batch_size]
    &#34;&#34;&#34;
    # Check that i and j are distinct
    indices_equal = torch.eye(labels.size(0)).bool().to(device)
    indices_not_equal = ~indices_equal

    # Check if labels[i] == labels[j]
    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)
    labels_equal = labels.unsqueeze(0) == labels.unsqueeze(1)
    return labels_equal &amp; indices_not_equal


  def _get_anchor_negative_triplet_mask(self,labels):
    &#34;&#34;&#34;Return a 2D mask where mask[a, n] is True iff a and n have distinct labels.
    Args:
        labels: tf.int32 `Tensor` with shape [batch_size]
    Returns:
        mask: tf.bool `Tensor` with shape [batch_size, batch_size]
    &#34;&#34;&#34;
    # Check if labels[i] != labels[k]
    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)

    return ~(labels.unsqueeze(0) == labels.unsqueeze(1))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="GLAMOR.loss.Loss" href="index.html#GLAMOR.loss.Loss">Loss</a></li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="GLAMOR.loss.TripletLoss.TripletLoss.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="GLAMOR.loss.TripletLoss.TripletLoss.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="GLAMOR.loss.TripletLoss.TripletLoss.all_mining"><code class="name flex">
<span>def <span class="ident">all_mining</span></span>(<span>self, features, labels, margin, squared=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Build the triplet loss over a batch of features.</p>
<p>We generate all the valid triplets and average the loss over the positive ones.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>labels</code></strong></dt>
<dd>labels of the batch, of size (batch_size,)</dd>
<dt><strong><code>features</code></strong></dt>
<dd>tensor of shape (batch_size, embed_dim)</dd>
<dt><strong><code>margin</code></strong></dt>
<dd>margin for triplet loss</dd>
<dt><strong><code>squared</code></strong></dt>
<dd>Boolean. If true, output is the pairwise squared euclidean distance matrix.
If false, output is the pairwise euclidean distance matrix.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>triplet_loss</code></dt>
<dd>scalar tensor containing the triplet loss</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def all_mining(self, features, labels, margin, squared=False):
  &#34;&#34;&#34;Build the triplet loss over a batch of features.

  We generate all the valid triplets and average the loss over the positive ones.
  
  Args:
      labels: labels of the batch, of size (batch_size,)
      features: tensor of shape (batch_size, embed_dim)
      margin: margin for triplet loss
      squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.
               If false, output is the pairwise euclidean distance matrix.

  Returns:
      triplet_loss: scalar tensor containing the triplet loss

  &#34;&#34;&#34;
  # Get the pairwise distance matrix
  pairwise_dist = self._pairwise_distances(features, squared=squared)

  anchor_positive_dist = pairwise_dist.unsqueeze(2)
  anchor_negative_dist = pairwise_dist.unsqueeze(1)

  # Compute a 3D tensor of size (batch_size, batch_size, batch_size)
  # triplet_loss[i, j, k] will contain the triplet loss of anchor=i, positive=j, negative=k
  # Uses broadcasting where the 1st argument has shape (batch_size, batch_size, 1)
  # and the 2nd (batch_size, 1, batch_size)
  triplet_loss = anchor_positive_dist - anchor_negative_dist + margin

  # Put to zero the invalid triplets
  # (where label(a) != label(p) or label(n) == label(a) or a == p)
  mask = self._get_triplet_mask(labels)
  triplet_loss = mask.float() * triplet_loss

  # Remove negative losses (i.e. the easy triplets)
  triplet_loss[triplet_loss &lt; 0] = 0

  # Count number of positive triplets (where triplet_loss &gt; 0)
  valid_triplets = triplet_loss[triplet_loss &gt; 1e-16]
  num_positive_triplets = valid_triplets.size(0)
  num_valid_triplets = mask.sum()

  fraction_positive_triplets = num_positive_triplets / (num_valid_triplets.float() + 1e-16)

  # Get final mean triplet loss over the positive valid triplets
  triplet_loss = triplet_loss.sum() / (num_positive_triplets + 1e-16)

  return triplet_loss #, fraction_positive_triplets</code></pre>
</details>
</dd>
<dt id="GLAMOR.loss.TripletLoss.TripletLoss.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, features, labels) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the triplet loss with either batch hard mining or batch all mining.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>features</code></strong></dt>
<dd>features matrix with shape (batch_size, emb_dim)</dd>
<dt><strong><code>labels</code></strong></dt>
<dd>ground truth labels with shape (batch_size)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, features, labels):
  &#34;&#34;&#34; Returns the triplet loss with either batch hard mining or batch all mining.
  Args:
      features: features matrix with shape (batch_size, emb_dim)
      labels: ground truth labels with shape (batch_size)
  &#34;&#34;&#34;

  return self.loss_fn(features, labels, self.margin)</code></pre>
</details>
</dd>
<dt id="GLAMOR.loss.TripletLoss.TripletLoss.hard_mining"><code class="name flex">
<span>def <span class="ident">hard_mining</span></span>(<span>self, features, labels, margin, squared=False, device='cuda')</span>
</code></dt>
<dd>
<div class="desc"><p>Build the triplet loss over a batch of features.</p>
<p>For each anchor, we get the hardest positive and hardest negative to form a triplet.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>labels</code></strong></dt>
<dd>labels of the batch, of size (batch_size,)</dd>
<dt><strong><code>features</code></strong></dt>
<dd>tensor of shape (batch_size, embed_dim)</dd>
<dt><strong><code>margin</code></strong></dt>
<dd>margin for triplet loss</dd>
<dt><strong><code>squared</code></strong></dt>
<dd>Boolean. If true, output is the pairwise squared euclidean distance matrix.
If false, output is the pairwise euclidean distance matrix.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>triplet_loss</code></dt>
<dd>scalar tensor containing the triplet loss</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def hard_mining(self, features, labels, margin, squared=False, device=&#39;cuda&#39;):
  &#34;&#34;&#34;Build the triplet loss over a batch of features.

  For each anchor, we get the hardest positive and hardest negative to form a triplet.
  
  Args:
      labels: labels of the batch, of size (batch_size,)
      features: tensor of shape (batch_size, embed_dim)
      margin: margin for triplet loss
      squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.
               If false, output is the pairwise euclidean distance matrix.
  
  Returns:
      triplet_loss: scalar tensor containing the triplet loss
      
  &#34;&#34;&#34;
  # Get the pairwise distance matrix
  pairwise_dist = self._pairwise_distances(features, squared=squared)

  # For each anchor, get the hardest positive
  # First, we need to get a mask for every valid positive (they should have same label)
  mask_anchor_positive = self._get_anchor_positive_triplet_mask(labels, device).float()

  # We put to 0 any element where (a, p) is not valid (valid if a != p and label(a) == label(p))
  anchor_positive_dist = mask_anchor_positive * pairwise_dist

  # shape (batch_size, 1)
  hardest_positive_dist, _ = anchor_positive_dist.max(1, keepdim=True)

  # For each anchor, get the hardest negative
  # First, we need to get a mask for every valid negative (they should have different labels)
  mask_anchor_negative = self._get_anchor_negative_triplet_mask(labels).float()

  # We add the maximum value in each row to the invalid negatives (label(a) == label(n))
  max_anchor_negative_dist, _ = pairwise_dist.max(1, keepdim=True)
  anchor_negative_dist = pairwise_dist + max_anchor_negative_dist * (1.0 - mask_anchor_negative)

  # shape (batch_size,)
  hardest_negative_dist, _ = anchor_negative_dist.min(1, keepdim=True)

  # Combine biggest d(a, p) and smallest d(a, n) into final triplet loss
  tl = hardest_positive_dist - hardest_negative_dist + margin
  tl[tl &lt; 0] = 0
  triplet_loss = tl.mean()

  return triplet_loss</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="GLAMOR.loss" href="index.html">GLAMOR.loss</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="GLAMOR.loss.TripletLoss.TripletLoss" href="#GLAMOR.loss.TripletLoss.TripletLoss">TripletLoss</a></code></h4>
<ul class="">
<li><code><a title="GLAMOR.loss.TripletLoss.TripletLoss.all_mining" href="#GLAMOR.loss.TripletLoss.TripletLoss.all_mining">all_mining</a></code></li>
<li><code><a title="GLAMOR.loss.TripletLoss.TripletLoss.dump_patches" href="#GLAMOR.loss.TripletLoss.TripletLoss.dump_patches">dump_patches</a></code></li>
<li><code><a title="GLAMOR.loss.TripletLoss.TripletLoss.forward" href="#GLAMOR.loss.TripletLoss.TripletLoss.forward">forward</a></code></li>
<li><code><a title="GLAMOR.loss.TripletLoss.TripletLoss.hard_mining" href="#GLAMOR.loss.TripletLoss.TripletLoss.hard_mining">hard_mining</a></code></li>
<li><code><a title="GLAMOR.loss.TripletLoss.TripletLoss.training" href="#GLAMOR.loss.TripletLoss.TripletLoss.training">training</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>